{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"TgyhdWrSs1oB","colab_type":"code","outputId":"c67bddd6-45cf-4607-b265-3b8a6d500a0b","executionInfo":{"status":"ok","timestamp":1567238978874,"user_tz":-540,"elapsed":28045,"user":{"displayName":"배지원","photoUrl":"","userId":"18394666524337924417"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","SEED = 42"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jLRGwfC1syke","colab_type":"code","outputId":"9da1a98c-12e1-4c6e-de6b-c6f200bae6ed","executionInfo":{"status":"error","timestamp":1567239954937,"user_tz":-540,"elapsed":442,"user":{"displayName":"배지원","photoUrl":"","userId":"18394666524337924417"}},"colab":{"base_uri":"https://localhost:8080/","height":402}},"source":["## main\n","def main():\n","    _process_data()\n","#     _load_merge_data()\n","#     _prepare_data()\n","#     _build_model()\n","#     _train_predict_amount()\n","#     _train_predict_survival()\n","#     _generate_submission()\n","    \n","\n","if __name__ == \"__main__\":\n","    \n","    testname = 'test1'\n","    print(('*'* 50) +'\\ntry model to test1 data\\n' + ('*'*50))\n","    main()\n","    print('\\n\\n\\nThe end.\\n\\n\\n')\n","    \n","#     testname = 'test2'\n","#     print(('*'* 50) +'\\ntry model to test2 data\\n' + ('*'*50))\n","#     main()\n","#     print('\\n\\n\\nThe end.\\n\\n\\n')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["**************************************************\n","try model to test1 data\n","**************************************************\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-cbcd72c2db2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtestname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'test1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'*'\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'\\ntry model to test1 data\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'*'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n\\nThe end.\\n\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-cbcd72c2db2b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#     _load_merge_data()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     _prepare_data()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     _build_model()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name '_process_data' is not defined"]}]},{"cell_type":"code","metadata":{"id":"_J73zj0yswnR","colab_type":"code","colab":{}},"source":["# -*- coding: utf-8 -*-\n","\n","\n","from enum import Enum\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","from sklearn import preprocessing\n","import xgboost as xgb\n","from datetime import datetime\n","import gc\n","\n","import warnings\n","def ignore_warn(*args, **kwargs):\n","    pass\n","warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n","\n","\n","## STEP0: do setting\n","class Settings(Enum):\n","    global train_path\n","    global test_path\n","    global submission_path\n","    global testname\n","    global codeversion\n","    \n","    codeversion = 'try 7 - delete and merge variables'\n","    \n","    path            = 'drive/My Drive/bigcontest2019/'\n","    train_path      = path + 'data/train/'\n","    test_path       = path + 'data/test/'\n","    submission_path = path + 'scripts/model/metrics/bjw_inference/'\n","        \n","    def __str__(self):\n","        return self.value\n","        \n","    \n","## STEP1: process data    \n","def fill_NA(df):\n","    print('\\nFilling NA ...')\n","    \n","    na_ratio = ((df.isnull().sum() / len(df)) * 100).sort_values(ascending=False)\n","    print('NA ratio: ')\n","    print(na_ratio) \n","    \n","    for feature in df:\n","        if df[feature].dtype == 'object':\n","            df[feature] = df[feature].fillna(\"None\")\n","        else:\n","            df[feature] = df[feature].fillna(0)\n","    \n","def encode_features(df):\n","    print('\\nEncoding features ...')\n","    \n","    for feature in df:\n","        if df[feature].dtype == 'object':\n","            print('Encoding ', feature)\n","            lbl = preprocessing.LabelEncoder()\n","            lbl.fit(list(df[feature].values))\n","            df[feature] = lbl.transform(list(df[feature].values))\n","    \n","# def display_feature_target(df, feature):\n","#     fig, ax = plt.subplots()\n","#     ax.scatter(x = df[feature], y = df['logerror'])\n","#     plt.ylabel('logerror', fontsize=13)\n","#     plt.xlabel(feature, fontsize=13)\n","#     plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Bhn4tnVyVOn","colab_type":"code","colab":{}},"source":["import sys\n","mod = sys.modules[__name__]\n","\n","# function define\n","\n","def week_transform(row):\n","    if row>0 and row<8:\n","        row = 1\n","    elif row>7 and row<15:\n","        row = 2\n","    elif row>14 and row<22:\n","        row = 3\n","    else:\n","        row = 4\n","    return row\n","\n","def merge_by_char_id(df_list):\n","    '''\n","    df_list\n","    '''\n","    df = eval(df_list[0])\n","\n","    for arg in df_list[1:]:\n","        df_arg = eval(arg)\n","        df = pd.concat([df, df_arg],axis=1)\n","\n","    return df\n","\n","def pivoting_df(df):\n","    \n","    ind = list(df.columns)[0]\n","    col = list(df.columns)[1]\n","    \n","    names = list(df.columns)[2:]\n","\n","    for name in names:\n","        tmp = df.reset_index(drop=True).pivot(index = ind, columns= col,values=name).fillna(0)\n","        tmp.columns = [name+'_day'+str(x) for x in tmp.columns]\n","\n","        setattr(mod, '{}'.format(name), tmp)\n","\n","    final_df = merge_by_char_id(names)\n","    \n","    return final_df\n","\n","def merge_final_df(df,*args):\n","    '''\n","    df : 기준이 되는 데이터 프레임 (char_id가 전부 있는 쪽으로!!)\n","    *args : 가변인자 (데이터 프레임을 원하는 만큼 집어넣으면 된다.)\n","    '''\n","    for arg in args:\n","        df = pd.merge(df, arg, how='left',left_index=True, right_index=True)\n","\n","    return df"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"s3VZBo7pyG8e","colab_type":"code","colab":{}},"source":["    def _process_data():\n","    \n","    print('CODE VERSION : ', codeversion)\n","    print('\\n\\nSTEP1: processing data ...')\n","    \n","    global train_merge\n","    global test_merge\n","\n","    # load data\n","    print('\\nLoading data ...')\n","    \n","    train_label = pd.read_csv(train_path + 'train_label.csv')\n","    train_activity = pd.read_csv(train_path + 'train_activity.csv')\n","    train_combat = pd.read_csv(train_path + 'train_combat.csv')\n","    train_payment = pd.read_csv(train_path + 'train_payment.csv')\n","    train_pledge = pd.read_csv(train_path + 'train_pledge.csv')\n","    train_trade = pd.read_csv(train_path + 'train_trade.csv')\n","\n","    test_activity = pd.read_csv(test_path + testname + '_activity.csv')\n","    test_combat = pd.read_csv(test_path + testname + '_combat.csv')\n","    test_payment = pd.read_csv(test_path + testname + '_payment.csv')\n","    test_pledge = pd.read_csv(test_path + testname + '_pledge.csv')\n","    test_trade = pd.read_csv(test_path  + testname + '_trade.csv')\n","\n","    train_activity['day'] = train_activity['day'].apply(week_transform)\n","    train_payment['day'] = train_payment['day'].apply(week_transform)\n","    train_trade['day'] = train_trade['day'].apply(week_transform)\n","    train_pledge['day'] = train_pledge['day'].apply(week_transform)\n","    train_combat['day'] = train_combat['day'].apply(week_transform)\n","    \n","    print('\\nGrouping activity data ...')\n","\n","    \n","\n","    train_activity['game_money_change_minus'] = train_activity['game_money_change'] #game_money_change를 마이너스끼리, 플러스끼리 분리\n","    train_activity['game_money_change_minus'] = [0 if i > 0 else i for i in train_activity['game_money_change_minus']]\n","    train_activity['game_money_change'] = [0 if i < 0 else i for i in train_activity['game_money_change']]\n","\n","    test_activity['game_money_change_minus'] = test_activity['game_money_change']\n","    test_activity['game_money_change_minus'] = [0 if i > 0 else i for i in test_activity['game_money_change_minus']]\n","    test_activity['game_money_change'] = [0 if i < 0 else i for i in test_activity['game_money_change']]\n","\n","\n","    activity_agg = {\n","         'char_id':'nunique','server':'nunique', 'playtime':'sum', 'npc_kill':'sum', #'day':'nunique',\n","        'solo_exp':'sum','party_exp':'sum','quest_exp':'sum','rich_monster':'sum', \n","        'death':'sum', 'revive':'sum','exp_recovery':'sum','fishing':'sum','private_shop':'sum',\n","        'game_money_change':'sum', 'game_money_change_minus':'sum','enchant_count':'sum'}\n","    \n","    train_activity_group = train_activity.groupby(['acc_id','day'], as_index = False).agg(activity_agg).reset_index(drop=True)\n","    train_activity_group.columns = ['activity_{}'.format(x) for x in train_activity_group.columns()]\n","    train_activity_merge = pivoting_df(train_activity_group)\n","\n","    test_activity_group = test_activity.groupby(['acc_id','day'], as_index = False).agg(activity_agg).reset_index(drop=True)\n","    test_activity_group.columns = ['activity_{}'.format(x) for x in test_activity_group.columns()]\n","    test_activity_merge = pivoting_df(test_activity_merge)\n","\n","    acc_id_list = train_activity['acc_id'].unique().tolist()\n","    login_day_max = []\n","    logout_day = []\n","    \n","    print('+ making day variables for train activity')\n","\n","    for i in acc_id_list :\n","        dayunique = train_activity[train_activity['acc_id'] == i]['day'].unique().tolist() # i아이디의 접속일 list화\n","        in_list = [0] * 28\n","        in_value = 0\n","        daylogin = 1\n","\n","        out_list = [0] * 28\n","        out_value = 0\n","        day_logout = 0\n","\n","        # login_day_max 변수 생성 (acc_id가 28일 중 최대 연속으로 접속한 일 수. 띄엄띄엄 접속자보다 매일 꾸준히 하는 사람 데이터를 모으고 싶었음)\n","        for j in range(0, len(dayunique)-1) :        \n","            if dayunique[j]+1 == dayunique[j+1] : # 전날 대비 연속 접속한 경우에 연속값 카운트\n","                daylogin += 1 \n","                in_list[in_value] = daylogin  # 연속값 저장\n","            else :\n","                in_value += 1 \n","                daylogin = 1 # 이전 접속일과 다음 접속일이 1이상 차이날 때, 연속값 초기화\n","        login_day_max.append(max(in_list)) # 연속 접속일 리스트 중 최대값 append\n","\n","         #logout-day 변수 생성 (28일 중 후반부에만 로그인 하는 사람들과, 초반 로그인 후 접속하지 않는 유저를 구분하기 위함)\n","        for k in range(0, len(dayunique)-1) :\n","            if dayunique[k]+1 != dayunique[k+1] : #연속 접속하지 않은 경우, 다음 접속일까지 로그아웃한 일자 카운트\n","                day_logout = (dayunique[k+1]-dayunique[k]-1)\n","                out_list[out_value] = day_logout\n","            else :\n","                out_value += 1\n","                day_logout = 0 #접속한 경우 pass 및 연속값 초기화\n","        logout_day.append(max(out_list)) \n","\n","    all_day = pd.DataFrame(columns = ['acc_id', 'login_day_max','logout_day']) # 빈프레임에 day 관련 데이터 2개 추가\n","    all_day['acc_id'] = acc_id_list\n","    all_day['login_day_max'] = login_day_max\n","    all_day['logout_day'] = logout_day\n","\n","    train_activity_merge = train_activity_merge.merge(all_day, on = ['acc_id'], how ='left')\n","    \n","    print('+ making day variables for test activity')\n","\n","    acc_id_list = test_activity['acc_id'].unique().tolist()\n","    login_day_max = []\n","    logout_day = []\n","\n","    for i in acc_id_list :\n","        dayunique = test_activity[test_activity['acc_id'] == i]['day'].unique().tolist() # i아이디의 접속일 list화\n","        in_list = [0] * 28\n","        in_value = 0\n","        daylogin = 1\n","\n","        out_list = [0] * 28\n","        out_value = 0\n","        day_logout = 0\n","\n","        # login_day_max 변수 생성 (acc_id가 28일 중 최대 연속으로 접속한 일 수. 띄엄띄엄 접속자보다 매일 꾸준히 하는 사람 데이터를 모으고 싶었음)\n","        for j in range(0, len(dayunique)-1) :        \n","            if dayunique[j]+1 == dayunique[j+1] : # 전날 대비 연속 접속한 경우에 연속값 카운트\n","                daylogin += 1 \n","                in_list[in_value] = daylogin  # 연속값 저장\n","            else :\n","                in_value += 1 \n","                daylogin = 1 # 이전 접속일과 다음 접속일이 1이상 차이날 때, 연속값 초기화\n","        login_day_max.append(max(in_list)) # 연속 접속일 리스트 중 최대값 append\n","\n","         #logout-day 변수 생성 (28일 중 후반부에만 로그인 하는 사람들과, 초반 로그인 후 접속하지 않는 유저를 구분하기 위함)\n","        for k in range(0, len(dayunique)-1) :\n","            if dayunique[k]+1 != dayunique[k+1] : #연속 접속하지 않은 경우, 다음 접속일까지 로그아웃한 일자 카운트\n","                day_logout = (dayunique[k+1]-dayunique[k]-1)\n","                out_list[out_value] = day_logout\n","            else :\n","                out_value += 1\n","                day_logout = 0 #접속한 경우 pass 및 연속값 초기화\n","        logout_day.append(max(out_list)) \n","\n","    all_day = pd.DataFrame(columns = ['acc_id', 'login_day_max','logout_day']) # 빈프레임에 day 관련 데이터 2개 추가\n","    all_day['acc_id'] = acc_id_list\n","    all_day['login_day_max'] = login_day_max\n","    all_day['logout_day'] = logout_day\n","\n","    test_activity_merge = test_activity_merge.merge(all_day, on = ['acc_id'], how ='left')\n","\n","    print('activity id unique and shape :')\n","    print(train_activity_merge['acc_id'].nunique(), train_activity_merge.shape)\n","    print(test_activity_merge['acc_id'].nunique(), test_activity_merge.shape)\n","    \n","    \n","    # merge payment\n","    print('\\nGrouping payment data ...')\n","    \n","#     train_payment_merge = train_payment.groupby( 'acc_id', as_index = False).agg({'day':'nunique','amount_spent':'max'}).reset_index(drop=True)\n","#     test_payment_merge = test_payment.groupby('acc_id', as_index = False).agg({'day':'nunique','amount_spent':'max'}).reset_index(drop=True)\n","\n","#     train_payment_merge.rename(columns={'amount_spent':'max_spent'}, inplace=True) #amout_spent를 일일 최고 구매량으로 변경\n","#     test_payment_merge.rename(columns={'amount_spent':'max_spent'}, inplace=True)\n","\n","\n","\n","    train_payment_merge = train_payment.groupby( ['acc_id','day'], as_index = False).agg({'amount_spent':'mean'}).reset_index(drop=True)\n","    test_payment_merge = test_payment.groupby(['acc_id','day'], as_index = False).agg({'amount_spent':'mean'}).reset_index(drop=True)\n","    \n","    train_payment_merge.rename(columns={'amount_spent':'mean_spent'}, inplace=True) #amout_spent를 일일 평균 구매량으로 변경\n","    test_payment_merge.rename(columns={'amount_spent':'mean_spent'}, inplace=True)\n","\n","    print('payment id unique and shape :')\n","    print(train_payment_merge['acc_id'].nunique(), train_payment_merge.shape)\n","    print(test_payment_merge['acc_id'].nunique(), test_payment_merge.shape)\n","    \n","    \n","    # merge combat \n","    print('\\nGrouping combat data ...')\n","    class_names = {0:'liege', 1:'knight', 2:'elf', 3:'magician', 4:'dark-elf', 5:'dragon',6:'illusionist',7:'warrior'}\n","    for elem in train_combat['class'].unique():train_combat[class_names[elem]] = train_combat['class'] == elem\n","    for elem in test_combat['class'].unique():test_combat[class_names[elem]] = test_combat['class'] == elem\n","    \n","    combat_agg = {'char_id' : 'nunique', 'server' : 'nunique', 'class' : 'nunique', 'level' : 'max', #'day':'nunique',\n","                  'pledge_cnt' : 'sum','random_attacker_cnt' : 'sum','random_defender_cnt' : 'sum', 'temp_cnt' : 'sum',\n","                  'same_pledge_cnt' : 'sum','etc_cnt' : 'sum','num_opponent' : 'sum','elf' : 'sum','magician' : 'sum',\n","                  'warrior' : 'sum','dark-elf' : 'sum','knight' : 'sum','dragon' : 'sum','liege' : 'sum','illusionist' : 'sum'}\n","        \n","    combat_rename = {'random_attacker_cnt':'combat_random_attacker_cnt','random_defender_cnt' : 'combat_random_defender_cnt',\n","                     'temp_cnt' : 'combat_temp_cnt','same_pledge_cnt' : 'combat_same_pledge_cnt','etc_cnt' : 'combat_etc_cnt'}\n","\n","    train_combat_merge = train_combat.groupby(['acc_id','day'], as_index = False).agg(combat_agg).reset_index(drop=True)\n","    test_combat_merge = test_combat.groupby(['acc_id','day'], as_index = False).agg(combat_agg).reset_index(drop=True)\n","\n","    train_combat_merge.rename(columns=combat_rename, inplace=True)\n","    test_combat_merge.rename(columns=combat_rename, inplace=True)\n","\n","    train_combat_merge = pivoting_df(train_combat_merge)\n","    test_combat_merge = pivoting_df(test_combat_merge)\n","\n","    print('combat id unique and shape :')\n","    print(train_combat_merge['acc_id'].nunique(), train_combat_merge.shape)\n","    print(test_combat_merge['acc_id'].nunique(), test_combat_merge.shape)\n","    \n","    \n","    # merge combat \n","    print('\\nGrouping pledge data ...')\n","    \n","    pledge_agg = {'char_id':'nunique','server':'nunique','pledge_id':'nunique', 'play_char_cnt' : 'sum',\n","                  'combat_char_cnt' : 'sum','pledge_combat_cnt' : 'sum','random_attacker_cnt' : 'sum','random_defender_cnt' : 'sum',\n","                  'same_pledge_cnt' : 'sum', 'temp_cnt' : 'sum','etc_cnt' : 'sum','combat_play_time' : 'sum','non_combat_play_time' : 'sum'}\n","    \n","    train_pledge_merge = train_pledge.groupby(['acc_id','day'], as_index = False).agg(pledge_agg).reset_index(drop=True)\n","    test_pledge_merge = test_pledge.groupby(['acc_id','day'], as_index = False).agg(pledge_agg).reset_index(drop=True)\n","    \n","    train_pledge_merge = pivoting_df(train_pledge_merge)\n","    test_pledge_merge = pivoting_df(train_pledge_merge)\n","\n","    print('pledge id unique and shape :')\n","    print(train_pledge_merge['acc_id'].nunique(), train_pledge_merge.shape)\n","    print(test_pledge_merge['acc_id'].nunique(), test_pledge_merge.shape)\n","    \n","    # merge trade \n","    print('\\nGrouping trade data ...')\n","\n","    # item_type dummies             \n","    train_item_type_dummy = pd.get_dummies(train_trade['item_type'])\n","    test_item_type_dummy = pd.get_dummies(test_trade['item_type'])\n","\n","    # trade_type dummies\n","    type_names = {0:'personal_shop', 1:'trade_shop'}\n","    for elem in train_trade['type'].unique():train_trade[type_names[elem]] = train_trade['type'] == elem\n","    for elem in test_trade['type'].unique():test_trade[type_names[elem]] = test_trade['type'] == elem\n","\n","    # merge dummies\n","    \n","    train_trade = train_trade.drop(columns=['type','item_type'])\n","    train_trade = pd.concat([train_trade, train_item_type_dummy], axis = 1)\n","    \n","    test_trade = test_trade.drop(columns=['type','item_type'])\n","    test_trade = pd.concat([test_trade, test_item_type_dummy], axis = 1)\n","\n","    # rename columns\n","    trade_rename = {'day':'trade_day','server' : 'trade_server', 'time' : 'trade_time'}\n","    train_trade.rename(columns=trade_rename, inplace=True)\n","    test_trade.rename(columns=trade_rename, inplace=True)\n","\n","    #divide source and target id\n","    \n","    train_source_trade = train_trade.drop(columns = ['target_acc_id','source_char_id','target_char_id']).rename(columns={'source_acc_id' : 'acc_id'})\n","    train_source_trade['source'] = 1\n","    train_target_trade = train_trade.drop(columns = ['source_acc_id','source_char_id','target_char_id']).rename(columns={'target_acc_id' : 'acc_id'})\n","    train_target_trade['target'] = 1\n","    \n","    test_source_trade = test_trade.drop(columns = ['target_acc_id','source_char_id','target_char_id']).rename(columns={'source_acc_id' : 'acc_id'})\n","    test_source_trade['source'] = 1\n","    test_target_trade = test_trade.drop(columns = ['source_acc_id','source_char_id','target_char_id']).rename(columns={'target_acc_id' : 'acc_id'})\n","    test_target_trade['target'] = 1\n","\n","\n","    #aggregate\n","    trade_source_agg = { 'trade_time' : 'nunique', 'trade_server' : 'nunique','item_amount' : 'sum', #'trade_day':'nunique',\n","                        'item_price': 'sum', 'trade_shop' : 'sum', 'personal_shop' : 'sum', 'accessory' : 'sum', 'adena' : 'sum',\n","                        'armor' : 'sum', 'enchant_scroll' : 'sum', 'etc' : 'sum', 'spell' : 'sum', 'weapon' : 'sum', 'source' : 'sum'}\n","\n","    trade_target_agg = { 'trade_time' : 'nunique', 'trade_server' : 'nunique','item_amount' : 'sum', #'trade_day':'nunique',\n","                        'item_price': 'sum', 'trade_shop' : 'sum', 'personal_shop' : 'sum', 'accessory' : 'sum', 'adena' : 'sum',\n","                        'armor' : 'sum', 'enchant_scroll' : 'sum', 'etc' : 'sum', 'spell' : 'sum', 'weapon' : 'sum', 'target' : 'sum'}\n","\n","    train_source_trade = train_source_trade.groupby(['acc_id','trade_day'], as_index = False).agg(trade_source_agg).reset_index(drop=True)\n","    train_target_trade = train_target_trade.groupby(['acc_id','trade_day'], as_index = False).agg(trade_target_agg).reset_index(drop=True)\n","    \n","    test_source_trade = test_source_trade.groupby(['acc_id','trade_day'], as_index = False).agg(trade_source_agg).reset_index(drop=True)\n","    test_target_trade = test_target_trade.groupby(['acc_id','trade_day'], as_index = False).agg(trade_target_agg).reset_index(drop=True)\n","\n","    train_source_trade = pivoting_df(train_source_trade)\n","    train_target_trade = pivoting_df(train_target_trade)\n","\n","    test_source_trade = pivoting_df(test_source_trade)\n","    test_target_trade = pivoting_df(test_target_trade)\n","\n","    print('trade id unique and shape :')\n","    print(train_source_trade['acc_id'].nunique(), train_source_trade.shape, train_target_trade['acc_id'].nunique(), train_target_trade.shape)\n","    print(test_source_trade['acc_id'].nunique(), test_source_trade.shape, test_target_trade['acc_id'].nunique(), test_target_trade.shape)\n","    \n","    \n","    \n","    #merge all data\n","    print('\\nMerging all data ...')\n","    train_merge = train_label.merge(train_activity_merge, on = ['acc_id'], how ='left').merge(\n","    train_combat_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    train_pledge_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    train_payment_merge.drop(['day'], axis=1), on = ['acc_id'], how ='left').merge(\n","    train_source_trade, on = ['acc_id'], how = 'left').merge(\n","    train_target_trade, on = ['acc_id'], how = 'left')\n","        \n","    test_merge = test_activity_merge.merge(\n","    test_combat_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    test_pledge_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    test_payment_merge.drop(['day'], axis=1), on = ['acc_id'], how ='left').merge(\n","    test_source_trade, on = ['acc_id'], how = 'left').merge(\n","    test_target_trade, on = ['acc_id'], how = 'left')\n","    \n","    train_merge['attack'] = (train_merge['random_attacker_cnt']+train_merge['combat_random_attacker_cnt']) -(train_merge['random_defender_cnt'] + train_merge['combat_random_defender_cnt'])\n","    test_merge['attack'] = (test_merge['random_attacker_cnt']+test_merge['combat_random_attacker_cnt']) -(test_merge['random_defender_cnt'] + test_merge['combat_random_defender_cnt'])\n","    \n","    train_merge['cnt'] = train_merge['temp_cnt']+train_merge['same_pledge_cnt']+train_merge['etc_cnt'] + train_merge['combat_temp_cnt']+train_merge['combat_same_pledge_cnt']+train_merge['combat_etc_cnt']\n","    test_merge['cnt'] = test_merge['temp_cnt']+test_merge['same_pledge_cnt']+test_merge['etc_cnt'] + test_merge['combat_temp_cnt']+test_merge['combat_same_pledge_cnt']+test_merge['combat_etc_cnt']\n","    \n","    \n","    train_merge['pledge_ratio'] = train_merge['combat_char_cnt']/train_merge['play_char_cnt']\n","    test_merge['pledge_ratio'] = test_merge['combat_char_cnt']/test_merge['play_char_cnt']\n","  \n","    train_merge['day_spent'] = train_merge['day']*train_merge['mean_spent']\n","    test_merge['day_spent'] = test_merge['day']*test_merge['mean_spent']\n","    \n","    drop_merge = ['day','fishing','revive','elf','magician','warrior','dark-elf','knight','dragon','illusionist','class','random_attacker_cnt',\n","                  'random_defender_cnt','temp_cnt','same_pledge_cnt','etc_cnt','same_pledge_cnt','num_opponent','combat_temp_cnt', \n","                  'combat_same_pledge_cnt', 'combat_etc_cnt','combat_random_defender_cnt','combat_random_attacker_cnt','playtime',\n","                  'combat_char_cnt','play_char_cnt','mean_spent']\n","    \n","    \n","    \n","    train_merge = train_merge.drop(drop_merge, axis = 1)\n","    test_merge = test_merge.drop(drop_merge, axis = 1)\n","    \n","    print('\\nmerge id unique and shape :')\n","    print(train_merge['acc_id'].nunique(), train_merge.shape)\n","    print(test_merge['acc_id'].nunique(), test_merge.shape)\n","    \n","    train_merge.to_csv(submission_path + '/merge/' + 'train_merge.csv',index=False)  \n","    test_merge.to_csv(submission_path + '/merge/' + testname + '_merge.csv',index=False)  \n","    print('test and train merge data are saved on ' + submission_path + '/merge/' )\n","    \n","    \n","def _load_merge_data(): \n","\n","    global train_merge\n","    global test_merge\n","    print('\\nSTEP1: load merged data ...')\n","    train_merge = pd.read_csv(submission_path + 'merge/' + 'train_merge.csv')\n","#     test_merge = pd.read_csv(submission_path + 'merge/' + testname + '_merge.csv')\n","    test_merge = pd.read_csv(submission_path + 'merge/' + 'train_merge.csv')\n","  \n","    print('\\ntrain & test merge id unique and shape :')\n","    print('train : ', train_merge['acc_id'].nunique(), train_merge.shape)\n","    print('test: ', test_merge['acc_id'].nunique(), test_merge.shape)\n","\n","    \n","    # fill NA\n","#     fill_NA(train_merge)\n","    \n","    # encode features\n","#     encode_features(train_merge)\n","    \n","### STEP1.1 : prepare train and vaild data\n","\n","\n","\n","def _prepare_data():\n","    # prepare train and valid data\n","    print('\\nPreparing train and valid data ...')\n","\n","    global train_x\n","    global train_ya\n","    global train_ys\n","    \n","    global valid_x\n","    global valid_ya\n","    global valid_ys\n","    \n","    global test_x\n","    global test_acc_id\n","    \n","    \n","    drop_vars = ['acc_id','amount_spent','survival_time']\n","    \n","    train_ya = train_merge['amount_spent']\n","    train_ys = train_merge['survival_time']\n","    \n","    train_x = train_merge\n","    train_x.drop(columns=drop_vars, inplace=True)\n","    \n","    valid_x = train_x[30000:]\n","    valid_ya = train_ya[30000:]\n","    \n","    train_x = train_x[:30000]\n","    train_ya = train_ya[:30000]\n","    \n","    valid_ys = train_ys[30000:]\n","    train_ys = train_ys[:30000]\n","    \n","    print('train x shape: ', train_x.shape)\n","    print('train ya shape: ', train_ya.shape, 'train ys shape: ', train_ys.shape)\n","\n","    print('valid x shape: ', valid_x.shape)\n","    print('valid ya shape: ', valid_ya.shape, 'valid ys shape: ', valid_ys.shape)\n","    \n","    # prepare test data\n","    print('\\nPreparing test data ...')\n","    \n","    test_x = test_merge\n","    test_acc_id = test_merge['acc_id']\n","#     test_x.drop(columns=['acc_id'], inplace=True)\n","    test_x.drop(columns=['acc_id','amount_spent','survival_time'], inplace=True) #train으로 train예측\n","    print('test x shape: ', test_x.shape)\n","\n","    \n","## STEP2: build model\n","def _build_model():\n","    print('\\n\\nSTEP2: building model ...')\n","    \n","    global xgb_params\n","\n","    xgb_params = {\n","        'eta': 0.007,\n","        'max_depth': 8, \n","        'subsample': 0.8,\n","        'objective': 'reg:linear',\n","        'eval_metric': 'mae',\n","        'lambda': 9.0,\n","        'alpha': 0.8,\n","        'colsample_bytree': 0.7,\n","        'silent': 1,\n","        'random_state' : 42\n","    }\n","\n","    \n","\n","## STEP3: train    \n","def _train_predict_amount():\n","    print('\\n\\nSTEP3: training amount...')\n","    \n","    global a_xgb_clf\n","    global a_pred\n","    \n","    ad_train = xgb.DMatrix(train_x, label=train_ya)\n","    ad_valid = xgb.DMatrix(valid_x, label=valid_ya)\n","    \n","    a_evals = [(ad_train, 'train'), (ad_valid, 'valid')]\n","    a_xgb_clf = xgb.train(xgb_params, ad_train, num_boost_round=10000, evals=a_evals, \n","                        early_stopping_rounds=100, verbose_eval=200)\n","    \n","\n","    ad_test = xgb.DMatrix(test_x)\n","    a_pred = a_xgb_clf.predict(ad_test)\n","    \n","def _train_predict_survival():\n","    print('\\n\\nSTEP4: training survival...')\n","    \n","    global s_xgb_clf\n","    global s_pred\n","    \n","    sd_train = xgb.DMatrix(train_x, label=train_ys)\n","    sd_valid = xgb.DMatrix(valid_x, label=valid_ys)\n","    \n","    s_evals = [(sd_train, 'train'), (sd_valid, 'valid')]\n","    s_xgb_clf = xgb.train(xgb_params, sd_train, num_boost_round=10000, evals=s_evals, \n","                        early_stopping_rounds=100, verbose_eval=200)\n","\n","    sd_test = xgb.DMatrix(test_x)\n","    s_pred = s_xgb_clf.predict(sd_test)\n","    \n","\n","## STEP5: generate submission \n","def _generate_submission():\n","    print('\\n\\nSTEP5: generating submission ...')\n","    \n","    test_predict = pd.DataFrame(columns = ['acc_id', 'survival_time','amount_spent'])\n","    \n","    test_predict['acc_id'] = test_acc_id\n","    test_predict['survival_time'] = s_pred\n","    test_predict['amount_spent'] = a_pred\n","    \n","    test_predict['amount_spent'] = [0 if i < 0 else i for i in test_predict['amount_spent']] # 마이너스값 0 처리 \n","    test_predict['survival_time'] = [1 if i < 1 else i for i in test_predict['survival_time']] # 1 미만 값 1 처리\n","    \n","    #testname에 따라 amountspent value 키우기\n","    if testname == 'test1' :\n","        test_predict['amount_spent'] = test_predict['amount_spent']*18 #best : 20\n","    elif testname == 'test2' :\n","        test_predict['amount_spent'] = test_predict['amount_spent']*5\n","    else :\n","        print('testname is wrong!')\n","        \n","    test_predict.to_csv(submission_path + testname + '_train_predict.csv',index=False)    \n"," "],"execution_count":0,"outputs":[]}]}