{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"UBFiiTrMveh9","colab_type":"code","colab":{}},"source":["# model\n","from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\n","from sklearn.model_selection import StratifiedKFold as KFold\n","\n","class model(object):\n","    def __init__(self, train, test, folds=FOLDS, seed=SEED, log=True):\n","        self.train = train\n","        self.test = test.drop('SELNG_PRER_STOR', axis=1)\n","        self.kf = KFold(n_splits=folds, random_state=seed)\n","        self.log = log\n","    \n","    def ridge_model(self):\n","        oof = np.zeros(len(self.train))\n","        pred = np.zeros(len(self.test))\n","        for trn_idx, val_idx in self.kf.split(self.train):\n","            train_df = self.train.loc[trn_idx]\n","            valid_df = self.train.loc[val_idx].drop('SELNG_PRER_STOR', axis=1)\n","            if self.log:\n","                ridge_model = Ridge().fit(train_df.drop('SELNG_PRER_STOR', axis=1), np.log1p(train_df['SELNG_PRER_STOR']))\n","            else:\n","                ridge_model = Ridge().fit(train_df.drop('SELNG_PRER_STOR', axis=1), train_df['SELNG_PRER_STOR'])\n","            oof[val_idx] = ridge_model.predict(valid_df)\n","            pred += ridge_model.predict(self.test)/self.kf.n_splits\n","        return oof, pred\n","    \n","    \n","    def rf_model(self):\n","        \n","    \n","        \n","        return oof, pred\n","    \n","    \n","    def xgb_model(self):\n","        warnings.filterwarnings(action='ignore')\n","        \n","        params={\n","            'eta': 0.001,\n","            'max_depth': 16,\n","            'min_child_weight': 16,\n","            'n_estimators' : 10000,\n","            'subsample': 0.9\n","        }\n","    \n","    \n","        oof = np.zeros(len(self.train))\n","        pred = np.zeros(len(self.test))\n","        for trn_idx, val_idx in self.kf.split(self.train):\n","            if self.log:\n","                train_df = xgb.DMatrix(self.train.loc[trn_idx].drop('SELNG_PRER_STOR', axis=1), label=np.log1p(self.train.loc[trn_idx, 'SELNG_PRER_STOR']))\n","                valid_df = xgb.DMatrix(self.train.loc[val_idx].drop('SELNG_PRER_STOR', axis=1), label=np.log1p(self.train.loc[val_idx, 'SELNG_PRER_STOR']))\n","            else:\n","                train_df = xgb.DMatrix(self.train.loc[trn_idx].drop('SELNG_PRER_STOR', axis=1), label=self.train.loc[trn_idx, 'SELNG_PRER_STOR'])\n","                valid_df = xgb.DMatrix(self.train.loc[val_idx].drop('SELNG_PRER_STOR', axis=1), label=self.train.loc[val_idx, 'SELNG_PRER_STOR'])\n","            xgb_model = xgb.train(params, train_df, num_boost_round=30000, evals=[(train_df, 'train'), (valid_df, 'val')], verbose_eval=5000, early_stopping_rounds=500)\n","            oof[val_idx] = xgb_model.predict(xgb.DMatrix(self.train.loc[val_idx].drop('SELNG_PRER_STOR', axis=1)))\n","            pred += xgb_model.predict(xgb.DMatrix(self.test))/self.kf.n_splits\n","        return oof, pred\n","    \n","    def rmse(self, true, pred):\n","        \n","        if self.log:\n","            true = np.expm1(pred)\n","            mse = mean_squared_error(true, pred)\n","            rmse = np.round(np.sqrt(mse), 2)\n","        else:\n","            mse = mean_squared_error(true, pred)\n","            rmse = np.round(np.sqrt(mse), 2)\n","    \n","        return rmse"],"execution_count":0,"outputs":[]}]}