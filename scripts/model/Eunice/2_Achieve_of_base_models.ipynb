{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2_Achieve_of_base_models.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["LzRg1SsfG69V","6GbCU8BXFncQ","9ds9iDQIGGOx","ThhDBj1MmEpV","C6ZulL9fGZY9","62XbnrCic8IR","YhAnTwxPKN_5"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"jZu7W-inOlaE","colab_type":"code","outputId":"5d53ce76-2167-4fe8-bf24-c6ca3e3d75ce","executionInfo":{"status":"ok","timestamp":1567832351975,"user_tz":-540,"elapsed":101084,"user":{"displayName":"Eunice","photoUrl":"https://lh5.googleusercontent.com/-cSzctfkN1RA/AAAAAAAAAAI/AAAAAAAAQlo/izeHohCao7Y/s64/photo.jpg","userId":"07489983497392537979"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","SEED = 42"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ij3Uv-2fdLy7","colab_type":"text"},"source":["#### MAIN"]},{"cell_type":"code","metadata":{"id":"5kzsJLiVdK7I","colab_type":"code","outputId":"22fb8ee6-bb0b-43e8-ffb6-22895e013578","executionInfo":{"status":"ok","timestamp":1567168971772,"user_tz":-540,"elapsed":429801,"user":{"displayName":"Eunice","photoUrl":"https://lh5.googleusercontent.com/-cSzctfkN1RA/AAAAAAAAAAI/AAAAAAAAQlo/izeHohCao7Y/s64/photo.jpg","userId":"07489983497392537979"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["## main\n","def main():\n","    _process_data()\n","    _prepare_data()\n","#     _load_merge_data()\n","    _build_model()\n","    _train_predict_amount()\n","    _train_predict_survival()\n","    _generate_submission()\n","    \n","\n","if __name__ == \"__main__\":\n","    \n","    testname = 'test1'\n","    print(('*'* 50) +'\\ntry model to test1 data\\n' + ('*'*50))\n","    main()\n","    print('\\n\\n\\nThe end.\\n\\n\\n')\n","    \n","#     testname = 'test2'\n","#     print(('*'* 50) +'\\ntry model to test2 data\\n' + ('*'*50))\n","#     main()\n","#     print('\\n\\n\\nThe end.\\n\\n\\n')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["**************************************************\n","try model to test2 data\n","**************************************************\n","CODE VERSION :  try 7 - delete and merge variables\n","\n","\n","STEP1: processing data ...\n","\n","Loading data ...\n","\n","Grouping activity data ...\n","\n"," + making day variables for train activity\n","\n"," + making day variables for test activity\n","activity id unique and shape :\n","40000 (40000, 20)\n","20000 (20000, 20)\n","\n","Grouping payment data ...\n","payment id unique and shape :\n","23726 (23726, 3)\n","9499 (9499, 3)\n","\n","Grouping combat data ...\n","combat id unique and shape :\n","40000 (40000, 21)\n","20000 (20000, 21)\n","\n","Grouping pledge data ...\n","pledge id unique and shape :\n","33854 (33854, 15)\n","16263 (16263, 15)\n","\n","Grouping trade data ...\n","trade id unique and shape :\n","60645 (60645, 16) 72300 (72300, 16)\n","36675 (36675, 16) 41107 (41107, 16)\n","\n","Merging all data ...\n","\n","merge id unique and shape :\n","40000 (40000, 59)\n","20000 (20000, 57)\n","test and train merge data are saved on drive/My Drive/bigcontest2019/scripts/model/metrics/nes_inference//merge/\n","\n","Preparing train and valid data ...\n","train x shape:  (30000, 56)\n","train ya shape:  (30000,)\n","train ys shape:  (30000,)\n","valid x shape:  (10000, 56)\n","valid ya shape:  (10000,)\n","valid ys shape:  (10000,)\n","\n","Preparing test data ...\n","test x shape:  (20000, 56)\n","\n","\n","STEP2: building model ...\n","\n","\n","STEP3: training amount...\n","[0]\ttrain-mae:0.454544\tvalid-mae:0.477287\n","Multiple eval metrics have been passed: 'valid-mae' will be used for early stopping.\n","\n","Will train until valid-mae hasn't improved in 100 rounds.\n","[20]\ttrain-mae:0.407422\tvalid-mae:0.427845\n","[40]\ttrain-mae:0.36686\tvalid-mae:0.38533\n","[60]\ttrain-mae:0.331983\tvalid-mae:0.348187\n","[80]\ttrain-mae:0.302082\tvalid-mae:0.316357\n","[100]\ttrain-mae:0.276404\tvalid-mae:0.288558\n","[120]\ttrain-mae:0.254507\tvalid-mae:0.264592\n","[140]\ttrain-mae:0.235875\tvalid-mae:0.244054\n","[160]\ttrain-mae:0.219838\tvalid-mae:0.226231\n","[180]\ttrain-mae:0.206096\tvalid-mae:0.210885\n","[200]\ttrain-mae:0.194258\tvalid-mae:0.196814\n","[220]\ttrain-mae:0.184099\tvalid-mae:0.185435\n","[240]\ttrain-mae:0.175412\tvalid-mae:0.175553\n","[260]\ttrain-mae:0.167917\tvalid-mae:0.166626\n","[280]\ttrain-mae:0.161333\tvalid-mae:0.159007\n","[300]\ttrain-mae:0.155544\tvalid-mae:0.152146\n","[320]\ttrain-mae:0.150629\tvalid-mae:0.14603\n","[340]\ttrain-mae:0.146391\tvalid-mae:0.14087\n","[360]\ttrain-mae:0.142612\tvalid-mae:0.136685\n","[380]\ttrain-mae:0.139375\tvalid-mae:0.132597\n","[400]\ttrain-mae:0.136516\tvalid-mae:0.129585\n","[420]\ttrain-mae:0.134046\tvalid-mae:0.126805\n","[440]\ttrain-mae:0.131769\tvalid-mae:0.124449\n","[460]\ttrain-mae:0.129835\tvalid-mae:0.122344\n","[480]\ttrain-mae:0.128052\tvalid-mae:0.120683\n","[500]\ttrain-mae:0.126524\tvalid-mae:0.119029\n","[520]\ttrain-mae:0.125196\tvalid-mae:0.117792\n","[540]\ttrain-mae:0.123969\tvalid-mae:0.116505\n","[560]\ttrain-mae:0.122857\tvalid-mae:0.115258\n","[580]\ttrain-mae:0.121867\tvalid-mae:0.114373\n","[600]\ttrain-mae:0.120913\tvalid-mae:0.113755\n","[620]\ttrain-mae:0.120084\tvalid-mae:0.113364\n","[640]\ttrain-mae:0.119381\tvalid-mae:0.11304\n","[660]\ttrain-mae:0.118694\tvalid-mae:0.112605\n","[680]\ttrain-mae:0.118017\tvalid-mae:0.112414\n","[700]\ttrain-mae:0.117486\tvalid-mae:0.112199\n","[720]\ttrain-mae:0.116984\tvalid-mae:0.11202\n","[740]\ttrain-mae:0.116475\tvalid-mae:0.111852\n","[760]\ttrain-mae:0.116019\tvalid-mae:0.111914\n","[780]\ttrain-mae:0.11554\tvalid-mae:0.111831\n","[800]\ttrain-mae:0.115132\tvalid-mae:0.111843\n","[820]\ttrain-mae:0.114683\tvalid-mae:0.111758\n","[840]\ttrain-mae:0.114239\tvalid-mae:0.111762\n","[860]\ttrain-mae:0.113807\tvalid-mae:0.111839\n","[880]\ttrain-mae:0.11348\tvalid-mae:0.111953\n","[900]\ttrain-mae:0.113132\tvalid-mae:0.111991\n","[920]\ttrain-mae:0.112812\tvalid-mae:0.112092\n","Stopping. Best iteration:\n","[833]\ttrain-mae:0.114409\tvalid-mae:0.111724\n","\n","\n","\n","STEP4: training survival...\n","[0]\ttrain-mae:45.9294\tvalid-mae:41.7759\n","Multiple eval metrics have been passed: 'valid-mae' will be used for early stopping.\n","\n","Will train until valid-mae hasn't improved in 100 rounds.\n","[20]\ttrain-mae:40.2155\tvalid-mae:36.5437\n","[40]\ttrain-mae:35.6407\tvalid-mae:32.5799\n","[60]\ttrain-mae:31.9675\tvalid-mae:29.5169\n","[80]\ttrain-mae:28.9865\tvalid-mae:27.0592\n","[100]\ttrain-mae:26.5546\tvalid-mae:25.0811\n","[120]\ttrain-mae:24.5583\tvalid-mae:23.4545\n","[140]\ttrain-mae:22.9133\tvalid-mae:22.145\n","[160]\ttrain-mae:21.5517\tvalid-mae:21.0575\n","[180]\ttrain-mae:20.4126\tvalid-mae:20.1761\n","[200]\ttrain-mae:19.4415\tvalid-mae:19.4397\n","[220]\ttrain-mae:18.6081\tvalid-mae:18.8223\n","[240]\ttrain-mae:17.8938\tvalid-mae:18.2996\n","[260]\ttrain-mae:17.2679\tvalid-mae:17.8576\n","[280]\ttrain-mae:16.7213\tvalid-mae:17.4935\n","[300]\ttrain-mae:16.2451\tvalid-mae:17.1877\n","[320]\ttrain-mae:15.8247\tvalid-mae:16.9257\n","[340]\ttrain-mae:15.4566\tvalid-mae:16.6996\n","[360]\ttrain-mae:15.1269\tvalid-mae:16.4939\n","[380]\ttrain-mae:14.8379\tvalid-mae:16.3266\n","[400]\ttrain-mae:14.5827\tvalid-mae:16.1821\n","[420]\ttrain-mae:14.3494\tvalid-mae:16.0561\n","[440]\ttrain-mae:14.1364\tvalid-mae:15.9494\n","[460]\ttrain-mae:13.9488\tvalid-mae:15.8545\n","[480]\ttrain-mae:13.7782\tvalid-mae:15.771\n","[500]\ttrain-mae:13.6252\tvalid-mae:15.6968\n","[520]\ttrain-mae:13.4796\tvalid-mae:15.6268\n","[540]\ttrain-mae:13.3548\tvalid-mae:15.5741\n","[560]\ttrain-mae:13.2409\tvalid-mae:15.5242\n","[580]\ttrain-mae:13.1383\tvalid-mae:15.4802\n","[600]\ttrain-mae:13.0459\tvalid-mae:15.4351\n","[620]\ttrain-mae:12.9521\tvalid-mae:15.4034\n","[640]\ttrain-mae:12.8636\tvalid-mae:15.3745\n","[660]\ttrain-mae:12.7928\tvalid-mae:15.3446\n","[680]\ttrain-mae:12.7145\tvalid-mae:15.3144\n","[700]\ttrain-mae:12.6372\tvalid-mae:15.2883\n","[720]\ttrain-mae:12.5683\tvalid-mae:15.2639\n","[740]\ttrain-mae:12.4974\tvalid-mae:15.2437\n","[760]\ttrain-mae:12.4349\tvalid-mae:15.2251\n","[780]\ttrain-mae:12.3728\tvalid-mae:15.2055\n","[800]\ttrain-mae:12.3158\tvalid-mae:15.1887\n","[820]\ttrain-mae:12.2588\tvalid-mae:15.1695\n","[840]\ttrain-mae:12.1986\tvalid-mae:15.1574\n","[860]\ttrain-mae:12.1467\tvalid-mae:15.1438\n","[880]\ttrain-mae:12.0922\tvalid-mae:15.1333\n","[900]\ttrain-mae:12.0415\tvalid-mae:15.1221\n","[920]\ttrain-mae:11.9924\tvalid-mae:15.1094\n","[940]\ttrain-mae:11.9378\tvalid-mae:15.096\n","[960]\ttrain-mae:11.8857\tvalid-mae:15.0855\n","[980]\ttrain-mae:11.8366\tvalid-mae:15.0767\n","[1000]\ttrain-mae:11.7866\tvalid-mae:15.065\n","[1020]\ttrain-mae:11.7477\tvalid-mae:15.0556\n","[1040]\ttrain-mae:11.7059\tvalid-mae:15.0465\n","[1060]\ttrain-mae:11.6612\tvalid-mae:15.0404\n","[1080]\ttrain-mae:11.6147\tvalid-mae:15.0347\n","[1100]\ttrain-mae:11.5669\tvalid-mae:15.0258\n","[1120]\ttrain-mae:11.5193\tvalid-mae:15.0172\n","[1140]\ttrain-mae:11.4768\tvalid-mae:15.0085\n","[1160]\ttrain-mae:11.4285\tvalid-mae:15.005\n","[1180]\ttrain-mae:11.3769\tvalid-mae:15.0023\n","[1200]\ttrain-mae:11.3391\tvalid-mae:14.9946\n","[1220]\ttrain-mae:11.2928\tvalid-mae:14.9872\n","[1240]\ttrain-mae:11.2529\tvalid-mae:14.9827\n","[1260]\ttrain-mae:11.2127\tvalid-mae:14.9768\n","[1280]\ttrain-mae:11.1689\tvalid-mae:14.9719\n","[1300]\ttrain-mae:11.1292\tvalid-mae:14.9679\n","[1320]\ttrain-mae:11.0868\tvalid-mae:14.9649\n","[1340]\ttrain-mae:11.0461\tvalid-mae:14.9639\n","[1360]\ttrain-mae:11.0059\tvalid-mae:14.9567\n","[1380]\ttrain-mae:10.9656\tvalid-mae:14.9502\n","[1400]\ttrain-mae:10.9274\tvalid-mae:14.9464\n","[1420]\ttrain-mae:10.8881\tvalid-mae:14.9422\n","[1440]\ttrain-mae:10.8533\tvalid-mae:14.9407\n","[1460]\ttrain-mae:10.8111\tvalid-mae:14.9382\n","[1480]\ttrain-mae:10.7746\tvalid-mae:14.9356\n","[1500]\ttrain-mae:10.7377\tvalid-mae:14.9326\n","[1520]\ttrain-mae:10.6962\tvalid-mae:14.9286\n","[1540]\ttrain-mae:10.6603\tvalid-mae:14.9243\n","[1560]\ttrain-mae:10.6281\tvalid-mae:14.9207\n","[1580]\ttrain-mae:10.5913\tvalid-mae:14.9206\n","[1600]\ttrain-mae:10.5549\tvalid-mae:14.917\n","[1620]\ttrain-mae:10.5197\tvalid-mae:14.9122\n","[1640]\ttrain-mae:10.4849\tvalid-mae:14.9072\n","[1660]\ttrain-mae:10.4499\tvalid-mae:14.9007\n","[1680]\ttrain-mae:10.4128\tvalid-mae:14.8966\n","[1700]\ttrain-mae:10.3738\tvalid-mae:14.8967\n","[1720]\ttrain-mae:10.3394\tvalid-mae:14.8943\n","[1740]\ttrain-mae:10.304\tvalid-mae:14.8902\n","[1760]\ttrain-mae:10.2717\tvalid-mae:14.8857\n","[1780]\ttrain-mae:10.2364\tvalid-mae:14.8839\n","[1800]\ttrain-mae:10.2045\tvalid-mae:14.8795\n","[1820]\ttrain-mae:10.1695\tvalid-mae:14.8731\n","[1840]\ttrain-mae:10.136\tvalid-mae:14.8724\n","[1860]\ttrain-mae:10.0994\tvalid-mae:14.8677\n","[1880]\ttrain-mae:10.0682\tvalid-mae:14.8642\n","[1900]\ttrain-mae:10.0329\tvalid-mae:14.8612\n","[1920]\ttrain-mae:10.0022\tvalid-mae:14.8602\n","[1940]\ttrain-mae:9.96498\tvalid-mae:14.855\n","[1960]\ttrain-mae:9.93446\tvalid-mae:14.8552\n","[1980]\ttrain-mae:9.90091\tvalid-mae:14.8565\n","[1999]\ttrain-mae:9.86613\tvalid-mae:14.8576\n","\n","\n","STEP5: generating submission ...\n","\n","\n","\n","The end.\n","\n","\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LzRg1SsfG69V","colab_type":"text"},"source":["#### try 7 - delete and merge variables \n","(2019-08-30 09:43:50 PM\t6772.33\t2496.24\t9268.57)"]},{"cell_type":"code","metadata":{"id":"CmkNLV2YG6de","colab_type":"code","colab":{}},"source":["\n","# -*- coding: utf-8 -*-\n","\n","\n","from enum import Enum\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","from sklearn import preprocessing\n","import xgboost as xgb\n","from datetime import datetime\n","import gc\n","\n","import warnings\n","def ignore_warn(*args, **kwargs):\n","    pass\n","warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n","\n","\n","## STEP0: do setting\n","class Settings(Enum):\n","    global train_path\n","    global test_path\n","    global submission_path\n","    global testname\n","    global codeversion\n","    \n","    codeversion = 'try 7 - delete and merge variables'\n","    \n","    path            = 'drive/My Drive/bigcontest2019/'\n","    train_path      = path + 'data/train/'\n","    test_path       = path + 'data/test/'\n","    submission_path = path + 'scripts/model/metrics/nes_inference/'\n","        \n","    def __str__(self):\n","        return self.value\n","        \n","    \n","## STEP1: process data    \n","def fill_NA(df):\n","    print('\\nFilling NA ...')\n","    \n","    na_ratio = ((df.isnull().sum() / len(df)) * 100).sort_values(ascending=False)\n","    print('NA ratio: ')\n","    print(na_ratio) \n","    \n","    for feature in df:\n","        if df[feature].dtype == 'object':\n","            df[feature] = df[feature].fillna(\"None\")\n","        else:\n","            df[feature] = df[feature].fillna(0)\n","    \n","def encode_features(df):\n","    print('\\nEncoding features ...')\n","    \n","    for feature in df:\n","        if df[feature].dtype == 'object':\n","            print('Encoding ', feature)\n","            lbl = preprocessing.LabelEncoder()\n","            lbl.fit(list(df[feature].values))\n","            df[feature] = lbl.transform(list(df[feature].values))\n","    \n","# def display_feature_target(df, feature):\n","#     fig, ax = plt.subplots()\n","#     ax.scatter(x = df[feature], y = df['logerror'])\n","#     plt.ylabel('logerror', fontsize=13)\n","#     plt.xlabel(feature, fontsize=13)\n","#     plt.show()\n","    \n","def _process_data():\n","    \n","    print('CODE VERSION : ', codeversion)\n","    print('\\n\\nSTEP1: processing data ...')\n","    \n","    global train_merge\n","    global test_merge\n","\n","        \n","    # load data\n","    print('\\nLoading data ...')\n","    \n","    train_label = pd.read_csv(train_path + 'train_label.csv')\n","    train_activity = pd.read_csv(train_path + 'train_activity.csv')\n","    train_combat = pd.read_csv(train_path + 'train_combat.csv')\n","    train_payment = pd.read_csv(train_path + 'train_payment.csv')\n","    train_pledge = pd.read_csv(train_path + 'train_pledge.csv')\n","    train_trade = pd.read_csv(train_path + 'train_trade.csv')\n","\n","    test_activity = pd.read_csv(test_path + testname + '_activity.csv')\n","    test_combat = pd.read_csv(test_path + testname + '_combat.csv')\n","    test_payment = pd.read_csv(test_path + testname + '_payment.csv')\n","    test_pledge = pd.read_csv(test_path + testname + '_pledge.csv')\n","    test_trade = pd.read_csv(test_path  + testname + '_trade.csv')\n","\n","    \n","    \n","    print('\\nGrouping activity data ...')\n","\n","    train_activity['game_money_change_minus'] = train_activity['game_money_change'] #game_money_change를 마이너스끼리, 플러스끼리 분리\n","    train_activity['game_money_change_minus'] = [0 if i > 0 else i for i in train_activity['game_money_change_minus']]\n","    train_activity['game_money_change'] = [0 if i < 0 else i for i in train_activity['game_money_change']]\n","\n","    test_activity['game_money_change_minus'] = test_activity['game_money_change']\n","    test_activity['game_money_change_minus'] = [0 if i > 0 else i for i in test_activity['game_money_change_minus']]\n","    test_activity['game_money_change'] = [0 if i < 0 else i for i in test_activity['game_money_change']]\n","\n","    activity_agg = {\n","        'day':'nunique', 'char_id':'nunique','server':'nunique', 'playtime':'sum', 'npc_kill':'sum', \n","        'solo_exp':'sum','party_exp':'sum','quest_exp':'sum','rich_monster':'sum', \n","        'death':'sum', 'revive':'sum','exp_recovery':'sum','fishing':'sum','private_shop':'sum',\n","        'game_money_change':'sum', 'game_money_change_minus':'sum','enchant_count':'sum'}\n","    \n","    train_activity_merge = train_activity.groupby('acc_id', as_index = False).agg(activity_agg).reset_index(drop=True)\n","    test_activity_merge = test_activity.groupby('acc_id', as_index = False).agg(activity_agg).reset_index(drop=True)\n","    \n","    acc_id_list = train_activity['acc_id'].unique().tolist()\n","    login_day_max = []\n","    logout_day = []\n","    \n","    print('\\n + making day variables for train activity')\n","\n","    for i in acc_id_list :\n","        dayunique = train_activity[train_activity['acc_id'] == i]['day'].unique().tolist() # i아이디의 접속일 list화\n","        in_list = [0] * 28\n","        in_value = 0\n","        daylogin = 1\n","\n","        out_list = [0] * 28\n","        out_value = 0\n","        day_logout = 0\n","\n","        # login_day_max 변수 생성 (acc_id가 28일 중 최대 연속으로 접속한 일 수. 띄엄띄엄 접속자보다 매일 꾸준히 하는 사람 데이터를 모으고 싶었음)\n","        for j in range(0, len(dayunique)-1) :        \n","            if dayunique[j]+1 == dayunique[j+1] : # 전날 대비 연속 접속한 경우에 연속값 카운트\n","                daylogin += 1 \n","                in_list[in_value] = daylogin  # 연속값 저장\n","            else :\n","                in_value += 1 \n","                daylogin = 1 # 이전 접속일과 다음 접속일이 1이상 차이날 때, 연속값 초기화\n","        login_day_max.append(max(in_list)) # 연속 접속일 리스트 중 최대값 append\n","\n","         #logout-day 변수 생성 (28일 중 후반부에만 로그인 하는 사람들과, 초반 로그인 후 접속하지 않는 유저를 구분하기 위함)\n","        for k in range(0, len(dayunique)-1) :\n","            if dayunique[k]+1 != dayunique[k+1] : #연속 접속하지 않은 경우, 다음 접속일까지 로그아웃한 일자 카운트\n","                day_logout = (dayunique[k+1]-dayunique[k]-1)\n","                out_list[out_value] = day_logout\n","            else :\n","                out_value += 1\n","                day_logout = 0 #접속한 경우 pass 및 연속값 초기화\n","        logout_day.append(max(out_list)) \n","\n","    all_day = pd.DataFrame(columns = ['acc_id', 'login_day_max','logout_day']) # 빈프레임에 day 관련 데이터 2개 추가\n","    all_day['acc_id'] = acc_id_list\n","    all_day['login_day_max'] = login_day_max\n","    all_day['logout_day'] = logout_day\n","\n","    train_activity_merge = train_activity_merge.merge(all_day, on = ['acc_id'], how ='left')\n","    \n","    print('\\n + making day variables for test activity')\n","\n","    acc_id_list = test_activity['acc_id'].unique().tolist()\n","    login_day_max = []\n","    logout_day = []\n","\n","    for i in acc_id_list :\n","        dayunique = test_activity[test_activity['acc_id'] == i]['day'].unique().tolist() # i아이디의 접속일 list화\n","        in_list = [0] * 28\n","        in_value = 0\n","        daylogin = 1\n","\n","        out_list = [0] * 28\n","        out_value = 0\n","        day_logout = 0\n","\n","        # login_day_max 변수 생성 (acc_id가 28일 중 최대 연속으로 접속한 일 수. 띄엄띄엄 접속자보다 매일 꾸준히 하는 사람 데이터를 모으고 싶었음)\n","        for j in range(0, len(dayunique)-1) :        \n","            if dayunique[j]+1 == dayunique[j+1] : # 전날 대비 연속 접속한 경우에 연속값 카운트\n","                daylogin += 1 \n","                in_list[in_value] = daylogin  # 연속값 저장\n","            else :\n","                in_value += 1 \n","                daylogin = 1 # 이전 접속일과 다음 접속일이 1이상 차이날 때, 연속값 초기화\n","        login_day_max.append(max(in_list)) # 연속 접속일 리스트 중 최대값 append\n","\n","         #logout-day 변수 생성 (28일 중 후반부에만 로그인 하는 사람들과, 초반 로그인 후 접속하지 않는 유저를 구분하기 위함)\n","        for k in range(0, len(dayunique)-1) :\n","            if dayunique[k]+1 != dayunique[k+1] : #연속 접속하지 않은 경우, 다음 접속일까지 로그아웃한 일자 카운트\n","                day_logout = (dayunique[k+1]-dayunique[k]-1)\n","                out_list[out_value] = day_logout\n","            else :\n","                out_value += 1\n","                day_logout = 0 #접속한 경우 pass 및 연속값 초기화\n","        logout_day.append(max(out_list)) \n","\n","    all_day = pd.DataFrame(columns = ['acc_id', 'login_day_max','logout_day']) # 빈프레임에 day 관련 데이터 2개 추가\n","    all_day['acc_id'] = acc_id_list\n","    all_day['login_day_max'] = login_day_max\n","    all_day['logout_day'] = logout_day\n","\n","    test_activity_merge = test_activity_merge.merge(all_day, on = ['acc_id'], how ='left')\n","    \n","    \n","    print('activity id unique and shape :')\n","    print(train_activity_merge['acc_id'].nunique(), train_activity_merge.shape)\n","    print(test_activity_merge['acc_id'].nunique(), test_activity_merge.shape)\n","    \n","    \n","    # merge payment\n","    print('\\nGrouping payment data ...')\n","    \n","#     train_payment_merge = train_payment.groupby( 'acc_id', as_index = False).agg({'day':'nunique','amount_spent':'max'}).reset_index(drop=True)\n","#     test_payment_merge = test_payment.groupby('acc_id', as_index = False).agg({'day':'nunique','amount_spent':'max'}).reset_index(drop=True)\n","\n","#     train_payment_merge.rename(columns={'amount_spent':'max_spent'}, inplace=True) #amout_spent를 일일 최고 구매량으로 변경\n","#     test_payment_merge.rename(columns={'amount_spent':'max_spent'}, inplace=True)\n","\n","\n","\n","    train_payment_merge = train_payment.groupby( 'acc_id', as_index = False).agg({'day':'nunique','amount_spent':'mean'}).reset_index(drop=True)\n","    test_payment_merge = test_payment.groupby('acc_id', as_index = False).agg({'day':'nunique','amount_spent':'mean'}).reset_index(drop=True)\n","\n","    train_payment_merge.rename(columns={'amount_spent':'mean_spent'}, inplace=True) #amout_spent를 일일 평균 구매량으로 변경\n","    test_payment_merge.rename(columns={'amount_spent':'mean_spent'}, inplace=True)\n","\n","    print('payment id unique and shape :')\n","    print(train_payment_merge['acc_id'].nunique(), train_payment_merge.shape)\n","    print(test_payment_merge['acc_id'].nunique(), test_payment_merge.shape)\n","    \n","    \n","    # merge combat \n","    print('\\nGrouping combat data ...')\n","    class_names = {0:'liege', 1:'knight', 2:'elf', 3:'magician', 4:'dark-elf', 5:'dragon',6:'illusionist',7:'warrior'}\n","    for elem in train_combat['class'].unique():train_combat[class_names[elem]] = train_combat['class'] == elem\n","    for elem in test_combat['class'].unique():test_combat[class_names[elem]] = test_combat['class'] == elem\n","    \n","    combat_agg = {'day':'nunique','char_id' : 'nunique', 'server' : 'nunique', 'class' : 'nunique', 'level' : 'max',\n","                  'pledge_cnt' : 'sum','random_attacker_cnt' : 'sum','random_defender_cnt' : 'sum', 'temp_cnt' : 'sum',\n","                  'same_pledge_cnt' : 'sum','etc_cnt' : 'sum','num_opponent' : 'sum','elf' : 'sum','magician' : 'sum',\n","                  'warrior' : 'sum','dark-elf' : 'sum','knight' : 'sum','dragon' : 'sum','liege' : 'sum','illusionist' : 'sum'}\n","        \n","    combat_rename = {'random_attacker_cnt':'combat_random_attacker_cnt','random_defender_cnt' : 'combat_random_defender_cnt',\n","                     'temp_cnt' : 'combat_temp_cnt','same_pledge_cnt' : 'combat_same_pledge_cnt','etc_cnt' : 'combat_etc_cnt'}\n","\n","    train_combat_merge = train_combat.groupby('acc_id', as_index = False).agg(combat_agg).reset_index(drop=True)\n","    test_combat_merge = test_combat.groupby('acc_id', as_index = False).agg(combat_agg).reset_index(drop=True)\n","  \n","    train_combat_merge.rename(columns=combat_rename, inplace=True)\n","    test_combat_merge.rename(columns=combat_rename, inplace=True)\n","\n","    print('combat id unique and shape :')\n","    print(train_combat_merge['acc_id'].nunique(), train_combat_merge.shape)\n","    print(test_combat_merge['acc_id'].nunique(), test_combat_merge.shape)\n","    \n","    \n","    # merge combat \n","    print('\\nGrouping pledge data ...')\n","    \n","    pledge_agg = {'day':'nunique','char_id':'nunique','server':'nunique','pledge_id':'nunique', 'play_char_cnt' : 'sum',\n","                  'combat_char_cnt' : 'sum','pledge_combat_cnt' : 'sum','random_attacker_cnt' : 'sum','random_defender_cnt' : 'sum',\n","                  'same_pledge_cnt' : 'sum', 'temp_cnt' : 'sum','etc_cnt' : 'sum','combat_play_time' : 'sum','non_combat_play_time' : 'sum'}\n","    \n","    train_pledge_merge = train_pledge.groupby('acc_id', as_index = False).agg(pledge_agg).reset_index(drop=True)\n","    test_pledge_merge = test_pledge.groupby('acc_id', as_index = False).agg(pledge_agg).reset_index(drop=True)\n","    \n","    print('pledge id unique and shape :')\n","    print(train_pledge_merge['acc_id'].nunique(), train_pledge_merge.shape)\n","    print(test_pledge_merge['acc_id'].nunique(), test_pledge_merge.shape)\n","    \n","    \n","    \n","    \n","    #merge all data\n","    print('\\nMerging all data ...')\n","    train_merge = train_label.merge(train_activity_merge, on = ['acc_id'], how ='left').merge(\n","    train_combat_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    train_pledge_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    train_payment_merge.drop(['day'], axis=1), on = ['acc_id'], how ='left').merge(\n","    train_source_trade, on = ['acc_id'], how = 'left').merge(\n","    train_target_trade, on = ['acc_id'], how = 'left')\n","        \n","    test_merge = test_activity_merge.merge(\n","    test_combat_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    test_pledge_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    test_payment_merge.drop(['day'], axis=1), on = ['acc_id'], how ='left').merge(\n","    test_source_trade, on = ['acc_id'], how = 'left').merge(\n","    test_target_trade, on = ['acc_id'], how = 'left')\n","    \n","    train_merge['attack'] = (train_merge['random_attacker_cnt']+train_merge['combat_random_attacker_cnt']) -(train_merge['random_defender_cnt'] + train_merge['combat_random_defender_cnt'])\n","    test_merge['attack'] = (test_merge['random_attacker_cnt']+test_merge['combat_random_attacker_cnt']) -(test_merge['random_defender_cnt'] + test_merge['combat_random_defender_cnt'])\n","    \n","    train_merge['cnt'] = train_merge['temp_cnt']+train_merge['same_pledge_cnt']+train_merge['etc_cnt'] + train_merge['combat_temp_cnt']+train_merge['combat_same_pledge_cnt']+train_merge['combat_etc_cnt']\n","    test_merge['cnt'] = test_merge['temp_cnt']+test_merge['same_pledge_cnt']+test_merge['etc_cnt'] + test_merge['combat_temp_cnt']+test_merge['combat_same_pledge_cnt']+test_merge['combat_etc_cnt']\n","    \n","    \n","    train_merge['pledge_ratio'] = train_merge['combat_char_cnt']/train_merge['play_char_cnt']\n","    test_merge['pledge_ratio'] = test_merge['combat_char_cnt']/test_merge['play_char_cnt']\n","  \n","    train_merge['day_spent'] = train_merge['day']*train_merge['mean_spent']\n","    test_merge['day_spent'] = test_merge['day']*test_merge['mean_spent']\n","    \n","    drop_merge = ['day','fishing','revive','elf','magician','warrior','dark-elf','knight','dragon','illusionist','class','random_attacker_cnt',\n","                  'random_defender_cnt','temp_cnt','same_pledge_cnt','etc_cnt','same_pledge_cnt','num_opponent','combat_temp_cnt', \n","                  'combat_same_pledge_cnt', 'combat_etc_cnt','combat_random_defender_cnt','combat_random_attacker_cnt','playtime',\n","                  'combat_char_cnt','play_char_cnt','mean_spent']\n","    \n","    \n","    \n","    train_merge = train_merge.drop(drop_merge, axis = 1)\n","    test_merge = test_merge.drop(drop_merge, axis = 1)\n","    \n","    print('\\nmerge id unique and shape :')\n","    print(train_merge['acc_id'].nunique(), train_merge.shape)\n","    print(test_merge['acc_id'].nunique(), test_merge.shape)\n","    \n","    train_merge.to_csv(submission_path + '/merge/' + 'train_merge.csv',index=False)  \n","    test_merge.to_csv(submission_path + '/merge/' + testname + '_merge.csv',index=False)  \n","    print('test and train merge data are saved on ' + submission_path + '/merge/' )\n","    \n","    \n","def _load_merge_data(): \n","\n","    train_merge = pd.read_csv(submission_path + 'merge/' + 'train_merge.csv')\n","    test_merge = pd.read_csv(submission_path + 'merge/' + testname + '_merge.csv')\n","  \n","    print('\\nmerge id unique and shape :')\n","    print(train_merge['acc_id'].nunique(), train_merge.shape)\n","    print(test_merge['acc_id'].nunique(), test_merge.shape)\n","\n","    \n","    # fill NA\n","#     fill_NA(train_merge)\n","    \n","    # encode features\n","#     encode_features(train_merge)\n","    \n","### STEP1.1 : prepare train and vaild data\n","\n","\n","\n","def _prepare_data():\n","    # prepare train and valid data\n","    print('\\nPreparing train and valid data ...')\n","\n","    global train_x\n","    global train_ya\n","    global train_ys\n","    \n","    global valid_x\n","    global valid_ya\n","    global valid_ys\n","    \n","    global test_x\n","    global test_acc_id\n","    \n","    \n","    drop_vars = ['acc_id','amount_spent','survival_time']\n","    \n","    train_ya = train_merge['amount_spent']\n","    train_ys = train_merge['survival_time']\n","    \n","    train_x = train_merge\n","    train_x.drop(columns=drop_vars, inplace=True)\n","    \n","    valid_x = train_x[30000:]\n","    valid_ya = train_ya[30000:]\n","    \n","    train_x = train_x[:30000]\n","    train_ya = train_ya[:30000]\n","    \n","    valid_ys = train_ys[30000:]\n","    train_ys = train_ys[:30000]\n","    \n","    print('train x shape: ', train_x.shape)\n","    print('train ya shape: ', train_ya.shape)\n","    print('train ys shape: ', train_ys.shape)\n","    \n","    print('valid x shape: ', valid_x.shape)\n","    print('valid ya shape: ', valid_ya.shape)\n","    print('valid ys shape: ', valid_ys.shape)\n","    \n","    \n","    # prepare test data\n","    print('\\nPreparing test data ...')\n","    \n","    test_x = test_merge\n","    test_acc_id = test_merge['acc_id']\n","    test_x.drop(columns=['acc_id'], inplace=True)\n","    print('test x shape: ', test_x.shape)\n","\n","    \n","## STEP2: build model\n","def _build_model():\n","    print('\\n\\nSTEP2: building model ...')\n","    \n","    global xgb_params\n","\n","    xgb_params = {\n","        'eta': 0.007,\n","        'max_depth': 8, \n","        'subsample': 0.8,\n","        'objective': 'reg:linear',\n","        'eval_metric': 'mae',\n","        'lambda': 9.0,\n","        'alpha': 0.8,\n","        'colsample_bytree': 0.7,\n","        'silent': 1,\n","        'random_state' : 42\n","    }\n","\n","    \n","\n","## STEP3: train    \n","def _train_predict_amount():\n","    print('\\n\\nSTEP3: training amount...')\n","    \n","    global a_xgb_clf\n","    global a_pred\n","    \n","    ad_train = xgb.DMatrix(train_x, label=train_ya)\n","    ad_valid = xgb.DMatrix(valid_x, label=valid_ya)\n","    \n","    a_evals = [(ad_train, 'train'), (ad_valid, 'valid')]\n","    a_xgb_clf = xgb.train(xgb_params, ad_train, num_boost_round=5000, evals=a_evals, \n","                        early_stopping_rounds=100, verbose_eval=100)\n","    \n","\n","    ad_test = xgb.DMatrix(test_x)\n","    a_pred = a_xgb_clf.predict(ad_test)\n","    \n","def _train_predict_survival():\n","    print('\\n\\nSTEP4: training survival...')\n","    \n","    global s_xgb_clf\n","    global s_pred\n","    \n","    sd_train = xgb.DMatrix(train_x, label=train_ys)\n","    sd_valid = xgb.DMatrix(valid_x, label=valid_ys)\n","    \n","    s_evals = [(sd_train, 'train'), (sd_valid, 'valid')]\n","    s_xgb_clf = xgb.train(xgb_params, sd_train, num_boost_round=5000, evals=s_evals, \n","                        early_stopping_rounds=100, verbose_eval=20)\n","\n","    sd_test = xgb.DMatrix(test_x)\n","    s_pred = s_xgb_clf.predict(sd_test)\n","    \n","\n","## STEP5: generate submission \n","def _generate_submission():\n","    print('\\n\\nSTEP5: generating submission ...')\n","    \n","    test_predict = pd.DataFrame(columns = ['acc_id', 'survival_time','amount_spent'])\n","    \n","    test_predict['acc_id'] = test_acc_id\n","    test_predict['survival_time'] = s_pred\n","    test_predict['amount_spent'] = a_pred\n","    \n","    test_predict['amount_spent'] = [0 if i < 0 else i for i in test_predict['amount_spent']] # 마이너스값 0 처리 \n","    test_predict['survival_time'] = [1 if i < 1 else i for i in test_predict['survival_time']] # 1 미만 값 1 처리\n","    \n","    #testname에 따라 amountspent value 키우기\n","    if testname == 'test1' :\n","        test_predict['amount_spent'] = test_predict['amount_spent']*18 #best : 20\n","    elif testname == 'test2' :\n","        test_predict['amount_spent'] = test_predict['amount_spent']*5\n","    else :\n","        print('testname is wrong!')\n","        \n","    test_predict.to_csv(submission_path + testname + '_predict.csv',index=False)    \n"," "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6GbCU8BXFncQ","colab_type":"text"},"source":["#### ver1.6 - Day 변수 추가 (login_day_max, logout_day)"]},{"cell_type":"code","metadata":{"id":"qe5w8ts5FsT9","colab_type":"code","colab":{}},"source":["\n","# -*- coding: utf-8 -*-\n","\n","\n","from enum import Enum\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","from sklearn import preprocessing\n","import xgboost as xgb\n","from datetime import datetime\n","import gc\n","\n","import warnings\n","def ignore_warn(*args, **kwargs):\n","    pass\n","warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n","\n","\n","## STEP0: do setting\n","class Settings(Enum):\n","    global train_path\n","    global test_path\n","    global submission_path\n","    global testname\n","    global codeversion\n","    \n","    codeversion = 'ver1.6 - Day 변수 추가 (login_day_max, logout_day)'\n","    \n","    path            = 'drive/My Drive/bigcontest2019/'\n","    train_path      = path + 'data/train/'\n","    test_path       = path + 'data/test/'\n","    submission_path = path + 'scripts/model/metrics/nes_inference/'\n","        \n","    def __str__(self):\n","        return self.value\n","        \n","    \n","## STEP1: process data    \n","def fill_NA(df):\n","    print('\\nFilling NA ...')\n","    \n","    na_ratio = ((df.isnull().sum() / len(df)) * 100).sort_values(ascending=False)\n","    print('NA ratio: ')\n","    print(na_ratio) \n","    \n","    for feature in df:\n","        if df[feature].dtype == 'object':\n","            df[feature] = df[feature].fillna(\"None\")\n","        else:\n","            df[feature] = df[feature].fillna(0)\n","    \n","def encode_features(df):\n","    print('\\nEncoding features ...')\n","    \n","    for feature in df:\n","        if df[feature].dtype == 'object':\n","            print('Encoding ', feature)\n","            lbl = preprocessing.LabelEncoder()\n","            lbl.fit(list(df[feature].values))\n","            df[feature] = lbl.transform(list(df[feature].values))\n","    \n","# def display_feature_target(df, feature):\n","#     fig, ax = plt.subplots()\n","#     ax.scatter(x = df[feature], y = df['logerror'])\n","#     plt.ylabel('logerror', fontsize=13)\n","#     plt.xlabel(feature, fontsize=13)\n","#     plt.show()\n","    \n","def _process_data():\n","    \n","    print('CODE VERSION : ', codeversion)\n","    print('\\n\\nSTEP1: processing data ...')\n","    \n","    global train_merge\n","    global test_merge\n","\n","        \n","    # load data\n","    print('\\nLoading data ...')\n","    \n","    train_label = pd.read_csv(train_path + 'train_label.csv')\n","    train_activity = pd.read_csv(train_path + 'train_activity.csv')\n","    train_combat = pd.read_csv(train_path + 'train_combat.csv')\n","    train_payment = pd.read_csv(train_path + 'train_payment.csv')\n","    train_pledge = pd.read_csv(train_path + 'train_pledge.csv')\n","    train_trade = pd.read_csv(train_path + 'train_trade.csv')\n","\n","    test_activity = pd.read_csv(test_path + testname + '_activity.csv')\n","    test_combat = pd.read_csv(test_path + testname + '_combat.csv')\n","    test_payment = pd.read_csv(test_path + testname + '_payment.csv')\n","    test_pledge = pd.read_csv(test_path + testname + '_pledge.csv')\n","    test_trade = pd.read_csv(test_path  + testname + '_trade.csv')\n","\n","    \n","    # merge payment\n","    print('\\nGrouping activity data ...')\n","\n","    train_activity['game_money_change_minus'] = train_activity['game_money_change'] #game_money_change를 마이너스끼리, 플러스끼리 분리\n","    train_activity['game_money_change_minus'] = [0 if i > 0 else i for i in train_activity['game_money_change_minus']]\n","    train_activity['game_money_change'] = [0 if i < 0 else i for i in train_activity['game_money_change']]\n","\n","    test_activity['game_money_change_minus'] = test_activity['game_money_change']\n","    test_activity['game_money_change_minus'] = [0 if i > 0 else i for i in test_activity['game_money_change_minus']]\n","    test_activity['game_money_change'] = [0 if i < 0 else i for i in test_activity['game_money_change']]\n","\n","    activity_agg = {\n","        'day':'nunique', 'char_id':'nunique','server':'nunique', 'playtime':'sum', 'npc_kill':'sum', \n","        'solo_exp':'sum','party_exp':'sum','quest_exp':'sum','rich_monster':'sum', \n","        'death':'sum', 'revive':'sum','exp_recovery':'sum','fishing':'sum','private_shop':'sum',\n","        'game_money_change':'sum', 'game_money_change_minus':'sum','enchant_count':'sum'}\n","    \n","    train_activity_merge = train_activity.groupby('acc_id', as_index = False).agg(activity_agg).reset_index(drop=True)\n","    test_activity_merge = test_activity.groupby('acc_id', as_index = False).agg(activity_agg).reset_index(drop=True)\n","    \n","    acc_id_list = train_activity['acc_id'].unique().tolist()\n","    login_day_max = []\n","    logout_day = []\n","    \n","    print('\\n + making day variables for train activity')\n","\n","    for i in acc_id_list :\n","        dayunique = train_activity[train_activity['acc_id'] == i]['day'].unique().tolist() # i아이디의 접속일 list화\n","        in_list = [0] * 28\n","        in_value = 0\n","        daylogin = 1\n","\n","        out_list = [0] * 28\n","        out_value = 0\n","        day_logout = 0\n","\n","        # login_day_max 변수 생성 (acc_id가 28일 중 최대 연속으로 접속한 일 수. 띄엄띄엄 접속자보다 매일 꾸준히 하는 사람 데이터를 모으고 싶었음)\n","        for j in range(0, len(dayunique)-1) :        \n","            if dayunique[j]+1 == dayunique[j+1] : # 전날 대비 연속 접속한 경우에 연속값 카운트\n","                daylogin += 1 \n","                in_list[in_value] = daylogin  # 연속값 저장\n","            else :\n","                in_value += 1 \n","                daylogin = 1 # 이전 접속일과 다음 접속일이 1이상 차이날 때, 연속값 초기화\n","        login_day_max.append(max(in_list)) # 연속 접속일 리스트 중 최대값 append\n","\n","         #logout-day 변수 생성 (28일 중 후반부에만 로그인 하는 사람들과, 초반 로그인 후 접속하지 않는 유저를 구분하기 위함)\n","        for k in range(0, len(dayunique)-1) :\n","            if dayunique[k]+1 != dayunique[k+1] : #연속 접속하지 않은 경우, 다음 접속일까지 로그아웃한 일자 카운트\n","                day_logout = (dayunique[k+1]-dayunique[k]-1)\n","                out_list[out_value] = day_logout\n","            else :\n","                out_value += 1\n","                day_logout = 0 #접속한 경우 pass 및 연속값 초기화\n","        logout_day.append(max(out_list)) \n","\n","    all_day = pd.DataFrame(columns = ['acc_id', 'login_day_max','logout_day']) # 빈프레임에 day 관련 데이터 2개 추가\n","    all_day['acc_id'] = acc_id_list\n","    all_day['login_day_max'] = login_day_max\n","    all_day['logout_day'] = logout_day\n","\n","    train_activity_merge = train_activity_merge.merge(all_day, on = ['acc_id'], how ='left')\n","    \n","    print('\\n + making day variables for test activity')\n","\n","    acc_id_list = test_activity['acc_id'].unique().tolist()\n","    login_day_max = []\n","    logout_day = []\n","\n","    for i in acc_id_list :\n","        dayunique = test_activity[test_activity['acc_id'] == i]['day'].unique().tolist() # i아이디의 접속일 list화\n","        in_list = [0] * 28\n","        in_value = 0\n","        daylogin = 1\n","\n","        out_list = [0] * 28\n","        out_value = 0\n","        day_logout = 0\n","\n","        # login_day_max 변수 생성 (acc_id가 28일 중 최대 연속으로 접속한 일 수. 띄엄띄엄 접속자보다 매일 꾸준히 하는 사람 데이터를 모으고 싶었음)\n","        for j in range(0, len(dayunique)-1) :        \n","            if dayunique[j]+1 == dayunique[j+1] : # 전날 대비 연속 접속한 경우에 연속값 카운트\n","                daylogin += 1 \n","                in_list[in_value] = daylogin  # 연속값 저장\n","            else :\n","                in_value += 1 \n","                daylogin = 1 # 이전 접속일과 다음 접속일이 1이상 차이날 때, 연속값 초기화\n","        login_day_max.append(max(in_list)) # 연속 접속일 리스트 중 최대값 append\n","\n","         #logout-day 변수 생성 (28일 중 후반부에만 로그인 하는 사람들과, 초반 로그인 후 접속하지 않는 유저를 구분하기 위함)\n","        for k in range(0, len(dayunique)-1) :\n","            if dayunique[k]+1 != dayunique[k+1] : #연속 접속하지 않은 경우, 다음 접속일까지 로그아웃한 일자 카운트\n","                day_logout = (dayunique[k+1]-dayunique[k]-1)\n","                out_list[out_value] = day_logout\n","            else :\n","                out_value += 1\n","                day_logout = 0 #접속한 경우 pass 및 연속값 초기화\n","        logout_day.append(max(out_list)) \n","\n","    all_day = pd.DataFrame(columns = ['acc_id', 'login_day_max','logout_day']) # 빈프레임에 day 관련 데이터 2개 추가\n","    all_day['acc_id'] = acc_id_list\n","    all_day['login_day_max'] = login_day_max\n","    all_day['logout_day'] = logout_day\n","\n","    test_activity_merge = test_activity_merge.merge(all_day, on = ['acc_id'], how ='left')\n","    \n","    \n","    print('activity id unique and shape :')\n","    print(train_activity_merge['acc_id'].nunique(), train_activity_merge.shape)\n","    print(test_activity_merge['acc_id'].nunique(), test_activity_merge.shape)\n","    \n","    \n","    # merge payment\n","    print('\\nGrouping payment data ...')\n","    \n","    train_payment_merge = train_payment.groupby( 'acc_id', as_index = False).agg({'day':'nunique','amount_spent':'max'}).reset_index(drop=True)\n","    test_payment_merge = test_payment.groupby('acc_id', as_index = False).agg({'day':'nunique','amount_spent':'max'}).reset_index(drop=True)\n","\n","    train_payment_merge.rename(columns={'amount_spent':'max_spent'}, inplace=True) #amout_spent를 일일 최고 구매량으로 변경\n","    test_payment_merge.rename(columns={'amount_spent':'max_spent'}, inplace=True)\n","\n","    print('payment id unique and shape :')\n","    print(train_payment_merge['acc_id'].nunique(), train_payment_merge.shape)\n","    print(test_payment_merge['acc_id'].nunique(), test_payment_merge.shape)\n","    \n","    \n","    # merge combat \n","    print('\\nGrouping combat data ...')\n","    class_names = {0:'liege', 1:'knight', 2:'elf', 3:'magician', 4:'dark-elf', 5:'dragon',6:'illusionist',7:'warrior'}\n","    for elem in train_combat['class'].unique():train_combat[class_names[elem]] = train_combat['class'] == elem\n","    for elem in test_combat['class'].unique():test_combat[class_names[elem]] = test_combat['class'] == elem\n","    \n","    combat_agg = {'day':'nunique','char_id' : 'nunique', 'server' : 'nunique', 'class' : 'nunique', 'level' : 'max',\n","                  'pledge_cnt' : 'sum','random_attacker_cnt' : 'sum','random_defender_cnt' : 'sum', 'temp_cnt' : 'sum',\n","                  'same_pledge_cnt' : 'sum','etc_cnt' : 'sum','num_opponent' : 'sum','elf' : 'sum','magician' : 'sum',\n","                  'warrior' : 'sum','dark-elf' : 'sum','knight' : 'sum','dragon' : 'sum','liege' : 'sum','illusionist' : 'sum'}\n","        \n","    combat_rename = {'random_attacker_cnt':'combat_random_attacker_cnt','random_defender_cnt' : 'combat_random_defender_cnt',\n","                     'temp_cnt' : 'combat_temp_cnt','same_pledge_cnt' : 'combat_same_pledge_cnt','etc_cnt' : 'combat_etc_cnt'}\n","\n","    train_combat_merge = train_combat.groupby('acc_id', as_index = False).agg(combat_agg).reset_index(drop=True)\n","    test_combat_merge = test_combat.groupby('acc_id', as_index = False).agg(combat_agg).reset_index(drop=True)\n","  \n","    train_combat_merge.rename(columns=combat_rename, inplace=True)\n","    test_combat_merge.rename(columns=combat_rename, inplace=True)\n","\n","    print('combat id unique and shape :')\n","    print(train_combat_merge['acc_id'].nunique(), train_combat_merge.shape)\n","    print(test_combat_merge['acc_id'].nunique(), test_combat_merge.shape)\n","    \n","    \n","    # merge combat \n","    print('\\nGrouping pledge data ...')\n","    \n","    pledge_agg = {'day':'nunique','char_id':'nunique','server':'nunique','pledge_id':'nunique', 'play_char_cnt' : 'sum',\n","                  'combat_char_cnt' : 'sum','pledge_combat_cnt' : 'sum','random_attacker_cnt' : 'sum','random_defender_cnt' : 'sum',\n","                  'same_pledge_cnt' : 'sum', 'temp_cnt' : 'sum','etc_cnt' : 'sum','combat_play_time' : 'sum','non_combat_play_time' : 'sum'}\n","    \n","    train_pledge_merge = train_pledge.groupby('acc_id', as_index = False).agg(pledge_agg).reset_index(drop=True)\n","    test_pledge_merge = test_pledge.groupby('acc_id', as_index = False).agg(pledge_agg).reset_index(drop=True)\n","    \n","    print('pledge id unique and shape :')\n","    print(train_pledge_merge['acc_id'].nunique(), train_pledge_merge.shape)\n","    print(test_pledge_merge['acc_id'].nunique(), test_pledge_merge.shape)\n","    \n","    \n","    # merge trade \n","    print('\\nGrouping trade data ...')\n","\n","    # item_type dummies             \n","    train_item_type_dummy = pd.get_dummies(train_trade['item_type'])\n","    test_item_type_dummy = pd.get_dummies(test_trade['item_type'])\n","\n","    # trade_type dummies\n","    type_names = {0:'personal_shop', 1:'trade_shop'}\n","    for elem in train_trade['type'].unique():train_trade[type_names[elem]] = train_trade['type'] == elem\n","    for elem in test_trade['type'].unique():test_trade[type_names[elem]] = test_trade['type'] == elem\n","\n","    # merge dummies\n","    \n","    train_trade = train_trade.drop(columns=['type','item_type'])\n","    train_trade = pd.concat([train_trade, train_item_type_dummy], axis = 1)\n","    \n","    test_trade = test_trade.drop(columns=['type','item_type'])\n","    test_trade = pd.concat([test_trade, test_item_type_dummy], axis = 1)\n","\n","    # rename columns\n","    trade_rename = {'day':'trade_day','server' : 'trade_server', 'time' : 'trade_time'}\n","    train_trade.rename(columns=trade_rename, inplace=True)\n","    test_trade.rename(columns=trade_rename, inplace=True)\n","\n","    #divide source and target id\n","    \n","    train_source_trade = train_trade.drop(columns = ['target_acc_id','source_char_id','target_char_id']).rename(columns={'source_acc_id' : 'acc_id'})\n","    train_source_trade['source'] = 1\n","    train_target_trade = train_trade.drop(columns = ['source_acc_id','source_char_id','target_char_id']).rename(columns={'target_acc_id' : 'acc_id'})\n","    train_target_trade['target'] = 1\n","    \n","    test_source_trade = test_trade.drop(columns = ['target_acc_id','source_char_id','target_char_id']).rename(columns={'source_acc_id' : 'acc_id'})\n","    test_source_trade['source'] = 1\n","    test_target_trade = test_trade.drop(columns = ['source_acc_id','source_char_id','target_char_id']).rename(columns={'target_acc_id' : 'acc_id'})\n","    test_target_trade['target'] = 1\n","\n","\n","    #aggregate\n","    trade_source_agg = {'trade_day':'nunique', 'trade_time' : 'nunique', 'trade_server' : 'nunique','item_amount' : 'sum',\n","                        'item_price': 'sum', 'trade_shop' : 'sum', 'personal_shop' : 'sum', 'accessory' : 'sum', 'adena' : 'sum',\n","                        'armor' : 'sum', 'enchant_scroll' : 'sum', 'etc' : 'sum', 'spell' : 'sum', 'weapon' : 'sum', 'source' : 'sum'}\n","\n","    trade_target_agg = {'trade_day':'nunique', 'trade_time' : 'nunique', 'trade_server' : 'nunique','item_amount' : 'sum',\n","                        'item_price': 'sum', 'trade_shop' : 'sum', 'personal_shop' : 'sum', 'accessory' : 'sum', 'adena' : 'sum',\n","                        'armor' : 'sum', 'enchant_scroll' : 'sum', 'etc' : 'sum', 'spell' : 'sum', 'weapon' : 'sum', 'target' : 'sum'}\n","\n","    train_source_trade = train_source_trade.groupby('acc_id', as_index = False).agg(trade_source_agg).reset_index(drop=True)\n","    train_target_trade = train_target_trade.groupby('acc_id', as_index = False).agg(trade_target_agg).reset_index(drop=True)\n","    \n","    test_source_trade = test_source_trade.groupby('acc_id', as_index = False).agg(trade_source_agg).reset_index(drop=True)\n","    test_target_trade = test_target_trade.groupby('acc_id', as_index = False).agg(trade_target_agg).reset_index(drop=True)\n","\n","    print('trade id unique and shape :')\n","    print(train_source_trade['acc_id'].nunique(), train_source_trade.shape, train_target_trade['acc_id'].nunique(), train_target_trade.shape)\n","    print(test_source_trade['acc_id'].nunique(), test_source_trade.shape, test_target_trade['acc_id'].nunique(), test_target_trade.shape)\n","    \n","    \n","    \n","    #merge all data\n","    print('\\nMerging all data ...')\n","    train_merge = train_label.merge(train_activity_merge, on = ['acc_id'], how ='left').merge(\n","    train_combat_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    train_pledge_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    train_payment_merge.drop(['day'], axis=1), on = ['acc_id'], how ='left').merge(\n","    train_source_trade, on = ['acc_id'], how = 'left').merge(\n","    train_target_trade, on = ['acc_id'], how = 'left')\n","        \n","    test_merge = test_activity_merge.merge(\n","    test_combat_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    test_pledge_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    test_payment_merge.drop(['day'], axis=1), on = ['acc_id'], how ='left').merge(\n","    test_source_trade, on = ['acc_id'], how = 'left').merge(\n","    test_target_trade, on = ['acc_id'], how = 'left')\n","    \n","    print('\\nmerge id unique and shape :')\n","    print(train_merge['acc_id'].nunique(), train_merge.shape)\n","    print(test_merge['acc_id'].nunique(), test_merge.shape)\n","    \n","    \n","    train_merge.to_csv(submission_path + '/merge/' + 'train_merge.csv',index=False)  \n","    test_merge.to_csv(submission_path + '/merge/' + testname + '_merge.csv',index=False)  \n","    print('test and train merge data are saved on ' + submission_path + '/merge/' )\n","    \n","    \n","def _load_merge_data(): \n","\n","    train_merge = pd.read_csv(submission_path + 'merge/' + 'train_merge.csv')\n","    test_merge = pd.read_csv(submission_path + 'merge/' + testname + '_merge.csv')\n","    \n","    train_merge['cnt'] = train_merge['random_attacker_cnt']+train_merge['random_defender_cnt']+train_merge['temp_cnt']+train_merge['same_pledge_cnt']+train_merge['etc_cnt']\n","    test_merge['cnt'] = test_merge['random_attacker_cnt']+test_merge['random_defender_cnt']+test_merge['temp_cnt']+test_merge['same_pledge_cnt']+test_merge['etc_cnt']\n","    \n","    train_merge = train_merge.drop(['day','fishing','revive','elf','magician','warrior','dark-elf','knight','dragon','illusionist','class',\n","                     'random_attacker_cnt','random_defender_cnt','temp_cnt','same_pledge_cnt','etc_cnt'], axis = 1)\n","    test_merge = test_merge.drop(['day','fishing','revive','elf','magician','warrior','dark-elf','knight','dragon','illusionist','class',\n","                     'random_attacker_cnt','random_defender_cnt','temp_cnt','same_pledge_cnt','etc_cnt'], axis = 1)\n","       \n","    print('test and train merge data are loaded on ' + submission_path + 'merge/' )\n","    \n","    # fill NA\n","#     fill_NA(train_merge)\n","    \n","    # encode features\n","#     encode_features(train_merge)\n","    \n","### STEP1.1 : prepare train and vaild data\n","\n","def _prepare_data():\n","    # prepare train and valid data\n","    print('\\nPreparing train and valid data ...')\n","\n","    global train_x\n","    global train_ya\n","    global train_ys\n","    \n","    global valid_x\n","    global valid_ya\n","    global valid_ys\n","    \n","    global test_x\n","    global test_acc_id\n","    \n","    \n","    drop_vars = ['acc_id','amount_spent','survival_time']\n","    \n","    train_ya = train_merge['amount_spent']\n","    train_ys = train_merge['survival_time']\n","    \n","    train_x = train_merge\n","    train_x.drop(columns=drop_vars, inplace=True)\n","    \n","    valid_x = train_x[30000:]\n","    valid_ya = train_ya[30000:]\n","    \n","    train_x = train_x[:30000]\n","    train_ya = train_ya[:30000]\n","    \n","    valid_ys = train_ys[30000:]\n","    train_ys = train_ys[:30000]\n","    \n","    print('train x shape: ', train_x.shape)\n","    print('train ya shape: ', train_ya.shape)\n","    print('train ys shape: ', train_ys.shape)\n","    \n","    print('valid x shape: ', valid_x.shape)\n","    print('valid ya shape: ', valid_ya.shape)\n","    print('valid ys shape: ', valid_ys.shape)\n","    \n","    \n","    # prepare test data\n","    print('\\nPreparing test data ...')\n","    \n","    test_x = test_merge\n","    test_acc_id = test_merge['acc_id']\n","    test_x.drop(columns=['acc_id'], inplace=True)\n","    print('test x shape: ', test_x.shape)\n","\n","    \n","## STEP2: build model\n","def _build_model():\n","    print('\\n\\nSTEP2: building model ...')\n","    \n","    global xgb_params\n","\n","    xgb_params = {\n","        'eta': 0.007,\n","        'max_depth': 8, \n","        'subsample': 0.8,\n","        'objective': 'reg:linear',\n","        'eval_metric': 'mae',\n","        'lambda': 9.0,\n","        'alpha': 0.8,\n","        'colsample_bytree': 0.7,\n","        'silent': 1,\n","        'random_state' : 42\n","    }\n","\n","    \n","\n","## STEP3: train    \n","def _train_predict_amount():\n","    print('\\n\\nSTEP3: training amount...')\n","    \n","    global a_xgb_clf\n","    global a_pred\n","    \n","    ad_train = xgb.DMatrix(train_x, label=train_ya)\n","    ad_valid = xgb.DMatrix(valid_x, label=valid_ya)\n","    \n","    a_evals = [(ad_train, 'train'), (ad_valid, 'valid')]\n","    a_xgb_clf = xgb.train(xgb_params, ad_train, num_boost_round=10000, evals=a_evals, \n","                        early_stopping_rounds=100, verbose_eval=20)\n","    \n","\n","    ad_test = xgb.DMatrix(test_x)\n","    a_pred = a_xgb_clf.predict(ad_test)\n","    \n","def _train_predict_survival():\n","    print('\\n\\nSTEP4: training survival...')\n","    \n","    global s_xgb_clf\n","    global s_pred\n","    \n","    sd_train = xgb.DMatrix(train_x, label=train_ys)\n","    sd_valid = xgb.DMatrix(valid_x, label=valid_ys)\n","    \n","    s_evals = [(sd_train, 'train'), (sd_valid, 'valid')]\n","    s_xgb_clf = xgb.train(xgb_params, sd_train, num_boost_round=10000, evals=s_evals, \n","                        early_stopping_rounds=100, verbose_eval=20)\n","\n","    sd_test = xgb.DMatrix(test_x)\n","    s_pred = s_xgb_clf.predict(sd_test)\n","    \n","\n","## STEP5: generate submission \n","def _generate_submission():\n","    print('\\n\\nSTEP5: generating submission ...')\n","    \n","    test_predict = pd.DataFrame(columns = ['acc_id', 'survival_time','amount_spent'])\n","    \n","    test_predict['acc_id'] = test_acc_id\n","    test_predict['survival_time'] = s_pred\n","    test_predict['amount_spent'] = a_pred\n","    \n","    test_predict['amount_spent'] = [0 if i < 0 else i for i in test_predict['amount_spent']] # 마이너스값 0 처리 \n","    test_predict['survival_time'] = [1 if i < 1 else i for i in test_predict['survival_time']] # 1 미만 값 1 처리\n","    \n","    #testname에 따라 amountspent value 키우기\n","    if testname == 'test1' :\n","        test_predict['amount_spent'] = test_predict['amount_spent']*18 #best : 20\n","    elif testname == 'test2' :\n","        test_predict['amount_spent'] = test_predict['amount_spent']*5\n","    else :\n","        print('testname is wrong!')\n","        \n","    test_predict.to_csv(submission_path + testname + '_predict.csv',index=False)    \n"," "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XGVqAt6_ezUO","colab_type":"text"},"source":["### history code - DO NOT MODIFY"]},{"cell_type":"markdown","metadata":{"id":"9ds9iDQIGGOx","colab_type":"text"},"source":["#### ver1.5 CODE - xgb with gridsearchCV (-50000점^^;)"]},{"cell_type":"code","metadata":{"id":"pSeyB5HYGM91","colab_type":"code","colab":{}},"source":["#ref : https://www.kaggle.com/cuongvn08/xgboost-cross-validation-clean-coding-lb-0-64x\n","\n","# -*- coding: utf-8 -*-\n","\n","from enum import Enum\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.metrics import mean_squared_error\n","from sklearn import preprocessing\n","\n","import xgboost as xgb\n","from xgboost import XGBRegressor\n","import lightgbm as lgb\n","\n","from datetime import datetime\n","import gc\n","\n","\n","from math import sqrt\n","\n","\n","import warnings\n","def ignore_warn(*args, **kwargs):\n","    pass\n","warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n","\n","\n","## STEP0: do setting\n","class Settings(Enum):\n","    global train_path\n","    global test_path\n","    global submission_path\n","    global testname\n","    \n","    testname = 'test1'\n","    \n","    path            = 'drive/My Drive/bigcontest2019/'\n","    train_path      = path + 'data/train/'\n","    test_path       = path + 'data/test/'\n","    submission_path = path + 'scripts/model/metrics/nes_inference/'\n","        \n","    def __str__(self):\n","        return self.value\n","        \n","    \n","# def display_feature_target(df, feature):\n","#     fig, ax = plt.subplots()\n","#     ax.scatter(x = df[feature], y = df['logerror'])\n","#     plt.ylabel('logerror', fontsize=13)\n","#     plt.xlabel(feature, fontsize=13)\n","#     plt.show()\n","    \n","def _process_data():\n","    print('\\n\\nSTEP1: processing data ...')\n","    \n","    global train_merge\n","    global test_merge\n","\n","        \n","    # load data\n","    print('\\nLoading data ...')\n","    \n","    train_label = pd.read_csv(train_path + 'train_label.csv')\n","    train_activity = pd.read_csv(train_path + 'train_activity.csv')\n","    train_combat = pd.read_csv(train_path + 'train_combat.csv')\n","    train_payment = pd.read_csv(train_path + 'train_payment.csv')\n","    train_pledge = pd.read_csv(train_path + 'train_pledge.csv')\n","    train_trade = pd.read_csv(train_path + 'train_trade.csv')\n","\n","    test_activity = pd.read_csv(test_path + testname + '_activity.csv')\n","    test_combat = pd.read_csv(test_path + testname + '_combat.csv')\n","    test_payment = pd.read_csv(test_path + testname + '_payment.csv')\n","    test_pledge = pd.read_csv(test_path + testname + '_pledge.csv')\n","    test_trade = pd.read_csv(test_path  + testname + '_trade.csv')\n","\n","    \n","    # merge payment\n","    print('\\nGrouping activity data ...')\n","\n","    train_activity['game_money_change_minus'] = train_activity['game_money_change'] #game_money_change를 마이너스끼리, 플러스끼리 분리\n","    train_activity['game_money_change_minus'] = [0 if i > 0 else i for i in train_activity['game_money_change_minus']]\n","    train_activity['game_money_change'] = [0 if i < 0 else i for i in train_activity['game_money_change']]\n","\n","    test_activity['game_money_change_minus'] = test_activity['game_money_change']\n","    test_activity['game_money_change_minus'] = [0 if i > 0 else i for i in test_activity['game_money_change_minus']]\n","    test_activity['game_money_change'] = [0 if i < 0 else i for i in test_activity['game_money_change']]\n","\n","    activity_agg = {\n","        'day':'nunique', 'char_id':'nunique','server':'nunique', 'playtime':'sum', 'npc_kill':'sum', \n","        'solo_exp':'sum','party_exp':'sum','quest_exp':'sum','rich_monster':'sum', \n","        'death':'sum', 'revive':'sum','exp_recovery':'sum','fishing':'sum','private_shop':'sum',\n","        'game_money_change':'sum', 'game_money_change_minus':'sum','enchant_count':'sum'}\n","    \n","    train_activity_merge = train_activity.groupby('acc_id', as_index = False).agg(activity_agg).reset_index(drop=True)\n","    test_activity_merge = test_activity.groupby('acc_id', as_index = False).agg(activity_agg).reset_index(drop=True)\n","    \n","    print('activity id unique and shape :')\n","    print(train_activity_merge['acc_id'].nunique(), train_activity_merge.shape)\n","    print(test_activity_merge['acc_id'].nunique(), test_activity_merge.shape)\n","    \n","    \n","    # merge payment\n","    print('\\nGrouping payment data ...')\n","    \n","    train_payment_merge = train_payment.groupby( 'acc_id', as_index = False).agg({'day':'nunique','amount_spent':'max'}).reset_index(drop=True)\n","    test_payment_merge = test_payment.groupby('acc_id', as_index = False).agg({'day':'nunique','amount_spent':'max'}).reset_index(drop=True)\n","\n","    train_payment_merge.rename(columns={'amount_spent':'max_spent'}, inplace=True) #amout_spent를 일일 최고 구매량인 max로 변경\n","    test_payment_merge.rename(columns={'amount_spent':'max_spent'}, inplace=True)\n","\n","    print('payment id unique and shape :')\n","    print(train_payment_merge['acc_id'].nunique(), train_payment_merge.shape)\n","    print(test_payment_merge['acc_id'].nunique(), test_payment_merge.shape)\n","    \n","    \n","    # merge combat \n","    print('\\nGrouping combat data ...')\n","    class_names = {0:'liege', 1:'knight', 2:'elf', 3:'magician', 4:'dark-elf', 5:'dragon',6:'illusionist',7:'warrior'}\n","    for elem in train_combat['class'].unique():train_combat[class_names[elem]] = train_combat['class'] == elem\n","    for elem in test_combat['class'].unique():test_combat[class_names[elem]] = test_combat['class'] == elem\n","    \n","    combat_agg = {'day':'nunique','char_id' : 'nunique', 'server' : 'nunique', 'class' : 'nunique', 'level' : 'max',\n","                  'pledge_cnt' : 'sum','random_attacker_cnt' : 'sum','random_defender_cnt' : 'sum', 'temp_cnt' : 'sum',\n","                  'same_pledge_cnt' : 'sum','etc_cnt' : 'sum','num_opponent' : 'sum','elf' : 'sum','magician' : 'sum',\n","                  'warrior' : 'sum','dark-elf' : 'sum','knight' : 'sum','dragon' : 'sum','liege' : 'sum','illusionist' : 'sum'}\n","        \n","    combat_rename = {'random_attacker_cnt':'combat_random_attacker_cnt','random_defender_cnt' : 'combat_random_defender_cnt',\n","                     'temp_cnt' : 'combat_temp_cnt','same_pledge_cnt' : 'combat_same_pledge_cnt','etc_cnt' : 'combat_etc_cnt'}\n","\n","    train_combat_merge = train_combat.groupby('acc_id', as_index = False).agg(combat_agg).reset_index(drop=True)\n","    test_combat_merge = test_combat.groupby('acc_id', as_index = False).agg(combat_agg).reset_index(drop=True)\n","  \n","    train_combat_merge.rename(columns=combat_rename, inplace=True) #combat와 pledge value구분\n","    test_combat_merge.rename(columns=combat_rename, inplace=True)\n","\n","    print('combat id unique and shape :')\n","    print(train_combat_merge['acc_id'].nunique(), train_combat_merge.shape)\n","    print(test_combat_merge['acc_id'].nunique(), test_combat_merge.shape)\n","    \n","    \n","    # merge combat \n","    print('\\nGrouping pledge data ...')\n","    \n","    pledge_agg = {'day':'nunique','char_id':'nunique','server':'nunique','pledge_id':'nunique', 'play_char_cnt' : 'sum',\n","                  'combat_char_cnt' : 'sum','pledge_combat_cnt' : 'sum','random_attacker_cnt' : 'sum','random_defender_cnt' : 'sum',\n","                  'same_pledge_cnt' : 'sum', 'temp_cnt' : 'sum','etc_cnt' : 'sum','combat_play_time' : 'sum','non_combat_play_time' : 'sum'}\n","    \n","    train_pledge_merge = train_pledge.groupby('acc_id', as_index = False).agg(pledge_agg).reset_index(drop=True) \n","    test_pledge_merge = test_pledge.groupby('acc_id', as_index = False).agg(pledge_agg).reset_index(drop=True)\n","    \n","    print('pledge id unique and shape :')\n","    print(train_pledge_merge['acc_id'].nunique(), train_pledge_merge.shape)\n","    print(test_pledge_merge['acc_id'].nunique(), test_pledge_merge.shape)\n","    \n","    \n","    # merge trade \n","    print('\\nGrouping trade data ...')\n","\n","    # item_type dummies             \n","    train_item_type_dummy = pd.get_dummies(train_trade['item_type']) #item 더미변수 생성\n","    test_item_type_dummy = pd.get_dummies(test_trade['item_type'])\n","\n","    # trade_type dummies\n","    type_names = {0:'personal_shop', 1:'trade_shop'}  #personal_shop, trade_shop구분하는 더미변수 생성\n","    for elem in train_trade['type'].unique():train_trade[type_names[elem]] = train_trade['type'] == elem\n","    for elem in test_trade['type'].unique():test_trade[type_names[elem]] = test_trade['type'] == elem\n","\n","    # merge dummies\n","    \n","    train_trade = train_trade.drop(columns=['type','item_type'])\n","    train_trade = pd.concat([train_trade, train_item_type_dummy], axis = 1)\n","    \n","    test_trade = test_trade.drop(columns=['type','item_type'])\n","    test_trade = pd.concat([test_trade, test_item_type_dummy], axis = 1)\n","\n","    # rename columns\n","    trade_rename = {'day':'trade_day','server' : 'trade_server', 'time' : 'trade_time'} #day, server, time 구분\n","    train_trade.rename(columns=trade_rename, inplace=True)\n","    test_trade.rename(columns=trade_rename, inplace=True)\n","\n","    #divide source and target id, source 및 target 횟수 카운트 할 수 있도록 변수 생성 후 merge\n","    \n","    train_source_trade = train_trade.drop(columns = ['target_acc_id','source_char_id','target_char_id']).rename(columns={'source_acc_id' : 'acc_id'})\n","    train_source_trade['source'] = 1\n","    train_target_trade = train_trade.drop(columns = ['source_acc_id','source_char_id','target_char_id']).rename(columns={'target_acc_id' : 'acc_id'})\n","    train_target_trade['target'] = 1\n","    \n","    test_source_trade = test_trade.drop(columns = ['target_acc_id','source_char_id','target_char_id']).rename(columns={'source_acc_id' : 'acc_id'})\n","    test_source_trade['source'] = 1\n","    test_target_trade = test_trade.drop(columns = ['source_acc_id','source_char_id','target_char_id']).rename(columns={'target_acc_id' : 'acc_id'})\n","    test_target_trade['target'] = 1\n","\n","\n","    #aggregate\n","    trade_source_agg = {'trade_day':'nunique', 'trade_time' : 'nunique', 'trade_server' : 'nunique','item_amount' : 'sum',\n","                        'item_price': 'sum', 'trade_shop' : 'sum', 'personal_shop' : 'sum', 'accessory' : 'sum', 'adena' : 'sum',\n","                        'armor' : 'sum', 'enchant_scroll' : 'sum', 'etc' : 'sum', 'spell' : 'sum', 'weapon' : 'sum', 'source' : 'sum'}\n","\n","    trade_target_agg = {'trade_day':'nunique', 'trade_time' : 'nunique', 'trade_server' : 'nunique','item_amount' : 'sum',\n","                        'item_price': 'sum', 'trade_shop' : 'sum', 'personal_shop' : 'sum', 'accessory' : 'sum', 'adena' : 'sum',\n","                        'armor' : 'sum', 'enchant_scroll' : 'sum', 'etc' : 'sum', 'spell' : 'sum', 'weapon' : 'sum', 'target' : 'sum'}\n","\n","    train_source_trade = train_source_trade.groupby('acc_id', as_index = False).agg(trade_source_agg).reset_index(drop=True)\n","    train_target_trade = train_target_trade.groupby('acc_id', as_index = False).agg(trade_target_agg).reset_index(drop=True)\n","    \n","    test_source_trade = test_source_trade.groupby('acc_id', as_index = False).agg(trade_source_agg).reset_index(drop=True)\n","    test_target_trade = test_target_trade.groupby('acc_id', as_index = False).agg(trade_target_agg).reset_index(drop=True)\n","\n","    print('trade id unique and shape :')\n","    print(train_source_trade['acc_id'].nunique(), train_source_trade.shape, train_target_trade['acc_id'].nunique(), train_target_trade.shape)\n","    print(test_source_trade['acc_id'].nunique(), test_source_trade.shape, test_target_trade['acc_id'].nunique(), test_target_trade.shape)\n","    \n","    \n","    \n","    #merge all data\n","    print('\\nMerging all data ...')\n","    train_merge = train_label.merge(train_activity_merge, on = ['acc_id'], how ='left').merge(\n","    train_combat_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    train_pledge_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    train_payment_merge.drop(['day'], axis=1), on = ['acc_id'], how ='left').merge(\n","    train_source_trade, on = ['acc_id'], how = 'left').merge(\n","    train_target_trade, on = ['acc_id'], how = 'left')\n","        \n","    test_merge = test_activity_merge.merge(\n","    test_combat_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    test_pledge_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    test_payment_merge.drop(['day'], axis=1), on = ['acc_id'], how ='left').merge(\n","    test_source_trade, on = ['acc_id'], how = 'left').merge(\n","    test_target_trade, on = ['acc_id'], how = 'left')\n","    \n","    print('\\nmerge id unique and shape :')\n","    print(train_merge['acc_id'].nunique(), train_merge.shape)\n","    print(test_merge['acc_id'].nunique(), test_merge.shape)\n","    \n","            \n","    def fill_NA(df):\n","        print('\\nFilling NA ...')\n","\n","        na_ratio = ((df.isnull().sum() / len(df)) * 100).sort_values(ascending=False)\n","#         print('NA ratio: ')\n","#         print(na_ratio) \n","\n","        for feature in df:\n","            if df[feature].dtype == 'object':\n","                df[feature] = df[feature].fillna(\"None\") #fill.na option. object type인 경우 None으로, 그 외는 0으로 설정\n","            else:\n","                df[feature] = df[feature].fillna(0)\n","\n","    def encode_features(df):\n","        print('\\nEncoding features ...')\n","\n","        for feature in df:\n","            if df[feature].dtype == 'object':\n","                print('Encoding ', feature)\n","                lbl = preprocessing.LabelEncoder()\n","                lbl.fit(list(df[feature].values))\n","                df[feature] = lbl.transform(list(df[feature].values))\n","    #fill NA\n","    fill_NA(train_merge)\n","    fill_NA(test_merge)\n","    \n","    #encode features\n","    encode_features(train_merge)\n","    encode_features(test_merge)\n","    \n","    \n","    \n","    \n","    \n","    \n","### STEP1.1 : prepare train and vaild data\n","\n","def _prepare_data():\n","    # prepare train and valid data\n","    print('\\nPreparing train and valid data ...')\n","\n","    global X\n","\n","    global X_train\n","    global X_test\n","    \n","    global ya_train\n","    global ya_test\n","    \n","    global ys_train\n","    global ys_test\n","    \n","    global X_test_dataset\n","    global test_acc_id\n","    \n","    \n","    drop_vars = ['acc_id','amount_spent','survival_time']\n","    \n","    \n","    ya = train_merge['amount_spent'].values\n","    ys = train_merge['survival_time'].values\n","    \n","    train_merge.drop(columns=drop_vars, inplace=True)\n","    X = train_merge.values\n","    \n","    test_acc_id = test_merge['acc_id']\n","    test_merge.drop(columns=['acc_id'], inplace=True)\n","    X_test_dataset = test_merge.values\n","\n","    X_train, X_test, ya_train, ya_test = train_test_split(X, ya, test_size=1/5, random_state = SEED) #test size = 1/10\n","    X_train, X_test, ys_train, ys_test = train_test_split(X, ys, test_size=1/5, random_state = SEED) #test size = 1/10\n","\n","    \n","    print('train x shape: ', X.shape)\n","    print('train ya shape: ', ya_train.shape)\n","    print('train ys shape: ', ys_train.shape)\n","    \n","    print('valid x shape: ', X_test.shape)\n","    print('valid ya shape: ', ya_test.shape)\n","    print('valid ys shape: ', ys_test.shape)\n","    \n","    print('test x shape: ', X_test_dataset.shape)   \n","    \n","   \n","    \n","    \n","    \n","    \n","## STEP2: build model\n","def _build_model():\n","    print('\\n\\nSTEP2: building model ...')\n","    \n","    XGB_regressor = xgb.XGBRegressor()\n","    \n","    global XGB_hyper_params\n","    \n","    parameters = {'learning_rate': [0.07, 0.1, 0.3],\n","                  'max_depth': [3, 5, 7],\n","                  'n_estimators': [200, 400, 500]}\n","\n","    XGB_hyper_params = GridSearchCV(estimator=XGB_regressor, param_grid=parameters, n_jobs=-1, cv=5)\n","    \n","   \n","    \n","## STEP3: train    \n","def _train_predict_amount():\n","    print('\\n\\nSTEP3: training amount...')\n","    \n","    global XGB_model_a\n","    global y_XGB_predict_a\n","    \n","    if testname == 'test2' : \n","        XGB_model_a = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n","             colsample_bynode=1, colsample_bytree=1, gamma=0,\n","             importance_type='gain', learning_rate=0.07, max_delta_step=0,\n","             max_depth=3, min_child_weight=1, missing=None, n_estimators=200,\n","             n_jobs=1, nthread=None, objective='reg:linear', random_state=SEED,\n","             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n","             silent=None, subsample=1, verbosity=1)\n","                \n","    elif testname == 'test1' :\n","        XGB_hyper_params.fit(X_train, ya_train)\n","        print('\\nXGB_hyper_params.best_params_ of amount :')\n","        print(XGB_hyper_params.best_estimator_) # find out the best hyper parameters\n","    \n","        XGB_model_a = XGB_hyper_params.best_estimator_\n","    else :\n","        print('testname is wrong!')\n","\n","    XGB_model_a.fit(X_train, ya_train)\n","    \n","    %%time\n","    y_XGB_predict_a = XGB_model_a.predict(X_test)\n","   \n","    # let's plot feature_importance \n","    print('\\nDraw feature importance of amount_spent...')\n","    sns.set_style('ticks')\n","    fig, ax = plt.subplots()\n","    sns.barplot(y=list(train_merge.columns), x=list(XGB_model_a.feature_importances_)).set_title('feature_importances_ of amount_spent')\n","    fig.set_size_inches(20, 20)\n","    sns.despine()\n","    plt.show()\n","    \n","    \n","    \n","    \n","def _train_predict_survival():\n","    print('\\n\\nSTEP4: training survival...')\n","    \n","    global XGB_model_s\n","    global y_XGB_predict_s\n","    \n","    if testname == 'test2' :\n","        XGB_model_s = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n","             colsample_bynode=1, colsample_bytree=1, gamma=0,\n","             importance_type='gain', learning_rate=0.07, max_delta_step=0,\n","             max_depth=7, min_child_weight=1, missing=None, n_estimators=200,\n","             n_jobs=1, nthread=None, objective='reg:linear', random_state=SEED,\n","             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n","             silent=None, subsample=1, verbosity=1)\n","        \n","    elif testname == 'test1' :\n","        XGB_hyper_params.fit(X_train, ys_train)\n","        print('\\nXGB_hyper_params.best_params_ of amount :')\n","        print(XGB_hyper_params.best_estimator_) # find out the best hyper parameters\n","        \n","        XGB_model_a = XGB_hyper_params.best_estimator_\n","    else :\n","        print('testname is wrong!')    \n","    \n","    XGB_model_s.fit(X_train, ys_train)\n","    %%time\n","    y_XGB_predict_s = XGB_model_s.predict(X_test)\n","\n","    # let's plot feature_importance \n","    print('\\nDraw feature importance of survival_time...')\n","    sns.set_style('ticks')\n","    fig, ax = plt.subplots()\n","    sns.barplot(y=list(train_merge.columns), x=list(XGB_model_s.feature_importances_)).set_title('feature_importances_ of survival_time')\n","    fig.set_size_inches(20, 20)\n","    sns.despine()\n","    plt.show()\n","\n","\n","## STEP5: generate submission \n","def _generate_submission():\n","    print('\\n\\nSTEP5: generating submission ...')\n","    \n","    test_predict = pd.DataFrame(columns = ['acc_id', 'survival_time','amount_spent'])\n","    \n","    ya_XGB_amount = np.expm1(XGB_model_a.predict(X_test_dataset))\n","    ys_XGB_amount = np.expm1(XGB_model_s.predict(X_test_dataset))\n","\n","    # submitting our predictions\n","    test_predict['acc_id'] = test_acc_id\n","    test_predict['survival_time'] = ys_XGB_amount\n","    test_predict['amount_spent'] = ya_XGB_amount\n","    \n","    test_predict['amount_spent'] = [0 if i < 0 else i for i in test_predict['amount_spent']]\n","    test_predict['survival_time'] = [1 if i < 1 else i for i in test_predict['survival_time']]\n","\n","    \n","    # testname에 따라 amountspent value 키우기\n","    if testname == 'test1' :\n","        test_predict['amount_spent'] = test_predict['amount_spent']*20\n","        print('amount * 20')\n","#     elif testname == 'test2' :\n","#         test_predict['amount_spent'] = test_predict['amount_spent']*5\n","#         print('amount * 5')\n","    else :\n","        print('eveything is fine!')\n","    \n","    test_predict.to_csv(submission_path + testname + '_predict.csv',index=False) \n","    print('\\nsave completed.\\n')\n","    print(testname + '_predict id unique and shape :')\n","    print(test_predict['acc_id'].nunique(), test_predict.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ThhDBj1MmEpV","colab_type":"text"},"source":["#### ver1.4 CODE - merge trade (4102.32\t2496.54\t6598.863)"]},{"cell_type":"code","metadata":{"id":"ku1AgsitmJPg","colab_type":"code","colab":{}},"source":["#ref : https://www.kaggle.com/cuongvn08/xgboost-cross-validation-clean-coding-lb-0-64x\n","\n","# -*- coding: utf-8 -*-\n","\n","\n","from enum import Enum\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","from sklearn import preprocessing\n","import xgboost as xgb\n","from datetime import datetime\n","import gc\n","\n","import warnings\n","def ignore_warn(*args, **kwargs):\n","    pass\n","warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n","\n","\n","## STEP0: do setting\n","class Settings(Enum):\n","    global train_path\n","    global test_path\n","    global submission_path\n","    global testname\n","    global codeversion\n","    \n","    codeversion = 'ver1.4 - simple merge trade'\n","    \n","#     testname = 'test1'\n","    \n","    path            = 'drive/My Drive/bigcontest2019/'\n","    train_path      = path + 'data/train/'\n","    test_path       = path + 'data/test/'\n","    submission_path = path + 'scripts/model/metrics/nes_inference/'\n","        \n","    def __str__(self):\n","        return self.value\n","        \n","    \n","## STEP1: process data    \n","def fill_NA(df):\n","    print('\\nFilling NA ...')\n","    \n","    na_ratio = ((df.isnull().sum() / len(df)) * 100).sort_values(ascending=False)\n","    print('NA ratio: ')\n","    print(na_ratio) \n","    \n","    for feature in df:\n","        if df[feature].dtype == 'object':\n","            df[feature] = df[feature].fillna(\"None\")\n","        else:\n","            df[feature] = df[feature].fillna(0)\n","    \n","def encode_features(df):\n","    print('\\nEncoding features ...')\n","    \n","    for feature in df:\n","        if df[feature].dtype == 'object':\n","            print('Encoding ', feature)\n","            lbl = preprocessing.LabelEncoder()\n","            lbl.fit(list(df[feature].values))\n","            df[feature] = lbl.transform(list(df[feature].values))\n","    \n","# def display_feature_target(df, feature):\n","#     fig, ax = plt.subplots()\n","#     ax.scatter(x = df[feature], y = df['logerror'])\n","#     plt.ylabel('logerror', fontsize=13)\n","#     plt.xlabel(feature, fontsize=13)\n","#     plt.show()\n","    \n","def _process_data():\n","    \n","    print('CODE VERSION : ', codeversion)\n","    print('\\n\\nSTEP1: processing data ...')\n","    \n","    global train_merge\n","    global test_merge\n","\n","        \n","    # load data\n","    print('\\nLoading data ...')\n","    \n","    train_label = pd.read_csv(train_path + 'train_label.csv')\n","    train_activity = pd.read_csv(train_path + 'train_activity.csv')\n","    train_combat = pd.read_csv(train_path + 'train_combat.csv')\n","    train_payment = pd.read_csv(train_path + 'train_payment.csv')\n","    train_pledge = pd.read_csv(train_path + 'train_pledge.csv')\n","    train_trade = pd.read_csv(train_path + 'train_trade.csv')\n","\n","    test_activity = pd.read_csv(test_path + testname + '_activity.csv')\n","    test_combat = pd.read_csv(test_path + testname + '_combat.csv')\n","    test_payment = pd.read_csv(test_path + testname + '_payment.csv')\n","    test_pledge = pd.read_csv(test_path + testname + '_pledge.csv')\n","    test_trade = pd.read_csv(test_path  + testname + '_trade.csv')\n","\n","    \n","    # merge payment\n","    print('\\nGrouping activity data ...')\n","\n","    train_activity['game_money_change_minus'] = train_activity['game_money_change'] #game_money_change를 마이너스끼리, 플러스끼리 분리\n","    train_activity['game_money_change_minus'] = [0 if i > 0 else i for i in train_activity['game_money_change_minus']]\n","    train_activity['game_money_change'] = [0 if i < 0 else i for i in train_activity['game_money_change']]\n","\n","    test_activity['game_money_change_minus'] = test_activity['game_money_change']\n","    test_activity['game_money_change_minus'] = [0 if i > 0 else i for i in test_activity['game_money_change_minus']]\n","    test_activity['game_money_change'] = [0 if i < 0 else i for i in test_activity['game_money_change']]\n","\n","    activity_agg = {\n","        'day':'nunique', 'char_id':'nunique','server':'nunique', 'playtime':'sum', 'npc_kill':'sum', \n","        'solo_exp':'sum','party_exp':'sum','quest_exp':'sum','rich_monster':'sum', \n","        'death':'sum', 'revive':'sum','exp_recovery':'sum','fishing':'sum','private_shop':'sum',\n","        'game_money_change':'sum', 'game_money_change_minus':'sum','enchant_count':'sum'}\n","    \n","    train_activity_merge = train_activity.groupby('acc_id', as_index = False).agg(activity_agg).reset_index(drop=True)\n","    test_activity_merge = test_activity.groupby('acc_id', as_index = False).agg(activity_agg).reset_index(drop=True)\n","    \n","    print('activity id unique and shape :')\n","    print(train_activity_merge['acc_id'].nunique(), train_activity_merge.shape)\n","    print(test_activity_merge['acc_id'].nunique(), test_activity_merge.shape)\n","    \n","    \n","    # merge payment\n","    print('\\nGrouping payment data ...')\n","    \n","    train_payment_merge = train_payment.groupby( 'acc_id', as_index = False).agg({'day':'nunique','amount_spent':'max'}).reset_index(drop=True)\n","    test_payment_merge = test_payment.groupby('acc_id', as_index = False).agg({'day':'nunique','amount_spent':'max'}).reset_index(drop=True)\n","\n","    train_payment_merge.rename(columns={'amount_spent':'max_spent'}, inplace=True) #amout_spent를 일일 최고 구매량으로 변경\n","    test_payment_merge.rename(columns={'amount_spent':'max_spent'}, inplace=True)\n","\n","    print('payment id unique and shape :')\n","    print(train_payment_merge['acc_id'].nunique(), train_payment_merge.shape)\n","    print(test_payment_merge['acc_id'].nunique(), test_payment_merge.shape)\n","    \n","    \n","    # merge combat \n","    print('\\nGrouping combat data ...')\n","    class_names = {0:'liege', 1:'knight', 2:'elf', 3:'magician', 4:'dark-elf', 5:'dragon',6:'illusionist',7:'warrior'}\n","    for elem in train_combat['class'].unique():train_combat[class_names[elem]] = train_combat['class'] == elem\n","    for elem in test_combat['class'].unique():test_combat[class_names[elem]] = test_combat['class'] == elem\n","    \n","    combat_agg = {'day':'nunique','char_id' : 'nunique', 'server' : 'nunique', 'class' : 'nunique', 'level' : 'max',\n","                  'pledge_cnt' : 'sum','random_attacker_cnt' : 'sum','random_defender_cnt' : 'sum', 'temp_cnt' : 'sum',\n","                  'same_pledge_cnt' : 'sum','etc_cnt' : 'sum','num_opponent' : 'sum','elf' : 'sum','magician' : 'sum',\n","                  'warrior' : 'sum','dark-elf' : 'sum','knight' : 'sum','dragon' : 'sum','liege' : 'sum','illusionist' : 'sum'}\n","        \n","    combat_rename = {'random_attacker_cnt':'combat_random_attacker_cnt','random_defender_cnt' : 'combat_random_defender_cnt',\n","                     'temp_cnt' : 'combat_temp_cnt','same_pledge_cnt' : 'combat_same_pledge_cnt','etc_cnt' : 'combat_etc_cnt'}\n","\n","    train_combat_merge = train_combat.groupby('acc_id', as_index = False).agg(combat_agg).reset_index(drop=True)\n","    test_combat_merge = test_combat.groupby('acc_id', as_index = False).agg(combat_agg).reset_index(drop=True)\n","  \n","    train_combat_merge.rename(columns=combat_rename, inplace=True)\n","    test_combat_merge.rename(columns=combat_rename, inplace=True)\n","\n","    print('combat id unique and shape :')\n","    print(train_combat_merge['acc_id'].nunique(), train_combat_merge.shape)\n","    print(test_combat_merge['acc_id'].nunique(), test_combat_merge.shape)\n","    \n","    \n","    # merge combat \n","    print('\\nGrouping pledge data ...')\n","    \n","    pledge_agg = {'day':'nunique','char_id':'nunique','server':'nunique','pledge_id':'nunique', 'play_char_cnt' : 'sum',\n","                  'combat_char_cnt' : 'sum','pledge_combat_cnt' : 'sum','random_attacker_cnt' : 'sum','random_defender_cnt' : 'sum',\n","                  'same_pledge_cnt' : 'sum', 'temp_cnt' : 'sum','etc_cnt' : 'sum','combat_play_time' : 'sum','non_combat_play_time' : 'sum'}\n","    \n","    train_pledge_merge = train_pledge.groupby('acc_id', as_index = False).agg(pledge_agg).reset_index(drop=True)\n","    test_pledge_merge = test_pledge.groupby('acc_id', as_index = False).agg(pledge_agg).reset_index(drop=True)\n","    \n","    print('pledge id unique and shape :')\n","    print(train_pledge_merge['acc_id'].nunique(), train_pledge_merge.shape)\n","    print(test_pledge_merge['acc_id'].nunique(), test_pledge_merge.shape)\n","    \n","    \n","    # merge trade \n","    print('\\nGrouping trade data ...')\n","\n","    # item_type dummies             \n","    train_item_type_dummy = pd.get_dummies(train_trade['item_type'])\n","    test_item_type_dummy = pd.get_dummies(test_trade['item_type'])\n","\n","    # trade_type dummies\n","    type_names = {0:'personal_shop', 1:'trade_shop'}\n","    for elem in train_trade['type'].unique():train_trade[type_names[elem]] = train_trade['type'] == elem\n","    for elem in test_trade['type'].unique():test_trade[type_names[elem]] = test_trade['type'] == elem\n","\n","    # merge dummies\n","    \n","    train_trade = train_trade.drop(columns=['type','item_type'])\n","    train_trade = pd.concat([train_trade, train_item_type_dummy], axis = 1)\n","    \n","    test_trade = test_trade.drop(columns=['type','item_type'])\n","    test_trade = pd.concat([test_trade, test_item_type_dummy], axis = 1)\n","\n","    # rename columns\n","    trade_rename = {'day':'trade_day','server' : 'trade_server', 'time' : 'trade_time'}\n","    train_trade.rename(columns=trade_rename, inplace=True)\n","    test_trade.rename(columns=trade_rename, inplace=True)\n","\n","    #divide source and target id\n","    \n","    train_source_trade = train_trade.drop(columns = ['target_acc_id','source_char_id','target_char_id']).rename(columns={'source_acc_id' : 'acc_id'})\n","    train_source_trade['source'] = 1\n","    train_target_trade = train_trade.drop(columns = ['source_acc_id','source_char_id','target_char_id']).rename(columns={'target_acc_id' : 'acc_id'})\n","    train_target_trade['target'] = 1\n","    \n","    test_source_trade = test_trade.drop(columns = ['target_acc_id','source_char_id','target_char_id']).rename(columns={'source_acc_id' : 'acc_id'})\n","    test_source_trade['source'] = 1\n","    test_target_trade = test_trade.drop(columns = ['source_acc_id','source_char_id','target_char_id']).rename(columns={'target_acc_id' : 'acc_id'})\n","    test_target_trade['target'] = 1\n","\n","\n","    #aggregate\n","    trade_source_agg = {'trade_day':'nunique', 'trade_time' : 'nunique', 'trade_server' : 'nunique','item_amount' : 'sum',\n","                        'item_price': 'sum', 'trade_shop' : 'sum', 'personal_shop' : 'sum', 'accessory' : 'sum', 'adena' : 'sum',\n","                        'armor' : 'sum', 'enchant_scroll' : 'sum', 'etc' : 'sum', 'spell' : 'sum', 'weapon' : 'sum', 'source' : 'sum'}\n","\n","    trade_target_agg = {'trade_day':'nunique', 'trade_time' : 'nunique', 'trade_server' : 'nunique','item_amount' : 'sum',\n","                        'item_price': 'sum', 'trade_shop' : 'sum', 'personal_shop' : 'sum', 'accessory' : 'sum', 'adena' : 'sum',\n","                        'armor' : 'sum', 'enchant_scroll' : 'sum', 'etc' : 'sum', 'spell' : 'sum', 'weapon' : 'sum', 'target' : 'sum'}\n","\n","    train_source_trade = train_source_trade.groupby('acc_id', as_index = False).agg(trade_source_agg).reset_index(drop=True)\n","    train_target_trade = train_target_trade.groupby('acc_id', as_index = False).agg(trade_target_agg).reset_index(drop=True)\n","    \n","    test_source_trade = test_source_trade.groupby('acc_id', as_index = False).agg(trade_source_agg).reset_index(drop=True)\n","    test_target_trade = test_target_trade.groupby('acc_id', as_index = False).agg(trade_target_agg).reset_index(drop=True)\n","\n","    print('trade id unique and shape :')\n","    print(train_source_trade['acc_id'].nunique(), train_source_trade.shape, train_target_trade['acc_id'].nunique(), train_target_trade.shape)\n","    print(test_source_trade['acc_id'].nunique(), test_source_trade.shape, test_target_trade['acc_id'].nunique(), test_target_trade.shape)\n","    \n","    \n","    \n","    #merge all data\n","    print('\\nMerging all data ...')\n","    train_merge = train_label.merge(train_activity_merge, on = ['acc_id'], how ='left').merge(\n","    train_combat_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    train_pledge_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    train_payment_merge.drop(['day'], axis=1), on = ['acc_id'], how ='left').merge(\n","    train_source_trade, on = ['acc_id'], how = 'left').merge(\n","    train_target_trade, on = ['acc_id'], how = 'left')\n","        \n","    test_merge = test_activity_merge.merge(\n","    test_combat_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    test_pledge_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    test_payment_merge.drop(['day'], axis=1), on = ['acc_id'], how ='left').merge(\n","    test_source_trade, on = ['acc_id'], how = 'left').merge(\n","    test_target_trade, on = ['acc_id'], how = 'left')\n","    \n","    print('\\nmerge id unique and shape :')\n","    print(train_merge['acc_id'].nunique(), train_merge.shape)\n","    print(test_merge['acc_id'].nunique(), test_merge.shape)\n","    \n","    \n","    # fill NA\n","#     fill_NA(train_merge)\n","    \n","    # encode features\n","#     encode_features(train_merge)\n","    \n","### STEP1.1 : prepare train and vaild data\n","\n","def _prepare_data():\n","    # prepare train and valid data\n","    print('\\nPreparing train and valid data ...')\n","\n","    global train_x\n","    global train_ya\n","    global train_ys\n","    \n","    global valid_x\n","    global valid_ya\n","    global valid_ys\n","    \n","    global test_x\n","    global test_acc_id\n","    \n","    \n","    drop_vars = ['acc_id','amount_spent','survival_time']\n","    \n","    train_ya = train_merge['amount_spent']\n","    train_ys = train_merge['survival_time']\n","    \n","    train_x = train_merge\n","    train_x.drop(columns=drop_vars, inplace=True)\n","    \n","    valid_x = train_x[30000:]\n","    valid_ya = train_ya[30000:]\n","    \n","    train_x = train_x[:30000]\n","    train_ya = train_ya[:30000]\n","    \n","    valid_ys = train_ys[30000:]\n","    train_ys = train_ys[:30000]\n","    \n","    print('train x shape: ', train_x.shape)\n","    print('train ya shape: ', train_ya.shape)\n","    print('train ys shape: ', train_ys.shape)\n","    \n","    print('valid x shape: ', valid_x.shape)\n","    print('valid ya shape: ', valid_ya.shape)\n","    print('valid ys shape: ', valid_ys.shape)\n","    \n","    \n","    # prepare test data\n","    print('\\nPreparing test data ...')\n","    \n","    test_x = test_merge\n","    test_acc_id = test_merge['acc_id']\n","    test_x.drop(columns=['acc_id'], inplace=True)\n","    print('test x shape: ', test_x.shape)\n","\n","    \n","## STEP2: build model\n","def _build_model():\n","    print('\\n\\nSTEP2: building model ...')\n","    \n","    global xgb_params\n","\n","    xgb_params = {\n","        'eta': 0.007,\n","        'max_depth': 8, \n","        'subsample': 0.8,\n","        'objective': 'reg:linear',\n","        'eval_metric': 'mae',\n","        'lambda': 9.0,\n","        'alpha': 0.8,\n","        'colsample_bytree': 0.7,\n","        'silent': 1,\n","        'random_state' : 42\n","    }\n","\n","    \n","\n","## STEP3: train    \n","def _train_predict_amount():\n","    print('\\n\\nSTEP3: training amount...')\n","    \n","    global a_xgb_clf\n","    global a_pred\n","    \n","    ad_train = xgb.DMatrix(train_x, label=train_ya)\n","    ad_valid = xgb.DMatrix(valid_x, label=valid_ya)\n","    \n","    a_evals = [(ad_train, 'train'), (ad_valid, 'valid')]\n","    a_xgb_clf = xgb.train(xgb_params, ad_train, num_boost_round=10000, evals=a_evals, \n","                        early_stopping_rounds=100, verbose_eval=20)\n","    \n","\n","    ad_test = xgb.DMatrix(test_x)\n","    a_pred = a_xgb_clf.predict(ad_test)\n","    \n","def _train_predict_survival():\n","    print('\\n\\nSTEP4: training survival...')\n","    \n","    global s_xgb_clf\n","    global s_pred\n","    \n","    sd_train = xgb.DMatrix(train_x, label=train_ys)\n","    sd_valid = xgb.DMatrix(valid_x, label=valid_ys)\n","    \n","    s_evals = [(sd_train, 'train'), (sd_valid, 'valid')]\n","    s_xgb_clf = xgb.train(xgb_params, sd_train, num_boost_round=10000, evals=s_evals, \n","                        early_stopping_rounds=100, verbose_eval=20)\n","\n","    sd_test = xgb.DMatrix(test_x)\n","    s_pred = s_xgb_clf.predict(sd_test)\n","    \n","\n","## STEP5: generate submission \n","def _generate_submission():\n","    print('\\n\\nSTEP5: generating submission ...')\n","    \n","    test_predict = pd.DataFrame(columns = ['acc_id', 'survival_time','amount_spent'])\n","    \n","    test_predict['acc_id'] = test_acc_id\n","    test_predict['survival_time'] = s_pred\n","    test_predict['amount_spent'] = a_pred\n","    \n","    test_predict['amount_spent'] = [0 if i < 0 else i for i in test_predict['amount_spent']] # 마이너스값 0 처리 \n","    test_predict['survival_time'] = [1 if i < 1 else i for i in test_predict['survival_time']] # 1 미만 값 1 처리\n","    \n","    #testname에 따라 amountspent value 키우기\n","    if testname == 'test1' :\n","        test_predict['amount_spent'] = test_predict['amount_spent']*30\n","    elif testname == 'test2' :\n","        test_predict['amount_spent'] = test_predict['amount_spent']*5\n","    else :\n","        print('testname is wrong!')\n","        \n","    test_predict.to_csv(submission_path + testname + '_predict.csv',index=False)    \n"," "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"r01vMENYkOSm","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C6ZulL9fGZY9","colab_type":"text"},"source":["\n","#### CODE - predict for testname"]},{"cell_type":"code","metadata":{"id":"r5IxfKrmeyrq","colab_type":"code","colab":{}},"source":["#ref : https://www.kaggle.com/cuongvn08/xgboost-cross-validation-clean-coding-lb-0-64x\n","\n","# -*- coding: utf-8 -*-\n","\n","from enum import Enum\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","from sklearn import preprocessing\n","import xgboost as xgb\n","from datetime import datetime\n","import gc\n","\n","import warnings\n","def ignore_warn(*args, **kwargs):\n","    pass\n","warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n","\n","\n","## STEP0: do setting\n","class Settings(Enum):\n","    global train_path\n","    global test_path\n","    global submission_path\n","    global testname\n","    \n","    testname = 'test2'\n","    \n","    path            = 'drive/My Drive/bigcontest2019/'\n","    train_path      = path + 'data/train/'\n","    test_path       = path + 'data/test/'\n","    submission_path = path + 'scripts/model/metrics/nes_inference/'\n","        \n","    def __str__(self):\n","        return self.value\n","        \n","    \n","## STEP1: process data    \n","def fill_NA(df):\n","    print('\\nFilling NA ...')\n","    \n","    na_ratio = ((df.isnull().sum() / len(df)) * 100).sort_values(ascending=False)\n","    print('NA ratio: ')\n","    print(na_ratio) \n","    \n","    for feature in df:\n","        if df[feature].dtype == 'object':\n","            df[feature] = df[feature].fillna(\"None\")\n","        else:\n","            df[feature] = df[feature].fillna(0)\n","    \n","def encode_features(df):\n","    print('\\nEncoding features ...')\n","    \n","    for feature in df:\n","        if df[feature].dtype == 'object':\n","            print('Encoding ', feature)\n","            lbl = preprocessing.LabelEncoder()\n","            lbl.fit(list(df[feature].values))\n","            df[feature] = lbl.transform(list(df[feature].values))\n","    \n","# def display_feature_target(df, feature):\n","#     fig, ax = plt.subplots()\n","#     ax.scatter(x = df[feature], y = df['logerror'])\n","#     plt.ylabel('logerror', fontsize=13)\n","#     plt.xlabel(feature, fontsize=13)\n","#     plt.show()\n","    \n","def _process_data():\n","    print('\\n\\nSTEP1: processing data ...')\n","    \n","    global train_merge\n","    global test_merge\n","\n","        \n","    # load data\n","    print('\\nLoading data ...')\n","    \n","    train_label = pd.read_csv(train_path + 'train_label.csv')\n","    train_activity = pd.read_csv(train_path + 'train_activity.csv')\n","    train_combat = pd.read_csv(train_path + 'train_combat.csv')\n","    train_payment = pd.read_csv(train_path + 'train_payment.csv')\n","    train_pledge = pd.read_csv(train_path + 'train_pledge.csv')\n","    train_trade = pd.read_csv(train_path + 'train_trade.csv')\n","\n","    test_activity = pd.read_csv(test_path + testname + '_activity.csv')\n","    test_combat = pd.read_csv(test_path + testname + '_combat.csv')\n","    test_payment = pd.read_csv(test_path + testname + '_payment.csv')\n","    test_pledge = pd.read_csv(test_path + testname + '_pledge.csv')\n","    test_trade = pd.read_csv(test_path  + testname + '_trade.csv')\n","\n","    \n","    # merge payment\n","    print('\\nGrouping activity data ...')\n","\n","    train_activity['game_money_change_minus'] = train_activity['game_money_change'] #game_money_change를 마이너스끼리, 플러스끼리 분리\n","    train_activity['game_money_change_minus'] = [0 if i > 0 else i for i in train_activity['game_money_change_minus']]\n","    train_activity['game_money_change'] = [0 if i < 0 else i for i in train_activity['game_money_change']]\n","\n","    test_activity['game_money_change_minus'] = test_activity['game_money_change']\n","    test_activity['game_money_change_minus'] = [0 if i > 0 else i for i in test_activity['game_money_change_minus']]\n","    test_activity['game_money_change'] = [0 if i < 0 else i for i in test_activity['game_money_change']]\n","\n","    activity_agg = {\n","        'day':'nunique', 'char_id':'nunique','server':'nunique', 'playtime':'sum', 'npc_kill':'sum', \n","        'solo_exp':'sum','party_exp':'sum','quest_exp':'sum','rich_monster':'sum', \n","        'death':'sum', 'revive':'sum','exp_recovery':'sum','fishing':'sum','private_shop':'sum',\n","        'game_money_change':'sum', 'game_money_change_minus':'sum','enchant_count':'sum'}\n","    \n","    train_activity_merge = train_activity.groupby('acc_id', as_index = False).agg(activity_agg).reset_index(drop=True)\n","    test_activity_merge = test_activity.groupby('acc_id', as_index = False).agg(activity_agg).reset_index(drop=True)\n","    \n","    print('activity id unique and shape :')\n","    print(train_activity_merge['acc_id'].nunique(), train_activity_merge.shape)\n","    print(test_activity_merge['acc_id'].nunique(), test_activity_merge.shape)\n","    \n","    \n","    # merge payment\n","    print('\\nGrouping payment data ...')\n","    \n","    train_payment_merge = train_payment.groupby( 'acc_id', as_index = False).agg({'day':'nunique','amount_spent':'max'}).reset_index(drop=True)\n","    test_payment_merge = test_payment.groupby('acc_id', as_index = False).agg({'day':'nunique','amount_spent':'max'}).reset_index(drop=True)\n","\n","    train_payment_merge.rename(columns={'amount_spent':'max_spent'}, inplace=True) #amout_spent를 일일 최고 구매량으로 변경\n","    test_payment_merge.rename(columns={'amount_spent':'max_spent'}, inplace=True)\n","\n","    print('payment id unique and shape :')\n","    print(train_payment_merge['acc_id'].nunique(), train_payment_merge.shape)\n","    print(test_payment_merge['acc_id'].nunique(), test_payment_merge.shape)\n","    \n","    \n","    # merge combat \n","    print('\\nGrouping combat data ...')\n","    class_names = {0:'liege', 1:'knight', 2:'elf', 3:'magician', 4:'dark-elf', 5:'dragon',6:'illusionist',7:'warrior'}\n","    for elem in train_combat['class'].unique():train_combat[class_names[elem]] = train_combat['class'] == elem\n","    for elem in test_combat['class'].unique():test_combat[class_names[elem]] = test_combat['class'] == elem\n","    \n","    combat_agg = {'day':'nunique','char_id' : 'nunique', 'server' : 'nunique', 'class' : 'nunique', 'level' : 'max',\n","                  'pledge_cnt' : 'sum','random_attacker_cnt' : 'sum','random_defender_cnt' : 'sum', 'temp_cnt' : 'sum',\n","                  'same_pledge_cnt' : 'sum','etc_cnt' : 'sum','num_opponent' : 'sum','elf' : 'sum','magician' : 'sum',\n","                  'warrior' : 'sum','dark-elf' : 'sum','knight' : 'sum','dragon' : 'sum','liege' : 'sum','illusionist' : 'sum'}\n","        \n","    combat_rename = {'random_attacker_cnt':'combat_random_attacker_cnt','random_defender_cnt' : 'combat_random_defender_cnt',\n","                     'temp_cnt' : 'combat_temp_cnt','same_pledge_cnt' : 'combat_same_pledge_cnt','etc_cnt' : 'combat_etc_cnt'}\n","\n","    train_combat_merge = train_combat.groupby('acc_id', as_index = False).agg(combat_agg).reset_index(drop=True)\n","    test_combat_merge = test_combat.groupby('acc_id', as_index = False).agg(combat_agg).reset_index(drop=True)\n","  \n","    train_combat_merge.rename(columns=combat_rename, inplace=True)\n","    test_combat_merge.rename(columns=combat_rename, inplace=True)\n","\n","    print('combat id unique and shape :')\n","    print(train_combat_merge['acc_id'].nunique(), train_combat_merge.shape)\n","    print(test_combat_merge['acc_id'].nunique(), test_combat_merge.shape)\n","    \n","    \n","    # merge combat \n","    print('\\nGrouping pledge data ...')\n","    \n","    pledge_agg = {'day':'nunique','char_id':'nunique','server':'nunique','pledge_id':'nunique', 'play_char_cnt' : 'sum',\n","                  'combat_char_cnt' : 'sum','pledge_combat_cnt' : 'sum','random_attacker_cnt' : 'sum','random_defender_cnt' : 'sum',\n","                  'same_pledge_cnt' : 'sum', 'temp_cnt' : 'sum','etc_cnt' : 'sum','combat_play_time' : 'sum','non_combat_play_time' : 'sum'}\n","    \n","    train_pledge_merge = train_pledge.groupby('acc_id', as_index = False).agg(pledge_agg).reset_index(drop=True)\n","    test_pledge_merge = test_pledge.groupby('acc_id', as_index = False).agg(pledge_agg).reset_index(drop=True)\n","    \n","    print('pledge id unique and shape :')\n","    print(train_pledge_merge['acc_id'].nunique(), train_pledge_merge.shape)\n","    print(test_pledge_merge['acc_id'].nunique(), test_pledge_merge.shape)\n","    \n","    \n","    #merge all data\n","    print('\\nMerging all data ...')\n","    train_merge = train_label.merge(train_activity_merge, on = ['acc_id'], how ='left').merge(\n","    train_combat_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    train_pledge_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    train_payment_merge.drop(['day'], axis=1), on = ['acc_id'], how ='left')\n","        \n","    test_merge = test_activity_merge.merge(\n","    test_combat_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    test_pledge_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    test_payment_merge.drop(['day'], axis=1), on = ['acc_id'], how ='left')\n","    \n","    print('\\nmerge id unique and shape :')\n","    print(train_merge['acc_id'].nunique(), train_merge.shape)\n","    print(test_merge['acc_id'].nunique(), test_merge.shape)\n","    \n","    \n","    # fill NA\n","#     fill_NA(train_merge)\n","    \n","    # encode features\n","#     encode_features(train_merge)\n","    \n","### STEP1.1 : prepare train and vaild data\n","\n","def _prepare_data():\n","    # prepare train and valid data\n","    print('\\nPreparing train and valid data ...')\n","\n","    global train_x\n","    global train_ya\n","    global train_ys\n","    \n","    global valid_x\n","    global valid_ya\n","    global valid_ys\n","    \n","    global test_x\n","    global test_acc_id\n","    \n","    drop_vars = ['acc_id','amount_spent','survival_time']\n","    \n","    train_ya = train_merge['amount_spent']\n","    train_ys = train_merge['survival_time']\n","    \n","    train_x = train_merge\n","    train_x.drop(columns=drop_vars, inplace=True)\n","    \n","    valid_x = train_x[30000:]\n","    valid_ya = train_ya[30000:]\n","    \n","    train_x = train_x[:30000]\n","    train_ya = train_ya[:30000]\n","    \n","    valid_ys = train_ys[30000:]\n","    train_ys = train_ys[:30000]\n","    \n","    print('train x shape: ', train_x.shape)\n","    print('train ya shape: ', train_ya.shape)\n","    print('train ys shape: ', train_ys.shape)\n","    \n","    print('valid x shape: ', valid_x.shape)\n","    print('valid ya shape: ', valid_ya.shape)\n","    print('valid ys shape: ', valid_ys.shape)\n","    \n","    \n","    # prepare test data\n","    print('\\nPreparing test data ...')\n","    \n","    test_x = test_merge\n","    test_acc_id = test_merge['acc_id']\n","    test_x.drop(columns=['acc_id'], inplace=True)\n","    print('test x shape: ', test_x.shape)\n","\n","    \n","## STEP2: build model\n","def _build_model():\n","    print('\\n\\nSTEP2: building model ...')\n","    \n","    global xgb_params\n","    xgb_params = {\n","        'eta': 0.007,\n","        'max_depth': 6, \n","        'subsample': 0.6,\n","        'objective': 'reg:linear',\n","        'eval_metric': 'mae',\n","        'lambda': 9.0,\n","        'alpha': 0.8,\n","        'colsample_bytree': 0.7,\n","        'silent': 1,\n","        'random_state' : 42\n","    }\n","    \n","\n","## STEP3: train    \n","def _train_predict_amount():\n","    print('\\n\\nSTEP3: training amount...')\n","    \n","    global a_xgb_clf\n","    global a_pred\n","    \n","    ad_train = xgb.DMatrix(train_x, label=train_ya)\n","    ad_valid = xgb.DMatrix(valid_x, label=valid_ya)\n","    \n","    a_evals = [(ad_train, 'train'), (ad_valid, 'valid')]\n","    a_xgb_clf = xgb.train(xgb_params, ad_train, num_boost_round=10000, evals=a_evals, \n","                        early_stopping_rounds=100, verbose_eval=10)\n","\n","    ad_test = xgb.DMatrix(test_x)\n","    a_pred = a_xgb_clf.predict(ad_test)\n","    \n","def _train_predict_survival():\n","    print('\\n\\nSTEP4: training survival...')\n","    \n","    global s_xgb_clf\n","    global s_pred\n","    \n","    sd_train = xgb.DMatrix(train_x, label=train_ys)\n","    sd_valid = xgb.DMatrix(valid_x, label=valid_ys)\n","    \n","    s_evals = [(sd_train, 'train'), (sd_valid, 'valid')]\n","    s_xgb_clf = xgb.train(xgb_params, sd_train, num_boost_round=10000, evals=s_evals, \n","                        early_stopping_rounds=100, verbose_eval=10)\n","\n","    sd_test = xgb.DMatrix(test_x)\n","    s_pred = s_xgb_clf.predict(sd_test)\n","    \n","\n","## STEP5: generate submission \n","def _generate_submission():\n","    print('\\n\\nSTEP5: generating submission ...')\n","    \n","    test_predict = pd.DataFrame(columns = ['acc_id', 'survival_time','amount_spent'])\n","    \n","    test_predict['acc_id'] = test_acc_id\n","    test_predict['survival_time'] = s_pred\n","    test_predict['amount_spent'] = a_pred\n","    \n","    test_predict['amount_spent'] = [0 if i < 0 else i for i in test_predict['amount_spent']]\n","    test_predict['survival_time'] = [1 if i < 1 else i for i in test_predict['survival_time']]\n","    \n","    test_predict.to_csv(submission_path + testname + '_predict.csv',index=False)    \n"," "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"62XbnrCic8IR","colab_type":"text"},"source":["#### CODE - predict for single test1"]},{"cell_type":"code","metadata":{"id":"VAzeUzdoPONN","colab_type":"code","colab":{}},"source":["#ref : https://www.kaggle.com/cuongvn08/xgboost-cross-validation-clean-coding-lb-0-64x\n","\n","# -*- coding: utf-8 -*-\n","\n","from enum import Enum\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","from sklearn import preprocessing\n","import xgboost as xgb\n","from datetime import datetime\n","import gc\n","\n","import warnings\n","def ignore_warn(*args, **kwargs):\n","    pass\n","warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n","\n","\n","## STEP0: do setting\n","class Settings(Enum):\n","    global train_path\n","    global test_path\n","    global submission_path\n","\n","    path            = 'drive/My Drive/bigcontest2019/'\n","    train_path      = path + 'data/train/'\n","    test_path       = path + 'data/test/'\n","    submission_path = path + 'scripts/model/metrics/nes_inference/'\n","        \n","    def __str__(self):\n","        return self.value\n","        \n","    \n","## STEP1: process data    \n","def fill_NA(df):\n","    print('\\nFilling NA ...')\n","    \n","    na_ratio = ((df.isnull().sum() / len(df)) * 100).sort_values(ascending=False)\n","    print('NA ratio: ')\n","    print(na_ratio) \n","    \n","    for feature in df:\n","        if df[feature].dtype == 'object':\n","            df[feature] = df[feature].fillna(\"None\")\n","        else:\n","            df[feature] = df[feature].fillna(0)\n","    \n","def encode_features(df):\n","    print('\\nEncoding features ...')\n","    \n","    for feature in df:\n","        if df[feature].dtype == 'object':\n","            print('Encoding ', feature)\n","            lbl = preprocessing.LabelEncoder()\n","            lbl.fit(list(df[feature].values))\n","            df[feature] = lbl.transform(list(df[feature].values))\n","    \n","# def display_feature_target(df, feature):\n","#     fig, ax = plt.subplots()\n","#     ax.scatter(x = df[feature], y = df['logerror'])\n","#     plt.ylabel('logerror', fontsize=13)\n","#     plt.xlabel(feature, fontsize=13)\n","#     plt.show()\n","    \n","def _process_data():\n","    print('\\n\\nSTEP1: processing data ...')\n","    \n","    global train_merge\n","    global test1_merge\n","        \n","    # load data\n","    print('\\nLoading data ...')\n","    \n","    train_label = pd.read_csv(train_path + 'train_label.csv')\n","    train_activity = pd.read_csv(train_path + 'train_activity.csv')\n","    train_combat = pd.read_csv(train_path + 'train_combat.csv')\n","    train_payment = pd.read_csv(train_path + 'train_payment.csv')\n","    train_pledge = pd.read_csv(train_path + 'train_pledge.csv')\n","    train_trade = pd.read_csv(train_path + 'train_trade.csv')\n","\n","    test1_activity = pd.read_csv(test_path + 'test1_activity.csv')\n","    test1_combat = pd.read_csv(test_path + 'test1_combat.csv')\n","    test1_payment = pd.read_csv(test_path + 'test1_payment.csv')\n","    test1_pledge = pd.read_csv(test_path + 'test1_pledge.csv')\n","    test1_trade = pd.read_csv(test_path + 'test1_trade.csv')\n","\n","    \n","    # merge payment\n","    print('\\nGrouping activity data ...')\n","\n","    train_activity['game_money_change_minus'] = train_activity['game_money_change'] #game_money_change를 마이너스끼리, 플러스끼리 분리\n","    train_activity['game_money_change_minus'] = [0 if i > 0 else i for i in train_activity['game_money_change_minus']]\n","    train_activity['game_money_change'] = [0 if i < 0 else i for i in train_activity['game_money_change']]\n","\n","    test1_activity['game_money_change_minus'] = test1_activity['game_money_change']\n","    test1_activity['game_money_change_minus'] = [0 if i > 0 else i for i in test1_activity['game_money_change_minus']]\n","    test1_activity['game_money_change'] = [0 if i < 0 else i for i in test1_activity['game_money_change']]\n","\n","    activity_agg = {\n","        'day':'nunique', 'char_id':'nunique','server':'nunique', 'playtime':'sum', 'npc_kill':'sum', \n","        'solo_exp':'sum','party_exp':'sum','quest_exp':'sum','rich_monster':'sum', \n","        'death':'sum', 'revive':'sum','exp_recovery':'sum','fishing':'sum','private_shop':'sum',\n","        'game_money_change':'sum', 'game_money_change_minus':'sum','enchant_count':'sum'}\n","    \n","    train_activity_merge = train_activity.groupby('acc_id', as_index = False).agg(activity_agg).reset_index(drop=True)\n","    test1_activity_merge = test1_activity.groupby('acc_id', as_index = False).agg(activity_agg).reset_index(drop=True)\n","    \n","    print('activity id unique and shape :')\n","    print(train_activity_merge['acc_id'].nunique(), train_activity_merge.shape)\n","    print(test1_activity_merge['acc_id'].nunique(), test1_activity_merge.shape)\n","    \n","    \n","    # merge payment\n","    print('\\nGrouping payment data ...')\n","    \n","    train_payment_merge = train_payment.groupby( 'acc_id', as_index = False).agg({'day':'nunique','amount_spent':'max'}).reset_index(drop=True)\n","    test1_payment_merge = test1_payment.groupby('acc_id', as_index = False).agg({'day':'nunique','amount_spent':'max'}).reset_index(drop=True)\n","\n","    train_payment_merge.rename(columns={'amount_spent':'max_spent'}, inplace=True) #amout_spent를 일일 최고 구매량으로 변경\n","    test1_payment_merge.rename(columns={'amount_spent':'max_spent'}, inplace=True)\n","\n","    print('payment id unique and shape :')\n","    print(train_payment_merge['acc_id'].nunique(), train_payment_merge.shape)\n","    print(test1_payment_merge['acc_id'].nunique(), test1_payment_merge.shape)\n","    \n","    \n","    # merge combat \n","    print('\\nGrouping combat data ...')\n","    class_names = {0:'liege', 1:'knight', 2:'elf', 3:'magician', 4:'dark-elf', 5:'dragon',6:'illusionist',7:'warrior'}\n","    for elem in train_combat['class'].unique():train_combat[class_names[elem]] = train_combat['class'] == elem\n","    for elem in test1_combat['class'].unique():test1_combat[class_names[elem]] = test1_combat['class'] == elem\n","    \n","    combat_agg = {'day':'nunique','char_id' : 'nunique', 'server' : 'nunique', 'class' : 'nunique', 'level' : 'max',\n","                  'pledge_cnt' : 'sum','random_attacker_cnt' : 'sum','random_defender_cnt' : 'sum', 'temp_cnt' : 'sum',\n","                  'same_pledge_cnt' : 'sum','etc_cnt' : 'sum','num_opponent' : 'sum','elf' : 'sum','magician' : 'sum',\n","                  'warrior' : 'sum','dark-elf' : 'sum','knight' : 'sum','dragon' : 'sum','liege' : 'sum','illusionist' : 'sum'}\n","        \n","    combat_rename = {'random_attacker_cnt':'combat_random_attacker_cnt','random_defender_cnt' : 'combat_random_defender_cnt',\n","                     'temp_cnt' : 'combat_temp_cnt','same_pledge_cnt' : 'combat_same_pledge_cnt','etc_cnt' : 'combat_etc_cnt'}\n","\n","    train_combat_merge = train_combat.groupby('acc_id', as_index = False).agg(combat_agg).reset_index(drop=True)\n","    test1_combat_merge = test1_combat.groupby('acc_id', as_index = False).agg(combat_agg).reset_index(drop=True)\n","  \n","    train_combat_merge.rename(columns=combat_rename, inplace=True)\n","    test1_combat_merge.rename(columns=combat_rename, inplace=True)\n","\n","    print('combat id unique and shape :')\n","    print(train_combat_merge['acc_id'].nunique(), train_combat_merge.shape)\n","    print(test1_combat_merge['acc_id'].nunique(), test1_combat_merge.shape)\n","    \n","    \n","    # merge combat \n","    print('\\nGrouping pledge data ...')\n","    \n","    pledge_agg = {'day':'nunique','char_id':'nunique','server':'nunique','pledge_id':'nunique', 'play_char_cnt' : 'sum',\n","                  'combat_char_cnt' : 'sum','pledge_combat_cnt' : 'sum','random_attacker_cnt' : 'sum','random_defender_cnt' : 'sum',\n","                  'same_pledge_cnt' : 'sum', 'temp_cnt' : 'sum','etc_cnt' : 'sum','combat_play_time' : 'sum','non_combat_play_time' : 'sum'}\n","    \n","    train_pledge_merge = train_pledge.groupby('acc_id', as_index = False).agg(pledge_agg).reset_index(drop=True)\n","    test1_pledge_merge = test1_pledge.groupby('acc_id', as_index = False).agg(pledge_agg).reset_index(drop=True)\n","    \n","    print('pledge id unique and shape :')\n","    print(train_pledge_merge['acc_id'].nunique(), train_pledge_merge.shape)\n","    print(test1_pledge_merge['acc_id'].nunique(), test1_pledge_merge.shape)\n","    \n","    \n","    #merge all data\n","    print('\\nMerging all data ...')\n","    train_merge = train_label.merge(train_activity_merge, on = ['acc_id'], how ='left').merge(\n","    train_combat_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    train_pledge_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    train_payment_merge.drop(['day'], axis=1), on = ['acc_id'], how ='left')\n","        \n","    test1_merge = test1_activity_merge.merge(\n","    test1_combat_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    test1_pledge_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    test1_payment_merge.drop(['day'], axis=1), on = ['acc_id'], how ='left')\n","    \n","    print('\\nmerge id unique and shape :')\n","    print(train_merge['acc_id'].nunique(), train_merge.shape)\n","    print(test1_merge['acc_id'].nunique(), test1_merge.shape)\n","    \n","    \n","    # fill NA\n","#     fill_NA(train_merge)\n","    \n","    # encode features\n","#     encode_features(train_merge)\n","    \n","### STEP1.1 : prepare train and vaild data\n","\n","def _prepare_data():\n","    # prepare train and valid data\n","    print('\\nPreparing train and valid data ...')\n","\n","    global train_x\n","    global train_ya\n","    global train_ys\n","    \n","    global valid_x\n","    global valid_ya\n","    global valid_ys\n","    \n","    global test_x\n","    global test_acc_id\n","    \n","    drop_vars = ['acc_id','amount_spent','survival_time']\n","    \n","    train_ya = train_merge['amount_spent']\n","    train_ys = train_merge['survival_time']\n","    \n","    train_x = train_merge\n","    train_x.drop(columns=drop_vars, inplace=True)\n","    \n","    valid_x = train_x[30000:]\n","    valid_ya = train_ya[30000:]\n","    \n","    train_x = train_x[:30000]\n","    train_ya = train_ya[:30000]\n","    \n","    valid_ys = train_ys[30000:]\n","    train_ys = train_ys[:30000]\n","    \n","    print('train x shape: ', train_x.shape)\n","    print('train ya shape: ', train_ya.shape)\n","    print('train ys shape: ', train_ys.shape)\n","    \n","    print('valid x shape: ', valid_x.shape)\n","    print('valid ya shape: ', valid_ya.shape)\n","    print('valid ys shape: ', valid_ys.shape)\n","    \n","    \n","    # prepare test data\n","    print('\\nPreparing test data ...')\n","    \n","    test_x = test1_merge\n","    test_acc_id = test1_merge['acc_id']\n","    test_x.drop(columns=['acc_id'], inplace=True)\n","    print('test x shape: ', test_x.shape)\n","\n","    \n","## STEP2: build model\n","def _build_model():\n","    print('\\n\\nSTEP2: building model ...')\n","    \n","    global xgb_params\n","    xgb_params = {\n","        'eta': 0.007,\n","        'max_depth': 6, \n","        'subsample': 0.6,\n","        'objective': 'reg:linear',\n","        'eval_metric': 'mae',\n","        'lambda': 9.0,\n","        'alpha': 0.8,\n","        'colsample_bytree': 0.7,\n","        'silent': 1,\n","        'random_state' : 42\n","    }\n","    \n","\n","## STEP3: train    \n","def _train_predict_amount():\n","    print('\\n\\nSTEP3: training amount...')\n","    \n","    global a_xgb_clf\n","    global a_pred\n","    \n","    ad_train = xgb.DMatrix(train_x, label=train_ya)\n","    ad_valid = xgb.DMatrix(valid_x, label=valid_ya)\n","    \n","    a_evals = [(ad_train, 'train'), (ad_valid, 'valid')]\n","    a_xgb_clf = xgb.train(xgb_params, ad_train, num_boost_round=10000, evals=a_evals, \n","                        early_stopping_rounds=100, verbose_eval=10)\n","\n","    ad_test = xgb.DMatrix(test_x)\n","    a_pred = a_xgb_clf.predict(ad_test)\n","    \n","def _train_predict_survival():\n","    print('\\n\\nSTEP4: training survival...')\n","    \n","    global s_xgb_clf\n","    global s_pred\n","    \n","    sd_train = xgb.DMatrix(train_x, label=train_ys)\n","    sd_valid = xgb.DMatrix(valid_x, label=valid_ys)\n","    \n","    s_evals = [(sd_train, 'train'), (sd_valid, 'valid')]\n","    s_xgb_clf = xgb.train(xgb_params, sd_train, num_boost_round=10000, evals=s_evals, \n","                        early_stopping_rounds=100, verbose_eval=10)\n","\n","    sd_test = xgb.DMatrix(test_x)\n","    s_pred = s_xgb_clf.predict(sd_test)\n","    \n","\n","## STEP5: generate submission \n","def _generate_submission():\n","    print('\\n\\nSTEP5: generating submission ...')\n","    \n","    filename = 'test1_predict.csv'\n","    \n","    test_predict = pd.DataFrame(columns = ['acc_id', 'survival_time','amount_spent'])\n","    \n","    test_predict['acc_id'] = test_acc_id\n","    test_predict['survival_time'] = s_pred\n","    test_predict['amount_spent'] = a_pred\n","    \n","    test_predict['amount_spent'] = [1 if i < 1 else i for i in test_predict['amount_spent']]\n","    test_predict['survival_time'] = [1 if i < 1 else i for i in test_predict['survival_time']]\n","    \n","    test_predict.to_csv(submission_path + filename,index=False)    \n"," "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YhAnTwxPKN_5","colab_type":"text"},"source":["### etc"]},{"cell_type":"code","metadata":{"id":"N-zXciZw2iAM","colab_type":"code","colab":{}},"source":["\n","from enum import Enum\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.metrics import mean_squared_error\n","from sklearn import preprocessing\n","\n","import xgboost as xgb\n","from xgboost import XGBRegressor\n","import lightgbm as lgb\n","\n","from datetime import datetime\n","import gc\n","\n","\n","from math import sqrt\n","\n","\n","import warnings\n","def ignore_warn(*args, **kwargs):\n","    pass\n","warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n","\n","\n","## STEP0: do setting\n","class Settings(Enum):\n","    global train_path\n","    global test_path\n","    global submission_path\n","    global testname\n","    \n","    testname = 'test2'\n","    \n","    path            = 'drive/My Drive/bigcontest2019/'\n","    train_path      = path + 'data/train/'\n","    test_path       = path + 'data/test/'\n","    submission_path = path + 'scripts/model/metrics/nes_inference/'\n","        \n","    def __str__(self):\n","        return self.value\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5t5jwqyl3_Z1","colab_type":"code","outputId":"fcb63a13-114c-45d9-e6f8-55da7a423f45","executionInfo":{"status":"ok","timestamp":1566788814259,"user_tz":-540,"elapsed":556,"user":{"displayName":"Eunice","photoUrl":"https://lh5.googleusercontent.com/-cSzctfkN1RA/AAAAAAAAAAI/AAAAAAAAQlo/izeHohCao7Y/s64/photo.jpg","userId":"07489983497392537979"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["a = test_trade['source_acc_id'].unique()\n","b = test_acc_id\n","len(set(a) & set(b))     # & is intersection - elements common to both"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["12362"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"jAxOjNVXCpoD","colab_type":"code","colab":{}},"source":["test1_predict = pd.read_csv(submission_path + 'test1_predict.csv')\n","test1_predict['amount_spent'] = test1_predict['amount_spent']*16\n","test1_predict.to_csv(submission_path + 'test1_predict_t.csv',index=False) \n","\n","test2_predict = pd.read_csv(submission_path + 'test2_predict.csv')\n","test2_predict['amount_spent'] = test2_predict['amount_spent']*3\n","test2_predict.to_csv(submission_path + 'test2_predict_t.csv',index=False) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CTZnVzEjuZVe","colab_type":"code","outputId":"322afafe-fbba-407f-e57a-3073318e4f66","executionInfo":{"status":"error","timestamp":1566999209552,"user_tz":-540,"elapsed":593,"user":{"displayName":"Eunice","photoUrl":"https://lh5.googleusercontent.com/-cSzctfkN1RA/AAAAAAAAAAI/AAAAAAAAQlo/izeHohCao7Y/s64/photo.jpg","userId":"07489983497392537979"}},"colab":{"base_uri":"https://localhost:8080/","height":181}},"source":["test1_predict['survival_time'] = [63 if i > 63 else i for i in test_predict['survival_time']] # 63 이상 값 63 처리\n","test1_predict.to_csv(submission_path + testname + '_predict.csv',index=False)    "],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-70-b70787e8668f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest1_predict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'survival_time'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m63\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m63\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_predict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'survival_time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# 63 이상 값 63 처리\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest1_predict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmission_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtestname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_predict.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'test_predict' is not defined"]}]},{"cell_type":"code","metadata":{"id":"PITWEJ5bYedD","colab_type":"code","colab":{}},"source":["train_merge = pd.read_csv(submission_path  + 'merge/' + 'train_merge.csv')\n","test1_merge = pd.read_csv(submission_path  + 'merge/' + 'test1_merge.csv')\n","test1_merge = pd.read_csv(submission_path  + 'merge/' + 'test1_merge.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ietgnLuMIOUX","colab_type":"code","outputId":"2ce0ce3f-0e20-4507-b9b4-6e849d57b788","executionInfo":{"status":"ok","timestamp":1567832367247,"user_tz":-540,"elapsed":805,"user":{"displayName":"Eunice","photoUrl":"https://lh5.googleusercontent.com/-cSzctfkN1RA/AAAAAAAAAAI/AAAAAAAAQlo/izeHohCao7Y/s64/photo.jpg","userId":"07489983497392537979"}},"colab":{"base_uri":"https://localhost:8080/","height":306}},"source":["train_merge['']"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['acc_id', 'day', 'char_id', 'server', 'npc_kill', 'solo_exp',\n","       'party_exp', 'quest_exp', 'rich_monster', 'death', 'exp_recovery',\n","       'private_shop', 'game_money_change', 'game_money_change_minus',\n","       'enchant_count', 'level', 'pledge_cnt', 'liege', 'pledge_id',\n","       'pledge_combat_cnt', 'combat_play_time', 'non_combat_play_time',\n","       'max_spent', 'mean_spent', 'trade_day_x', 'trade_server_x',\n","       'item_amount_x', 't23-02_x', 't03-06_x', 't07-10_x', 't11-14_x',\n","       't15-18_x', 't19-22_x', 'item_price_x', 'trade_shop_x',\n","       'personal_shop_x', 'accessory_x', 'adena_x', 'armor_x',\n","       'enchant_scroll_x', 'etc_x', 'spell_x', 'weapon_x', 'source',\n","       'trade_day_y', 'trade_server_y', 'item_amount_y', 't23-02_y',\n","       't03-06_y', 't07-10_y', 't11-14_y', 't15-18_y', 't19-22_y',\n","       'item_price_y', 'trade_shop_y', 'personal_shop_y', 'accessory_y',\n","       'adena_y', 'armor_y', 'enchant_scroll_y', 'etc_y', 'spell_y',\n","       'weapon_y', 'target', 'attack', 'cnt', 'pledge_ratio', 'day_spent',\n","       'survival_time', 'amount_spent'],\n","      dtype='object')"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"N0hTE9LRKB5e","colab_type":"code","colab":{}},"source":["train_payment = pd.read_csv(train_path + 'train_payment.csv')\n","test1_payment = pd.read_csv(test_path + 'test1' + '_payment.csv')\n","test2_payment = pd.read_csv(test_path + 'test2' + '_payment.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hCb7XGlfGcEL","colab_type":"code","outputId":"53360d4b-796a-427e-b362-fcbc43871c37","executionInfo":{"status":"ok","timestamp":1567832637017,"user_tz":-540,"elapsed":677,"user":{"displayName":"Eunice","photoUrl":"https://lh5.googleusercontent.com/-cSzctfkN1RA/AAAAAAAAAAI/AAAAAAAAQlo/izeHohCao7Y/s64/photo.jpg","userId":"07489983497392537979"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["print(train_payment['amount_spent'].min(),train_payment['amount_spent'].mean(),train_payment['amount_spent'].max())\n","print(test1_payment['amount_spent'].min(),test1_payment['amount_spent'].mean(),test1_payment['amount_spent'].max())\n","print(test2_payment['amount_spent'].min(),test2_payment['amount_spent'].mean(),test2_payment['amount_spent'].max())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.01173470155204053 0.6681977124565469 11.733528081885328\n","0.01173470155204053 0.7679355719841668 10.561231396836476\n","0.01173470155204053 0.8403611243368606 11.73470155204053\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EUmSLsBhGdf5","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}