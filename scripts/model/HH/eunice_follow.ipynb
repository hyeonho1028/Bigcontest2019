{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"eunice_follow.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"jZu7W-inOlaE","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","SEED = 42"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ij3Uv-2fdLy7","colab_type":"text"},"source":["#### MAIN"]},{"cell_type":"code","metadata":{"id":"5kzsJLiVdK7I","colab_type":"code","outputId":"64716ca0-187b-467e-af09-0c1c6bc4d5e1","executionInfo":{"status":"error","timestamp":1567261529853,"user_tz":-540,"elapsed":2389,"user":{"displayName":"허현","photoUrl":"","userId":"15943349948421319767"}},"colab":{"base_uri":"https://localhost:8080/","height":397}},"source":["## main\n","def main():\n","    _process_data()\n","    _prepare_data()\n","#     _load_merge_data()\n","    _build_model()\n","    _train_predict_amount()\n","    _train_predict_survival()\n","    _generate_submission()\n","    \n","\n","if __name__ == \"__main__\":\n","    \n","    testname = 'test1'\n","    print(('*'* 50) +'\\ntry model to test1 data\\n' + ('*'*50))\n","    main()\n","    print('\\n\\n\\nThe end.\\n\\n\\n')\n","    \n","#     testname = 'test2'\n","#     print(('*'* 50) +'\\ntry model to test2 data\\n' + ('*'*50))\n","#     main()\n","#     print('\\n\\n\\nThe end.\\n\\n\\n')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["**************************************************\n","try model to test1 data\n","**************************************************\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-9f7ffabbdb36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtestname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'test1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'*'\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'\\ntry model to test1 data\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'*'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n\\nThe end.\\n\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-9f7ffabbdb36>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0m_prepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     _load_merge_data()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0m_build_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name '_process_data' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"LzRg1SsfG69V","colab_type":"text"},"source":["#### try 7 - delete and merge variables \n","(2019-08-30 09:43:50 PM\t6772.33\t2496.24\t9268.57)"]},{"cell_type":"code","metadata":{"id":"CmkNLV2YG6de","colab_type":"code","colab":{}},"source":["\n","# -*- coding: utf-8 -*-\n","\n","\n","from enum import Enum\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","from sklearn import preprocessing\n","import xgboost as xgb\n","from datetime import datetime\n","import gc\n","\n","import warnings\n","def ignore_warn(*args, **kwargs):\n","    pass\n","warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n","\n","\n","## STEP0: do setting\n","class Settings(Enum):\n","    global train_path\n","    global test_path\n","    global submission_path\n","    global testname\n","    global codeversion\n","    \n","    codeversion = 'try 7 - delete and merge variables'\n","    \n","    path            = 'drive/My Drive/bigcontest2019/'\n","    train_path      = path + 'data/train/'\n","    test_path       = path + 'data/test/'\n","    submission_path = path + 'scripts/model/metrics/nes_inference/'\n","        \n","    def __str__(self):\n","        return self.value\n","        \n","    \n","## STEP1: process data    \n","def fill_NA(df):\n","    print('\\nFilling NA ...')\n","    \n","    na_ratio = ((df.isnull().sum() / len(df)) * 100).sort_values(ascending=False)\n","    print('NA ratio: ')\n","    print(na_ratio) \n","    \n","    for feature in df:\n","        if df[feature].dtype == 'object':\n","            df[feature] = df[feature].fillna(\"None\")\n","        else:\n","            df[feature] = df[feature].fillna(0)\n","    \n","def encode_features(df):\n","    print('\\nEncoding features ...')\n","    \n","    for feature in df:\n","        if df[feature].dtype == 'object':\n","            print('Encoding ', feature)\n","            lbl = preprocessing.LabelEncoder()\n","            lbl.fit(list(df[feature].values))\n","            df[feature] = lbl.transform(list(df[feature].values))\n","    \n","# def display_feature_target(df, feature):\n","#     fig, ax = plt.subplots()\n","#     ax.scatter(x = df[feature], y = df['logerror'])\n","#     plt.ylabel('logerror', fontsize=13)\n","#     plt.xlabel(feature, fontsize=13)\n","#     plt.show()\n","    \n","def _process_data():\n","    \n","    print('CODE VERSION : ', codeversion)\n","    print('\\n\\nSTEP1: processing data ...')\n","    \n","    global train_merge\n","    global test_merge\n","\n","        \n","    # load data\n","    print('\\nLoading data ...')\n","    \n","    train_label = pd.read_csv(train_path + 'train_label.csv')\n","    train_activity = pd.read_csv(train_path + 'train_activity.csv')\n","    train_combat = pd.read_csv(train_path + 'train_combat.csv')\n","    train_payment = pd.read_csv(train_path + 'train_payment.csv')\n","    train_pledge = pd.read_csv(train_path + 'train_pledge.csv')\n","    train_trade = pd.read_csv(train_path + 'train_trade.csv')\n","\n","    test_activity = pd.read_csv(test_path + testname + '_activity.csv')\n","    test_combat = pd.read_csv(test_path + testname + '_combat.csv')\n","    test_payment = pd.read_csv(test_path + testname + '_payment.csv')\n","    test_pledge = pd.read_csv(test_path + testname + '_pledge.csv')\n","    test_trade = pd.read_csv(test_path  + testname + '_trade.csv')\n","\n","    \n","    \n","    print('\\nGrouping activity data ...')\n","\n","    train_activity['game_money_change_minus'] = train_activity['game_money_change'] #game_money_change를 마이너스끼리, 플러스끼리 분리\n","    train_activity['game_money_change_minus'] = [0 if i > 0 else i for i in train_activity['game_money_change_minus']]\n","    train_activity['game_money_change'] = [0 if i < 0 else i for i in train_activity['game_money_change']]\n","\n","    test_activity['game_money_change_minus'] = test_activity['game_money_change']\n","    test_activity['game_money_change_minus'] = [0 if i > 0 else i for i in test_activity['game_money_change_minus']]\n","    test_activity['game_money_change'] = [0 if i < 0 else i for i in test_activity['game_money_change']]\n","\n","    activity_agg = {\n","        'day':'nunique', 'char_id':'nunique','server':'nunique', 'playtime':'sum', 'npc_kill':'sum', \n","        'solo_exp':'sum','party_exp':'sum','quest_exp':'sum','rich_monster':'sum', \n","        'death':'sum', 'revive':'sum','exp_recovery':'sum','fishing':'sum','private_shop':'sum',\n","        'game_money_change':'sum', 'game_money_change_minus':'sum','enchant_count':'sum'}\n","    \n","    train_activity_merge = train_activity.groupby('acc_id', as_index = False).agg(activity_agg).reset_index(drop=True)\n","    test_activity_merge = test_activity.groupby('acc_id', as_index = False).agg(activity_agg).reset_index(drop=True)\n","    \n","    acc_id_list = train_activity['acc_id'].unique().tolist()\n","    login_day_max = []\n","    logout_day = []\n","    \n","    print('\\n + making day variables for train activity')\n","\n","    for i in acc_id_list :\n","        dayunique = train_activity[train_activity['acc_id'] == i]['day'].unique().tolist() # i아이디의 접속일 list화\n","        in_list = [0] * 28\n","        in_value = 0\n","        daylogin = 1\n","\n","        out_list = [0] * 28\n","        out_value = 0\n","        day_logout = 0\n","\n","        # login_day_max 변수 생성 (acc_id가 28일 중 최대 연속으로 접속한 일 수. 띄엄띄엄 접속자보다 매일 꾸준히 하는 사람 데이터를 모으고 싶었음)\n","        for j in range(0, len(dayunique)-1) :        \n","            if dayunique[j]+1 == dayunique[j+1] : # 전날 대비 연속 접속한 경우에 연속값 카운트\n","                daylogin += 1 \n","                in_list[in_value] = daylogin  # 연속값 저장\n","            else :\n","                in_value += 1 \n","                daylogin = 1 # 이전 접속일과 다음 접속일이 1이상 차이날 때, 연속값 초기화\n","        login_day_max.append(max(in_list)) # 연속 접속일 리스트 중 최대값 append\n","\n","         #logout-day 변수 생성 (28일 중 후반부에만 로그인 하는 사람들과, 초반 로그인 후 접속하지 않는 유저를 구분하기 위함)\n","        for k in range(0, len(dayunique)-1) :\n","            if dayunique[k]+1 != dayunique[k+1] : #연속 접속하지 않은 경우, 다음 접속일까지 로그아웃한 일자 카운트\n","                day_logout = (dayunique[k+1]-dayunique[k]-1)\n","                out_list[out_value] = day_logout\n","            else :\n","                out_value += 1\n","                day_logout = 0 #접속한 경우 pass 및 연속값 초기화\n","        logout_day.append(max(out_list)) \n","\n","    all_day = pd.DataFrame(columns = ['acc_id', 'login_day_max','logout_day']) # 빈프레임에 day 관련 데이터 2개 추가\n","    all_day['acc_id'] = acc_id_list\n","    all_day['login_day_max'] = login_day_max\n","    all_day['logout_day'] = logout_day\n","\n","    train_activity_merge = train_activity_merge.merge(all_day, on = ['acc_id'], how ='left')\n","    \n","    print('\\n + making day variables for test activity')\n","\n","    acc_id_list = test_activity['acc_id'].unique().tolist()\n","    login_day_max = []\n","    logout_day = []\n","\n","    for i in acc_id_list :\n","        dayunique = test_activity[test_activity['acc_id'] == i]['day'].unique().tolist() # i아이디의 접속일 list화\n","        in_list = [0] * 28\n","        in_value = 0\n","        daylogin = 1\n","\n","        out_list = [0] * 28\n","        out_value = 0\n","        day_logout = 0\n","\n","        # login_day_max 변수 생성 (acc_id가 28일 중 최대 연속으로 접속한 일 수. 띄엄띄엄 접속자보다 매일 꾸준히 하는 사람 데이터를 모으고 싶었음)\n","        for j in range(0, len(dayunique)-1) :        \n","            if dayunique[j]+1 == dayunique[j+1] : # 전날 대비 연속 접속한 경우에 연속값 카운트\n","                daylogin += 1 \n","                in_list[in_value] = daylogin  # 연속값 저장\n","            else :\n","                in_value += 1 \n","                daylogin = 1 # 이전 접속일과 다음 접속일이 1이상 차이날 때, 연속값 초기화\n","        login_day_max.append(max(in_list)) # 연속 접속일 리스트 중 최대값 append\n","\n","         #logout-day 변수 생성 (28일 중 후반부에만 로그인 하는 사람들과, 초반 로그인 후 접속하지 않는 유저를 구분하기 위함)\n","        for k in range(0, len(dayunique)-1) :\n","            if dayunique[k]+1 != dayunique[k+1] : #연속 접속하지 않은 경우, 다음 접속일까지 로그아웃한 일자 카운트\n","                day_logout = (dayunique[k+1]-dayunique[k]-1)\n","                out_list[out_value] = day_logout\n","            else :\n","                out_value += 1\n","                day_logout = 0 #접속한 경우 pass 및 연속값 초기화\n","        logout_day.append(max(out_list)) \n","\n","    all_day = pd.DataFrame(columns = ['acc_id', 'login_day_max','logout_day']) # 빈프레임에 day 관련 데이터 2개 추가\n","    all_day['acc_id'] = acc_id_list\n","    all_day['login_day_max'] = login_day_max\n","    all_day['logout_day'] = logout_day\n","\n","    test_activity_merge = test_activity_merge.merge(all_day, on = ['acc_id'], how ='left')\n","    \n","    \n","    print('activity id unique and shape :')\n","    print(train_activity_merge['acc_id'].nunique(), train_activity_merge.shape)\n","    print(test_activity_merge['acc_id'].nunique(), test_activity_merge.shape)\n","    \n","    \n","    # merge payment\n","    print('\\nGrouping payment data ...')\n","    \n","#     train_payment_merge = train_payment.groupby( 'acc_id', as_index = False).agg({'day':'nunique','amount_spent':'max'}).reset_index(drop=True)\n","#     test_payment_merge = test_payment.groupby('acc_id', as_index = False).agg({'day':'nunique','amount_spent':'max'}).reset_index(drop=True)\n","\n","#     train_payment_merge.rename(columns={'amount_spent':'max_spent'}, inplace=True) #amout_spent를 일일 최고 구매량으로 변경\n","#     test_payment_merge.rename(columns={'amount_spent':'max_spent'}, inplace=True)\n","\n","\n","\n","    train_payment_merge = train_payment.groupby( 'acc_id', as_index = False).agg({'day':'nunique','amount_spent':'mean'}).reset_index(drop=True)\n","    test_payment_merge = test_payment.groupby('acc_id', as_index = False).agg({'day':'nunique','amount_spent':'mean'}).reset_index(drop=True)\n","\n","    train_payment_merge.rename(columns={'amount_spent':'mean_spent'}, inplace=True) #amout_spent를 일일 평균 구매량으로 변경\n","    test_payment_merge.rename(columns={'amount_spent':'mean_spent'}, inplace=True)\n","\n","    print('payment id unique and shape :')\n","    print(train_payment_merge['acc_id'].nunique(), train_payment_merge.shape)\n","    print(test_payment_merge['acc_id'].nunique(), test_payment_merge.shape)\n","    \n","    \n","    # merge combat \n","    print('\\nGrouping combat data ...')\n","    class_names = {0:'liege', 1:'knight', 2:'elf', 3:'magician', 4:'dark-elf', 5:'dragon',6:'illusionist',7:'warrior'}\n","    for elem in train_combat['class'].unique():train_combat[class_names[elem]] = train_combat['class'] == elem\n","    for elem in test_combat['class'].unique():test_combat[class_names[elem]] = test_combat['class'] == elem\n","    \n","    combat_agg = {'day':'nunique','char_id' : 'nunique', 'server' : 'nunique', 'class' : 'nunique', 'level' : 'max',\n","                  'pledge_cnt' : 'sum','random_attacker_cnt' : 'sum','random_defender_cnt' : 'sum', 'temp_cnt' : 'sum',\n","                  'same_pledge_cnt' : 'sum','etc_cnt' : 'sum','num_opponent' : 'sum','elf' : 'sum','magician' : 'sum',\n","                  'warrior' : 'sum','dark-elf' : 'sum','knight' : 'sum','dragon' : 'sum','liege' : 'sum','illusionist' : 'sum'}\n","        \n","    combat_rename = {'random_attacker_cnt':'combat_random_attacker_cnt','random_defender_cnt' : 'combat_random_defender_cnt',\n","                     'temp_cnt' : 'combat_temp_cnt','same_pledge_cnt' : 'combat_same_pledge_cnt','etc_cnt' : 'combat_etc_cnt'}\n","\n","    train_combat_merge = train_combat.groupby('acc_id', as_index = False).agg(combat_agg).reset_index(drop=True)\n","    test_combat_merge = test_combat.groupby('acc_id', as_index = False).agg(combat_agg).reset_index(drop=True)\n","  \n","    train_combat_merge.rename(columns=combat_rename, inplace=True)\n","    test_combat_merge.rename(columns=combat_rename, inplace=True)\n","\n","    print('combat id unique and shape :')\n","    print(train_combat_merge['acc_id'].nunique(), train_combat_merge.shape)\n","    print(test_combat_merge['acc_id'].nunique(), test_combat_merge.shape)\n","    \n","    \n","    # merge combat \n","    print('\\nGrouping pledge data ...')\n","    \n","    pledge_agg = {'day':'nunique','char_id':'nunique','server':'nunique','pledge_id':'nunique', 'play_char_cnt' : 'sum',\n","                  'combat_char_cnt' : 'sum','pledge_combat_cnt' : 'sum','random_attacker_cnt' : 'sum','random_defender_cnt' : 'sum',\n","                  'same_pledge_cnt' : 'sum', 'temp_cnt' : 'sum','etc_cnt' : 'sum','combat_play_time' : 'sum','non_combat_play_time' : 'sum'}\n","    \n","    train_pledge_merge = train_pledge.groupby('acc_id', as_index = False).agg(pledge_agg).reset_index(drop=True)\n","    test_pledge_merge = test_pledge.groupby('acc_id', as_index = False).agg(pledge_agg).reset_index(drop=True)\n","    \n","    print('pledge id unique and shape :')\n","    print(train_pledge_merge['acc_id'].nunique(), train_pledge_merge.shape)\n","    print(test_pledge_merge['acc_id'].nunique(), test_pledge_merge.shape)\n","    \n","    \n","    # merge trade \n","    print('\\nGrouping trade data ...')\n","\n","    # item_type dummies             \n","    train_item_type_dummy = pd.get_dummies(train_trade['item_type'])\n","    test_item_type_dummy = pd.get_dummies(test_trade['item_type'])\n","\n","    # trade_type dummies\n","    type_names = {0:'personal_shop', 1:'trade_shop'}\n","    for elem in train_trade['type'].unique():train_trade[type_names[elem]] = train_trade['type'] == elem\n","    for elem in test_trade['type'].unique():test_trade[type_names[elem]] = test_trade['type'] == elem\n","\n","    # merge dummies\n","    \n","    train_trade = train_trade.drop(columns=['type','item_type'])\n","    train_trade = pd.concat([train_trade, train_item_type_dummy], axis = 1)\n","    \n","    test_trade = test_trade.drop(columns=['type','item_type'])\n","    test_trade = pd.concat([test_trade, test_item_type_dummy], axis = 1)\n","\n","    # rename columns\n","    trade_rename = {'day':'trade_day','server' : 'trade_server', 'time' : 'trade_time'}\n","    train_trade.rename(columns=trade_rename, inplace=True)\n","    test_trade.rename(columns=trade_rename, inplace=True)\n","\n","    #divide source and target id\n","    \n","    train_source_trade = train_trade.drop(columns = ['target_acc_id','source_char_id','target_char_id']).rename(columns={'source_acc_id' : 'acc_id'})\n","    train_source_trade['source'] = 1\n","    train_target_trade = train_trade.drop(columns = ['source_acc_id','source_char_id','target_char_id']).rename(columns={'target_acc_id' : 'acc_id'})\n","    train_target_trade['target'] = 1\n","    \n","    test_source_trade = test_trade.drop(columns = ['target_acc_id','source_char_id','target_char_id']).rename(columns={'source_acc_id' : 'acc_id'})\n","    test_source_trade['source'] = 1\n","    test_target_trade = test_trade.drop(columns = ['source_acc_id','source_char_id','target_char_id']).rename(columns={'target_acc_id' : 'acc_id'})\n","    test_target_trade['target'] = 1\n","\n","\n","    #aggregate\n","    trade_source_agg = {'trade_day':'nunique', 'trade_time' : 'nunique', 'trade_server' : 'nunique','item_amount' : 'sum',\n","                        'item_price': 'sum', 'trade_shop' : 'sum', 'personal_shop' : 'sum', 'accessory' : 'sum', 'adena' : 'sum',\n","                        'armor' : 'sum', 'enchant_scroll' : 'sum', 'etc' : 'sum', 'spell' : 'sum', 'weapon' : 'sum', 'source' : 'sum'}\n","\n","    trade_target_agg = {'trade_day':'nunique', 'trade_time' : 'nunique', 'trade_server' : 'nunique','item_amount' : 'sum',\n","                        'item_price': 'sum', 'trade_shop' : 'sum', 'personal_shop' : 'sum', 'accessory' : 'sum', 'adena' : 'sum',\n","                        'armor' : 'sum', 'enchant_scroll' : 'sum', 'etc' : 'sum', 'spell' : 'sum', 'weapon' : 'sum', 'target' : 'sum'}\n","\n","    train_source_trade = train_source_trade.groupby('acc_id', as_index = False).agg(trade_source_agg).reset_index(drop=True)\n","    train_target_trade = train_target_trade.groupby('acc_id', as_index = False).agg(trade_target_agg).reset_index(drop=True)\n","    \n","    test_source_trade = test_source_trade.groupby('acc_id', as_index = False).agg(trade_source_agg).reset_index(drop=True)\n","    test_target_trade = test_target_trade.groupby('acc_id', as_index = False).agg(trade_target_agg).reset_index(drop=True)\n","\n","    print('trade id unique and shape :')\n","    print(train_source_trade['acc_id'].nunique(), train_source_trade.shape, train_target_trade['acc_id'].nunique(), train_target_trade.shape)\n","    print(test_source_trade['acc_id'].nunique(), test_source_trade.shape, test_target_trade['acc_id'].nunique(), test_target_trade.shape)\n","    \n","    \n","    \n","    #merge all data\n","    print('\\nMerging all data ...')\n","    train_merge = train_label.merge(train_activity_merge, on = ['acc_id'], how ='left').merge(\n","    train_combat_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    train_pledge_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    train_payment_merge.drop(['day'], axis=1), on = ['acc_id'], how ='left').merge(\n","    train_source_trade, on = ['acc_id'], how = 'left').merge(\n","    train_target_trade, on = ['acc_id'], how = 'left')\n","        \n","    test_merge = test_activity_merge.merge(\n","    test_combat_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    test_pledge_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    test_payment_merge.drop(['day'], axis=1), on = ['acc_id'], how ='left').merge(\n","    test_source_trade, on = ['acc_id'], how = 'left').merge(\n","    test_target_trade, on = ['acc_id'], how = 'left')\n","    \n","    train_merge['attack'] = (train_merge['random_attacker_cnt']+train_merge['combat_random_attacker_cnt']) -(train_merge['random_defender_cnt'] + train_merge['combat_random_defender_cnt'])\n","    test_merge['attack'] = (test_merge['random_attacker_cnt']+test_merge['combat_random_attacker_cnt']) -(test_merge['random_defender_cnt'] + test_merge['combat_random_defender_cnt'])\n","    \n","    train_merge['cnt'] = train_merge['temp_cnt']+train_merge['same_pledge_cnt']+train_merge['etc_cnt'] + train_merge['combat_temp_cnt']+train_merge['combat_same_pledge_cnt']+train_merge['combat_etc_cnt']\n","    test_merge['cnt'] = test_merge['temp_cnt']+test_merge['same_pledge_cnt']+test_merge['etc_cnt'] + test_merge['combat_temp_cnt']+test_merge['combat_same_pledge_cnt']+test_merge['combat_etc_cnt']\n","    \n","    \n","    train_merge['pledge_ratio'] = train_merge['combat_char_cnt']/train_merge['play_char_cnt']\n","    test_merge['pledge_ratio'] = test_merge['combat_char_cnt']/test_merge['play_char_cnt']\n","  \n","    train_merge['day_spent'] = train_merge['day']*train_merge['mean_spent']\n","    test_merge['day_spent'] = test_merge['day']*test_merge['mean_spent']\n","    \n","    drop_merge = ['day','fishing','revive','elf','magician','warrior','dark-elf','knight','dragon','illusionist','class','random_attacker_cnt',\n","                  'random_defender_cnt','temp_cnt','same_pledge_cnt','etc_cnt','same_pledge_cnt','num_opponent','combat_temp_cnt', \n","                  'combat_same_pledge_cnt', 'combat_etc_cnt','combat_random_defender_cnt','combat_random_attacker_cnt','playtime',\n","                  'combat_char_cnt','play_char_cnt','mean_spent']\n","    \n","    \n","    \n","    train_merge = train_merge.drop(drop_merge, axis = 1)\n","    test_merge = test_merge.drop(drop_merge, axis = 1)\n","    \n","    print('\\nmerge id unique and shape :')\n","    print(train_merge['acc_id'].nunique(), train_merge.shape)\n","    print(test_merge['acc_id'].nunique(), test_merge.shape)\n","    \n","    train_merge.to_csv(submission_path + '/merge/' + 'train_merge.csv',index=False)  \n","    test_merge.to_csv(submission_path + '/merge/' + testname + '_merge.csv',index=False)  \n","    print('test and train merge data are saved on ' + submission_path + '/merge/' )\n","    \n","    \n","def _load_merge_data(): \n","\n","    train_merge = pd.read_csv(submission_path + 'merge/' + 'train_merge.csv')\n","    test_merge = pd.read_csv(submission_path + 'merge/' + testname + '_merge.csv')\n","  \n","    print('\\nmerge id unique and shape :')\n","    print(train_merge['acc_id'].nunique(), train_merge.shape)\n","    print(test_merge['acc_id'].nunique(), test_merge.shape)\n","\n","    \n","    # fill NA\n","#     fill_NA(train_merge)\n","    \n","    # encode features\n","#     encode_features(train_merge)\n","    \n","### STEP1.1 : prepare train and vaild data\n","\n","\n","\n","def _prepare_data():\n","    # prepare train and valid data\n","    print('\\nPreparing train and valid data ...')\n","\n","    global train_x\n","    global train_ya\n","    global train_ys\n","    \n","    global valid_x\n","    global valid_ya\n","    global valid_ys\n","    \n","    global test_x\n","    global test_acc_id\n","    \n","    \n","    drop_vars = ['acc_id','amount_spent','survival_time']\n","    \n","    train_ya = train_merge['amount_spent']\n","    train_ys = train_merge['survival_time']\n","    \n","    train_x = train_merge\n","    train_x.drop(columns=drop_vars, inplace=True)\n","    \n","    valid_x = train_x[30000:]\n","    valid_ya = train_ya[30000:]\n","    \n","    train_x = train_x[:30000]\n","    train_ya = train_ya[:30000]\n","    \n","    valid_ys = train_ys[30000:]\n","    train_ys = train_ys[:30000]\n","    \n","    print('train x shape: ', train_x.shape)\n","    print('train ya shape: ', train_ya.shape)\n","    print('train ys shape: ', train_ys.shape)\n","    \n","    print('valid x shape: ', valid_x.shape)\n","    print('valid ya shape: ', valid_ya.shape)\n","    print('valid ys shape: ', valid_ys.shape)\n","    \n","    \n","    # prepare test data\n","    print('\\nPreparing test data ...')\n","    \n","    test_x = test_merge\n","    test_acc_id = test_merge['acc_id']\n","    test_x.drop(columns=['acc_id'], inplace=True)\n","    print('test x shape: ', test_x.shape)\n","\n","    \n","## STEP2: build model\n","def _build_model():\n","    print('\\n\\nSTEP2: building model ...')\n","    \n","    global xgb_params\n","\n","    xgb_params = {\n","        'eta': 0.007,\n","        'max_depth': 8, \n","        'subsample': 0.8,\n","        'objective': 'reg:linear',\n","        'eval_metric': 'mae',\n","        'lambda': 9.0,\n","        'alpha': 0.8,\n","        'colsample_bytree': 0.7,\n","        'silent': 1,\n","        'random_state' : 42\n","    }\n","\n","    \n","\n","## STEP3: train    \n","def _train_predict_amount():\n","    print('\\n\\nSTEP3: training amount...')\n","    \n","    global a_xgb_clf\n","    global a_pred\n","    \n","    ad_train = xgb.DMatrix(train_x, label=train_ya)\n","    ad_valid = xgb.DMatrix(valid_x, label=valid_ya)\n","    \n","    a_evals = [(ad_train, 'train'), (ad_valid, 'valid')]\n","    a_xgb_clf = xgb.train(xgb_params, ad_train, num_boost_round=5000, evals=a_evals, \n","                        early_stopping_rounds=100, verbose_eval=100)\n","    \n","\n","    ad_test = xgb.DMatrix(test_x)\n","    a_pred = a_xgb_clf.predict(ad_test)\n","    \n","def _train_predict_survival():\n","    print('\\n\\nSTEP4: training survival...')\n","    \n","    global s_xgb_clf\n","    global s_pred\n","    \n","    sd_train = xgb.DMatrix(train_x, label=train_ys)\n","    sd_valid = xgb.DMatrix(valid_x, label=valid_ys)\n","    \n","    s_evals = [(sd_train, 'train'), (sd_valid, 'valid')]\n","    s_xgb_clf = xgb.train(xgb_params, sd_train, num_boost_round=5000, evals=s_evals, \n","                        early_stopping_rounds=100, verbose_eval=20)\n","\n","    sd_test = xgb.DMatrix(test_x)\n","    s_pred = s_xgb_clf.predict(sd_test)\n","    \n","\n","## STEP5: generate submission \n","def _generate_submission():\n","    print('\\n\\nSTEP5: generating submission ...')\n","    \n","    test_predict = pd.DataFrame(columns = ['acc_id', 'survival_time','amount_spent'])\n","    \n","    test_predict['acc_id'] = test_acc_id\n","    test_predict['survival_time'] = s_pred\n","    test_predict['amount_spent'] = a_pred\n","    \n","    test_predict['amount_spent'] = [0 if i < 0 else i for i in test_predict['amount_spent']] # 마이너스값 0 처리 \n","    test_predict['survival_time'] = [1 if i < 1 else i for i in test_predict['survival_time']] # 1 미만 값 1 처리\n","    \n","    #testname에 따라 amountspent value 키우기\n","    if testname == 'test1' :\n","        test_predict['amount_spent'] = test_predict['amount_spent']*18 #best : 20\n","    elif testname == 'test2' :\n","        test_predict['amount_spent'] = test_predict['amount_spent']*5\n","    else :\n","        print('testname is wrong!')\n","        \n","    test_predict.to_csv(submission_path + testname + '_predict.csv',index=False)    \n"," "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6GbCU8BXFncQ","colab_type":"text"},"source":["#### ver1.6 - Day 변수 추가 (login_day_max, logout_day)"]},{"cell_type":"code","metadata":{"id":"qe5w8ts5FsT9","colab_type":"code","colab":{}},"source":["\n","# -*- coding: utf-8 -*-\n","\n","\n","from enum import Enum\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","from sklearn import preprocessing\n","import xgboost as xgb\n","from datetime import datetime\n","import gc\n","\n","import warnings\n","def ignore_warn(*args, **kwargs):\n","    pass\n","warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n","\n","\n","## STEP0: do setting\n","class Settings(Enum):\n","    global train_path\n","    global test_path\n","    global submission_path\n","    global testname\n","    global codeversion\n","    \n","    codeversion = 'ver1.6 - Day 변수 추가 (login_day_max, logout_day)'\n","    \n","    path            = 'drive/My Drive/bigcontest2019/'\n","    train_path      = path + 'data/train/'\n","    test_path       = path + 'data/test/'\n","    submission_path = path + 'scripts/model/metrics/nes_inference/'\n","        \n","    def __str__(self):\n","        return self.value\n","        \n","    \n","## STEP1: process data    \n","def fill_NA(df):\n","    print('\\nFilling NA ...')\n","    \n","    na_ratio = ((df.isnull().sum() / len(df)) * 100).sort_values(ascending=False)\n","    print('NA ratio: ')\n","    print(na_ratio) \n","    \n","    for feature in df:\n","        if df[feature].dtype == 'object':\n","            df[feature] = df[feature].fillna(\"None\")\n","        else:\n","            df[feature] = df[feature].fillna(0)\n","    \n","def encode_features(df):\n","    print('\\nEncoding features ...')\n","    \n","    for feature in df:\n","        if df[feature].dtype == 'object':\n","            print('Encoding ', feature)\n","            lbl = preprocessing.LabelEncoder()\n","            lbl.fit(list(df[feature].values))\n","            df[feature] = lbl.transform(list(df[feature].values))\n","    \n","# def display_feature_target(df, feature):\n","#     fig, ax = plt.subplots()\n","#     ax.scatter(x = df[feature], y = df['logerror'])\n","#     plt.ylabel('logerror', fontsize=13)\n","#     plt.xlabel(feature, fontsize=13)\n","#     plt.show()\n","    \n","def _process_data():\n","    \n","    print('CODE VERSION : ', codeversion)\n","    print('\\n\\nSTEP1: processing data ...')\n","    \n","    global train_merge\n","    global test_merge\n","\n","        \n","    # load data\n","    print('\\nLoading data ...')\n","    \n","    train_label = pd.read_csv(train_path + 'train_label.csv')\n","    train_activity = pd.read_csv(train_path + 'train_activity.csv')\n","    train_combat = pd.read_csv(train_path + 'train_combat.csv')\n","    train_payment = pd.read_csv(train_path + 'train_payment.csv')\n","    train_pledge = pd.read_csv(train_path + 'train_pledge.csv')\n","    train_trade = pd.read_csv(train_path + 'train_trade.csv')\n","\n","    test_activity = pd.read_csv(test_path + testname + '_activity.csv')\n","    test_combat = pd.read_csv(test_path + testname + '_combat.csv')\n","    test_payment = pd.read_csv(test_path + testname + '_payment.csv')\n","    test_pledge = pd.read_csv(test_path + testname + '_pledge.csv')\n","    test_trade = pd.read_csv(test_path  + testname + '_trade.csv')\n","\n","    \n","    # merge payment\n","    print('\\nGrouping activity data ...')\n","\n","    train_activity['game_money_change_minus'] = train_activity['game_money_change'] #game_money_change를 마이너스끼리, 플러스끼리 분리\n","    train_activity['game_money_change_minus'] = [0 if i > 0 else i for i in train_activity['game_money_change_minus']]\n","    train_activity['game_money_change'] = [0 if i < 0 else i for i in train_activity['game_money_change']]\n","\n","    test_activity['game_money_change_minus'] = test_activity['game_money_change']\n","    test_activity['game_money_change_minus'] = [0 if i > 0 else i for i in test_activity['game_money_change_minus']]\n","    test_activity['game_money_change'] = [0 if i < 0 else i for i in test_activity['game_money_change']]\n","\n","    activity_agg = {\n","        'day':'nunique', 'char_id':'nunique','server':'nunique', 'playtime':'sum', 'npc_kill':'sum', \n","        'solo_exp':'sum','party_exp':'sum','quest_exp':'sum','rich_monster':'sum', \n","        'death':'sum', 'revive':'sum','exp_recovery':'sum','fishing':'sum','private_shop':'sum',\n","        'game_money_change':'sum', 'game_money_change_minus':'sum','enchant_count':'sum'}\n","    \n","    train_activity_merge = train_activity.groupby('acc_id', as_index = False).agg(activity_agg).reset_index(drop=True)\n","    test_activity_merge = test_activity.groupby('acc_id', as_index = False).agg(activity_agg).reset_index(drop=True)\n","    \n","    acc_id_list = train_activity['acc_id'].unique().tolist()\n","    login_day_max = []\n","    logout_day = []\n","    \n","    print('\\n + making day variables for train activity')\n","\n","    for i in acc_id_list :\n","        dayunique = train_activity[train_activity['acc_id'] == i]['day'].unique().tolist() # i아이디의 접속일 list화\n","        in_list = [0] * 28\n","        in_value = 0\n","        daylogin = 1\n","\n","        out_list = [0] * 28\n","        out_value = 0\n","        day_logout = 0\n","\n","        # login_day_max 변수 생성 (acc_id가 28일 중 최대 연속으로 접속한 일 수. 띄엄띄엄 접속자보다 매일 꾸준히 하는 사람 데이터를 모으고 싶었음)\n","        for j in range(0, len(dayunique)-1) :        \n","            if dayunique[j]+1 == dayunique[j+1] : # 전날 대비 연속 접속한 경우에 연속값 카운트\n","                daylogin += 1 \n","                in_list[in_value] = daylogin  # 연속값 저장\n","            else :\n","                in_value += 1 \n","                daylogin = 1 # 이전 접속일과 다음 접속일이 1이상 차이날 때, 연속값 초기화\n","        login_day_max.append(max(in_list)) # 연속 접속일 리스트 중 최대값 append\n","\n","         #logout-day 변수 생성 (28일 중 후반부에만 로그인 하는 사람들과, 초반 로그인 후 접속하지 않는 유저를 구분하기 위함)\n","        for k in range(0, len(dayunique)-1) :\n","            if dayunique[k]+1 != dayunique[k+1] : #연속 접속하지 않은 경우, 다음 접속일까지 로그아웃한 일자 카운트\n","                day_logout = (dayunique[k+1]-dayunique[k]-1)\n","                out_list[out_value] = day_logout\n","            else :\n","                out_value += 1\n","                day_logout = 0 #접속한 경우 pass 및 연속값 초기화\n","        logout_day.append(max(out_list)) \n","\n","    all_day = pd.DataFrame(columns = ['acc_id', 'login_day_max','logout_day']) # 빈프레임에 day 관련 데이터 2개 추가\n","    all_day['acc_id'] = acc_id_list\n","    all_day['login_day_max'] = login_day_max\n","    all_day['logout_day'] = logout_day\n","\n","    train_activity_merge = train_activity_merge.merge(all_day, on = ['acc_id'], how ='left')\n","    \n","    print('\\n + making day variables for test activity')\n","\n","    acc_id_list = test_activity['acc_id'].unique().tolist()\n","    login_day_max = []\n","    logout_day = []\n","\n","    for i in acc_id_list :\n","        dayunique = test_activity[test_activity['acc_id'] == i]['day'].unique().tolist() # i아이디의 접속일 list화\n","        in_list = [0] * 28\n","        in_value = 0\n","        daylogin = 1\n","\n","        out_list = [0] * 28\n","        out_value = 0\n","        day_logout = 0\n","\n","        # login_day_max 변수 생성 (acc_id가 28일 중 최대 연속으로 접속한 일 수. 띄엄띄엄 접속자보다 매일 꾸준히 하는 사람 데이터를 모으고 싶었음)\n","        for j in range(0, len(dayunique)-1) :        \n","            if dayunique[j]+1 == dayunique[j+1] : # 전날 대비 연속 접속한 경우에 연속값 카운트\n","                daylogin += 1 \n","                in_list[in_value] = daylogin  # 연속값 저장\n","            else :\n","                in_value += 1 \n","                daylogin = 1 # 이전 접속일과 다음 접속일이 1이상 차이날 때, 연속값 초기화\n","        login_day_max.append(max(in_list)) # 연속 접속일 리스트 중 최대값 append\n","\n","         #logout-day 변수 생성 (28일 중 후반부에만 로그인 하는 사람들과, 초반 로그인 후 접속하지 않는 유저를 구분하기 위함)\n","        for k in range(0, len(dayunique)-1) :\n","            if dayunique[k]+1 != dayunique[k+1] : #연속 접속하지 않은 경우, 다음 접속일까지 로그아웃한 일자 카운트\n","                day_logout = (dayunique[k+1]-dayunique[k]-1)\n","                out_list[out_value] = day_logout\n","            else :\n","                out_value += 1\n","                day_logout = 0 #접속한 경우 pass 및 연속값 초기화\n","        logout_day.append(max(out_list)) \n","\n","    all_day = pd.DataFrame(columns = ['acc_id', 'login_day_max','logout_day']) # 빈프레임에 day 관련 데이터 2개 추가\n","    all_day['acc_id'] = acc_id_list\n","    all_day['login_day_max'] = login_day_max\n","    all_day['logout_day'] = logout_day\n","\n","    test_activity_merge = test_activity_merge.merge(all_day, on = ['acc_id'], how ='left')\n","    \n","    \n","    print('activity id unique and shape :')\n","    print(train_activity_merge['acc_id'].nunique(), train_activity_merge.shape)\n","    print(test_activity_merge['acc_id'].nunique(), test_activity_merge.shape)\n","    \n","    \n","    # merge payment\n","    print('\\nGrouping payment data ...')\n","    \n","    train_payment_merge = train_payment.groupby( 'acc_id', as_index = False).agg({'day':'nunique','amount_spent':'max'}).reset_index(drop=True)\n","    test_payment_merge = test_payment.groupby('acc_id', as_index = False).agg({'day':'nunique','amount_spent':'max'}).reset_index(drop=True)\n","\n","    train_payment_merge.rename(columns={'amount_spent':'max_spent'}, inplace=True) #amout_spent를 일일 최고 구매량으로 변경\n","    test_payment_merge.rename(columns={'amount_spent':'max_spent'}, inplace=True)\n","\n","    print('payment id unique and shape :')\n","    print(train_payment_merge['acc_id'].nunique(), train_payment_merge.shape)\n","    print(test_payment_merge['acc_id'].nunique(), test_payment_merge.shape)\n","    \n","    \n","    # merge combat \n","    print('\\nGrouping combat data ...')\n","    class_names = {0:'liege', 1:'knight', 2:'elf', 3:'magician', 4:'dark-elf', 5:'dragon',6:'illusionist',7:'warrior'}\n","    for elem in train_combat['class'].unique():train_combat[class_names[elem]] = train_combat['class'] == elem\n","    for elem in test_combat['class'].unique():test_combat[class_names[elem]] = test_combat['class'] == elem\n","    \n","    combat_agg = {'day':'nunique','char_id' : 'nunique', 'server' : 'nunique', 'class' : 'nunique', 'level' : 'max',\n","                  'pledge_cnt' : 'sum','random_attacker_cnt' : 'sum','random_defender_cnt' : 'sum', 'temp_cnt' : 'sum',\n","                  'same_pledge_cnt' : 'sum','etc_cnt' : 'sum','num_opponent' : 'sum','elf' : 'sum','magician' : 'sum',\n","                  'warrior' : 'sum','dark-elf' : 'sum','knight' : 'sum','dragon' : 'sum','liege' : 'sum','illusionist' : 'sum'}\n","        \n","    combat_rename = {'random_attacker_cnt':'combat_random_attacker_cnt','random_defender_cnt' : 'combat_random_defender_cnt',\n","                     'temp_cnt' : 'combat_temp_cnt','same_pledge_cnt' : 'combat_same_pledge_cnt','etc_cnt' : 'combat_etc_cnt'}\n","\n","    train_combat_merge = train_combat.groupby('acc_id', as_index = False).agg(combat_agg).reset_index(drop=True)\n","    test_combat_merge = test_combat.groupby('acc_id', as_index = False).agg(combat_agg).reset_index(drop=True)\n","  \n","    train_combat_merge.rename(columns=combat_rename, inplace=True)\n","    test_combat_merge.rename(columns=combat_rename, inplace=True)\n","\n","    print('combat id unique and shape :')\n","    print(train_combat_merge['acc_id'].nunique(), train_combat_merge.shape)\n","    print(test_combat_merge['acc_id'].nunique(), test_combat_merge.shape)\n","    \n","    \n","    # merge combat \n","    print('\\nGrouping pledge data ...')\n","    \n","    pledge_agg = {'day':'nunique','char_id':'nunique','server':'nunique','pledge_id':'nunique', 'play_char_cnt' : 'sum',\n","                  'combat_char_cnt' : 'sum','pledge_combat_cnt' : 'sum','random_attacker_cnt' : 'sum','random_defender_cnt' : 'sum',\n","                  'same_pledge_cnt' : 'sum', 'temp_cnt' : 'sum','etc_cnt' : 'sum','combat_play_time' : 'sum','non_combat_play_time' : 'sum'}\n","    \n","    train_pledge_merge = train_pledge.groupby('acc_id', as_index = False).agg(pledge_agg).reset_index(drop=True)\n","    test_pledge_merge = test_pledge.groupby('acc_id', as_index = False).agg(pledge_agg).reset_index(drop=True)\n","    \n","    print('pledge id unique and shape :')\n","    print(train_pledge_merge['acc_id'].nunique(), train_pledge_merge.shape)\n","    print(test_pledge_merge['acc_id'].nunique(), test_pledge_merge.shape)\n","    \n","    \n","    # merge trade \n","    print('\\nGrouping trade data ...')\n","\n","    # item_type dummies             \n","    train_item_type_dummy = pd.get_dummies(train_trade['item_type'])\n","    test_item_type_dummy = pd.get_dummies(test_trade['item_type'])\n","\n","    # trade_type dummies\n","    type_names = {0:'personal_shop', 1:'trade_shop'}\n","    for elem in train_trade['type'].unique():train_trade[type_names[elem]] = train_trade['type'] == elem\n","    for elem in test_trade['type'].unique():test_trade[type_names[elem]] = test_trade['type'] == elem\n","\n","    # merge dummies\n","    \n","    train_trade = train_trade.drop(columns=['type','item_type'])\n","    train_trade = pd.concat([train_trade, train_item_type_dummy], axis = 1)\n","    \n","    test_trade = test_trade.drop(columns=['type','item_type'])\n","    test_trade = pd.concat([test_trade, test_item_type_dummy], axis = 1)\n","\n","    # rename columns\n","    trade_rename = {'day':'trade_day','server' : 'trade_server', 'time' : 'trade_time'}\n","    train_trade.rename(columns=trade_rename, inplace=True)\n","    test_trade.rename(columns=trade_rename, inplace=True)\n","\n","    #divide source and target id\n","    \n","    train_source_trade = train_trade.drop(columns = ['target_acc_id','source_char_id','target_char_id']).rename(columns={'source_acc_id' : 'acc_id'})\n","    train_source_trade['source'] = 1\n","    train_target_trade = train_trade.drop(columns = ['source_acc_id','source_char_id','target_char_id']).rename(columns={'target_acc_id' : 'acc_id'})\n","    train_target_trade['target'] = 1\n","    \n","    test_source_trade = test_trade.drop(columns = ['target_acc_id','source_char_id','target_char_id']).rename(columns={'source_acc_id' : 'acc_id'})\n","    test_source_trade['source'] = 1\n","    test_target_trade = test_trade.drop(columns = ['source_acc_id','source_char_id','target_char_id']).rename(columns={'target_acc_id' : 'acc_id'})\n","    test_target_trade['target'] = 1\n","\n","\n","    #aggregate\n","    trade_source_agg = {'trade_day':'nunique', 'trade_time' : 'nunique', 'trade_server' : 'nunique','item_amount' : 'sum',\n","                        'item_price': 'sum', 'trade_shop' : 'sum', 'personal_shop' : 'sum', 'accessory' : 'sum', 'adena' : 'sum',\n","                        'armor' : 'sum', 'enchant_scroll' : 'sum', 'etc' : 'sum', 'spell' : 'sum', 'weapon' : 'sum', 'source' : 'sum'}\n","\n","    trade_target_agg = {'trade_day':'nunique', 'trade_time' : 'nunique', 'trade_server' : 'nunique','item_amount' : 'sum',\n","                        'item_price': 'sum', 'trade_shop' : 'sum', 'personal_shop' : 'sum', 'accessory' : 'sum', 'adena' : 'sum',\n","                        'armor' : 'sum', 'enchant_scroll' : 'sum', 'etc' : 'sum', 'spell' : 'sum', 'weapon' : 'sum', 'target' : 'sum'}\n","\n","    train_source_trade = train_source_trade.groupby('acc_id', as_index = False).agg(trade_source_agg).reset_index(drop=True)\n","    train_target_trade = train_target_trade.groupby('acc_id', as_index = False).agg(trade_target_agg).reset_index(drop=True)\n","    \n","    test_source_trade = test_source_trade.groupby('acc_id', as_index = False).agg(trade_source_agg).reset_index(drop=True)\n","    test_target_trade = test_target_trade.groupby('acc_id', as_index = False).agg(trade_target_agg).reset_index(drop=True)\n","\n","    print('trade id unique and shape :')\n","    print(train_source_trade['acc_id'].nunique(), train_source_trade.shape, train_target_trade['acc_id'].nunique(), train_target_trade.shape)\n","    print(test_source_trade['acc_id'].nunique(), test_source_trade.shape, test_target_trade['acc_id'].nunique(), test_target_trade.shape)\n","    \n","    \n","    \n","    #merge all data\n","    print('\\nMerging all data ...')\n","    train_merge = train_label.merge(train_activity_merge, on = ['acc_id'], how ='left').merge(\n","    train_combat_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    train_pledge_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    train_payment_merge.drop(['day'], axis=1), on = ['acc_id'], how ='left').merge(\n","    train_source_trade, on = ['acc_id'], how = 'left').merge(\n","    train_target_trade, on = ['acc_id'], how = 'left')\n","        \n","    test_merge = test_activity_merge.merge(\n","    test_combat_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    test_pledge_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    test_payment_merge.drop(['day'], axis=1), on = ['acc_id'], how ='left').merge(\n","    test_source_trade, on = ['acc_id'], how = 'left').merge(\n","    test_target_trade, on = ['acc_id'], how = 'left')\n","    \n","    print('\\nmerge id unique and shape :')\n","    print(train_merge['acc_id'].nunique(), train_merge.shape)\n","    print(test_merge['acc_id'].nunique(), test_merge.shape)\n","    \n","    \n","    train_merge.to_csv(submission_path + '/merge/' + 'train_merge.csv',index=False)  \n","    test_merge.to_csv(submission_path + '/merge/' + testname + '_merge.csv',index=False)  \n","    print('test and train merge data are saved on ' + submission_path + '/merge/' )\n","    \n","    \n","def _load_merge_data(): \n","\n","    train_merge = pd.read_csv(submission_path + 'merge/' + 'train_merge.csv')\n","    test_merge = pd.read_csv(submission_path + 'merge/' + testname + '_merge.csv')\n","    \n","    train_merge['cnt'] = train_merge['random_attacker_cnt']+train_merge['random_defender_cnt']+train_merge['temp_cnt']+train_merge['same_pledge_cnt']+train_merge['etc_cnt']\n","    test_merge['cnt'] = test_merge['random_attacker_cnt']+test_merge['random_defender_cnt']+test_merge['temp_cnt']+test_merge['same_pledge_cnt']+test_merge['etc_cnt']\n","    \n","    train_merge = train_merge.drop(['day','fishing','revive','elf','magician','warrior','dark-elf','knight','dragon','illusionist','class',\n","                     'random_attacker_cnt','random_defender_cnt','temp_cnt','same_pledge_cnt','etc_cnt'], axis = 1)\n","    test_merge = test_merge.drop(['day','fishing','revive','elf','magician','warrior','dark-elf','knight','dragon','illusionist','class',\n","                     'random_attacker_cnt','random_defender_cnt','temp_cnt','same_pledge_cnt','etc_cnt'], axis = 1)\n","       \n","    print('test and train merge data are loaded on ' + submission_path + 'merge/' )\n","    \n","    # fill NA\n","#     fill_NA(train_merge)\n","    \n","    # encode features\n","#     encode_features(train_merge)\n","    \n","### STEP1.1 : prepare train and vaild data\n","\n","def _prepare_data():\n","    # prepare train and valid data\n","    print('\\nPreparing train and valid data ...')\n","\n","    global train_x\n","    global train_ya\n","    global train_ys\n","    \n","    global valid_x\n","    global valid_ya\n","    global valid_ys\n","    \n","    global test_x\n","    global test_acc_id\n","    \n","    \n","    drop_vars = ['acc_id','amount_spent','survival_time']\n","    \n","    train_ya = train_merge['amount_spent']\n","    train_ys = train_merge['survival_time']\n","    \n","    train_x = train_merge\n","    train_x.drop(columns=drop_vars, inplace=True)\n","    \n","    valid_x = train_x[30000:]\n","    valid_ya = train_ya[30000:]\n","    \n","    train_x = train_x[:30000]\n","    train_ya = train_ya[:30000]\n","    \n","    valid_ys = train_ys[30000:]\n","    train_ys = train_ys[:30000]\n","    \n","    print('train x shape: ', train_x.shape)\n","    print('train ya shape: ', train_ya.shape)\n","    print('train ys shape: ', train_ys.shape)\n","    \n","    print('valid x shape: ', valid_x.shape)\n","    print('valid ya shape: ', valid_ya.shape)\n","    print('valid ys shape: ', valid_ys.shape)\n","    \n","    \n","    # prepare test data\n","    print('\\nPreparing test data ...')\n","    \n","    test_x = test_merge\n","    test_acc_id = test_merge['acc_id']\n","    test_x.drop(columns=['acc_id'], inplace=True)\n","    print('test x shape: ', test_x.shape)\n","\n","    \n","## STEP2: build model\n","def _build_model():\n","    print('\\n\\nSTEP2: building model ...')\n","    \n","    global xgb_params\n","\n","    xgb_params = {\n","        'eta': 0.007,\n","        'max_depth': 8, \n","        'subsample': 0.8,\n","        'objective': 'reg:linear',\n","        'eval_metric': 'mae',\n","        'lambda': 9.0,\n","        'alpha': 0.8,\n","        'colsample_bytree': 0.7,\n","        'silent': 1,\n","        'random_state' : 42\n","    }\n","\n","    \n","\n","## STEP3: train    \n","def _train_predict_amount():\n","    print('\\n\\nSTEP3: training amount...')\n","    \n","    global a_xgb_clf\n","    global a_pred\n","    \n","    ad_train = xgb.DMatrix(train_x, label=train_ya)\n","    ad_valid = xgb.DMatrix(valid_x, label=valid_ya)\n","    \n","    a_evals = [(ad_train, 'train'), (ad_valid, 'valid')]\n","    a_xgb_clf = xgb.train(xgb_params, ad_train, num_boost_round=10000, evals=a_evals, \n","                        early_stopping_rounds=100, verbose_eval=20)\n","    \n","\n","    ad_test = xgb.DMatrix(test_x)\n","    a_pred = a_xgb_clf.predict(ad_test)\n","    \n","def _train_predict_survival():\n","    print('\\n\\nSTEP4: training survival...')\n","    \n","    global s_xgb_clf\n","    global s_pred\n","    \n","    sd_train = xgb.DMatrix(train_x, label=train_ys)\n","    sd_valid = xgb.DMatrix(valid_x, label=valid_ys)\n","    \n","    s_evals = [(sd_train, 'train'), (sd_valid, 'valid')]\n","    s_xgb_clf = xgb.train(xgb_params, sd_train, num_boost_round=10000, evals=s_evals, \n","                        early_stopping_rounds=100, verbose_eval=20)\n","\n","    sd_test = xgb.DMatrix(test_x)\n","    s_pred = s_xgb_clf.predict(sd_test)\n","    \n","\n","## STEP5: generate submission \n","def _generate_submission():\n","    print('\\n\\nSTEP5: generating submission ...')\n","    \n","    test_predict = pd.DataFrame(columns = ['acc_id', 'survival_time','amount_spent'])\n","    \n","    test_predict['acc_id'] = test_acc_id\n","    test_predict['survival_time'] = s_pred\n","    test_predict['amount_spent'] = a_pred\n","    \n","    test_predict['amount_spent'] = [0 if i < 0 else i for i in test_predict['amount_spent']] # 마이너스값 0 처리 \n","    test_predict['survival_time'] = [1 if i < 1 else i for i in test_predict['survival_time']] # 1 미만 값 1 처리\n","    \n","    #testname에 따라 amountspent value 키우기\n","    if testname == 'test1' :\n","        test_predict['amount_spent'] = test_predict['amount_spent']*18 #best : 20\n","    elif testname == 'test2' :\n","        test_predict['amount_spent'] = test_predict['amount_spent']*5\n","    else :\n","        print('testname is wrong!')\n","        \n","    test_predict.to_csv(submission_path + testname + '_predict.csv',index=False)    \n"," "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XGVqAt6_ezUO","colab_type":"text"},"source":["### history code - DO NOT MODIFY"]},{"cell_type":"markdown","metadata":{"id":"9ds9iDQIGGOx","colab_type":"text"},"source":["#### ver1.5 CODE - xgb with gridsearchCV (-50000점^^;)"]},{"cell_type":"code","metadata":{"id":"pSeyB5HYGM91","colab_type":"code","colab":{}},"source":["#ref : https://www.kaggle.com/cuongvn08/xgboost-cross-validation-clean-coding-lb-0-64x\n","\n","# -*- coding: utf-8 -*-\n","\n","from enum import Enum\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.metrics import mean_squared_error\n","from sklearn import preprocessing\n","\n","import xgboost as xgb\n","from xgboost import XGBRegressor\n","import lightgbm as lgb\n","\n","from datetime import datetime\n","import gc\n","\n","\n","from math import sqrt\n","\n","\n","import warnings\n","def ignore_warn(*args, **kwargs):\n","    pass\n","warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n","\n","\n","## STEP0: do setting\n","class Settings(Enum):\n","    global train_path\n","    global test_path\n","    global submission_path\n","    global testname\n","    \n","    testname = 'test1'\n","    \n","    path            = 'drive/My Drive/bigcontest2019/'\n","    train_path      = path + 'data/train/'\n","    test_path       = path + 'data/test/'\n","    submission_path = path + 'scripts/model/metrics/nes_inference/'\n","        \n","    def __str__(self):\n","        return self.value\n","        \n","    \n","# def display_feature_target(df, feature):\n","#     fig, ax = plt.subplots()\n","#     ax.scatter(x = df[feature], y = df['logerror'])\n","#     plt.ylabel('logerror', fontsize=13)\n","#     plt.xlabel(feature, fontsize=13)\n","#     plt.show()\n","    \n","def _process_data():\n","    print('\\n\\nSTEP1: processing data ...')\n","    \n","    global train_merge\n","    global test_merge\n","\n","        \n","    # load data\n","    print('\\nLoading data ...')\n","    \n","    train_label = pd.read_csv(train_path + 'train_label.csv')\n","    train_activity = pd.read_csv(train_path + 'train_activity.csv')\n","    train_combat = pd.read_csv(train_path + 'train_combat.csv')\n","    train_payment = pd.read_csv(train_path + 'train_payment.csv')\n","    train_pledge = pd.read_csv(train_path + 'train_pledge.csv')\n","    train_trade = pd.read_csv(train_path + 'train_trade.csv')\n","\n","    test_activity = pd.read_csv(test_path + testname + '_activity.csv')\n","    test_combat = pd.read_csv(test_path + testname + '_combat.csv')\n","    test_payment = pd.read_csv(test_path + testname + '_payment.csv')\n","    test_pledge = pd.read_csv(test_path + testname + '_pledge.csv')\n","    test_trade = pd.read_csv(test_path  + testname + '_trade.csv')\n","\n","    \n","    # merge payment\n","    print('\\nGrouping activity data ...')\n","\n","    train_activity['game_money_change_minus'] = train_activity['game_money_change'] #game_money_change를 마이너스끼리, 플러스끼리 분리\n","    train_activity['game_money_change_minus'] = [0 if i > 0 else i for i in train_activity['game_money_change_minus']]\n","    train_activity['game_money_change'] = [0 if i < 0 else i for i in train_activity['game_money_change']]\n","\n","    test_activity['game_money_change_minus'] = test_activity['game_money_change']\n","    test_activity['game_money_change_minus'] = [0 if i > 0 else i for i in test_activity['game_money_change_minus']]\n","    test_activity['game_money_change'] = [0 if i < 0 else i for i in test_activity['game_money_change']]\n","\n","    activity_agg = {\n","        'day':'nunique', 'char_id':'nunique','server':'nunique', 'playtime':'sum', 'npc_kill':'sum', \n","        'solo_exp':'sum','party_exp':'sum','quest_exp':'sum','rich_monster':'sum', \n","        'death':'sum', 'revive':'sum','exp_recovery':'sum','fishing':'sum','private_shop':'sum',\n","        'game_money_change':'sum', 'game_money_change_minus':'sum','enchant_count':'sum'}\n","    \n","    train_activity_merge = train_activity.groupby('acc_id', as_index = False).agg(activity_agg).reset_index(drop=True)\n","    test_activity_merge = test_activity.groupby('acc_id', as_index = False).agg(activity_agg).reset_index(drop=True)\n","    \n","    print('activity id unique and shape :')\n","    print(train_activity_merge['acc_id'].nunique(), train_activity_merge.shape)\n","    print(test_activity_merge['acc_id'].nunique(), test_activity_merge.shape)\n","    \n","    \n","    # merge payment\n","    print('\\nGrouping payment data ...')\n","    \n","    train_payment_merge = train_payment.groupby( 'acc_id', as_index = False).agg({'day':'nunique','amount_spent':'max'}).reset_index(drop=True)\n","    test_payment_merge = test_payment.groupby('acc_id', as_index = False).agg({'day':'nunique','amount_spent':'max'}).reset_index(drop=True)\n","\n","    train_payment_merge.rename(columns={'amount_spent':'max_spent'}, inplace=True) #amout_spent를 일일 최고 구매량인 max로 변경\n","    test_payment_merge.rename(columns={'amount_spent':'max_spent'}, inplace=True)\n","\n","    print('payment id unique and shape :')\n","    print(train_payment_merge['acc_id'].nunique(), train_payment_merge.shape)\n","    print(test_payment_merge['acc_id'].nunique(), test_payment_merge.shape)\n","    \n","    \n","    # merge combat \n","    print('\\nGrouping combat data ...')\n","    class_names = {0:'liege', 1:'knight', 2:'elf', 3:'magician', 4:'dark-elf', 5:'dragon',6:'illusionist',7:'warrior'}\n","    for elem in train_combat['class'].unique():train_combat[class_names[elem]] = train_combat['class'] == elem\n","    for elem in test_combat['class'].unique():test_combat[class_names[elem]] = test_combat['class'] == elem\n","    \n","    combat_agg = {'day':'nunique','char_id' : 'nunique', 'server' : 'nunique', 'class' : 'nunique', 'level' : 'max',\n","                  'pledge_cnt' : 'sum','random_attacker_cnt' : 'sum','random_defender_cnt' : 'sum', 'temp_cnt' : 'sum',\n","                  'same_pledge_cnt' : 'sum','etc_cnt' : 'sum','num_opponent' : 'sum','elf' : 'sum','magician' : 'sum',\n","                  'warrior' : 'sum','dark-elf' : 'sum','knight' : 'sum','dragon' : 'sum','liege' : 'sum','illusionist' : 'sum'}\n","        \n","    combat_rename = {'random_attacker_cnt':'combat_random_attacker_cnt','random_defender_cnt' : 'combat_random_defender_cnt',\n","                     'temp_cnt' : 'combat_temp_cnt','same_pledge_cnt' : 'combat_same_pledge_cnt','etc_cnt' : 'combat_etc_cnt'}\n","\n","    train_combat_merge = train_combat.groupby('acc_id', as_index = False).agg(combat_agg).reset_index(drop=True)\n","    test_combat_merge = test_combat.groupby('acc_id', as_index = False).agg(combat_agg).reset_index(drop=True)\n","  \n","    train_combat_merge.rename(columns=combat_rename, inplace=True) #combat와 pledge value구분\n","    test_combat_merge.rename(columns=combat_rename, inplace=True)\n","\n","    print('combat id unique and shape :')\n","    print(train_combat_merge['acc_id'].nunique(), train_combat_merge.shape)\n","    print(test_combat_merge['acc_id'].nunique(), test_combat_merge.shape)\n","    \n","    \n","    # merge combat \n","    print('\\nGrouping pledge data ...')\n","    \n","    pledge_agg = {'day':'nunique','char_id':'nunique','server':'nunique','pledge_id':'nunique', 'play_char_cnt' : 'sum',\n","                  'combat_char_cnt' : 'sum','pledge_combat_cnt' : 'sum','random_attacker_cnt' : 'sum','random_defender_cnt' : 'sum',\n","                  'same_pledge_cnt' : 'sum', 'temp_cnt' : 'sum','etc_cnt' : 'sum','combat_play_time' : 'sum','non_combat_play_time' : 'sum'}\n","    \n","    train_pledge_merge = train_pledge.groupby('acc_id', as_index = False).agg(pledge_agg).reset_index(drop=True) \n","    test_pledge_merge = test_pledge.groupby('acc_id', as_index = False).agg(pledge_agg).reset_index(drop=True)\n","    \n","    print('pledge id unique and shape :')\n","    print(train_pledge_merge['acc_id'].nunique(), train_pledge_merge.shape)\n","    print(test_pledge_merge['acc_id'].nunique(), test_pledge_merge.shape)\n","    \n","    \n","    # merge trade \n","    print('\\nGrouping trade data ...')\n","\n","    # item_type dummies             \n","    train_item_type_dummy = pd.get_dummies(train_trade['item_type']) #item 더미변수 생성\n","    test_item_type_dummy = pd.get_dummies(test_trade['item_type'])\n","\n","    # trade_type dummies\n","    type_names = {0:'personal_shop', 1:'trade_shop'}  #personal_shop, trade_shop구분하는 더미변수 생성\n","    for elem in train_trade['type'].unique():train_trade[type_names[elem]] = train_trade['type'] == elem\n","    for elem in test_trade['type'].unique():test_trade[type_names[elem]] = test_trade['type'] == elem\n","\n","    # merge dummies\n","    \n","    train_trade = train_trade.drop(columns=['type','item_type'])\n","    train_trade = pd.concat([train_trade, train_item_type_dummy], axis = 1)\n","    \n","    test_trade = test_trade.drop(columns=['type','item_type'])\n","    test_trade = pd.concat([test_trade, test_item_type_dummy], axis = 1)\n","\n","    # rename columns\n","    trade_rename = {'day':'trade_day','server' : 'trade_server', 'time' : 'trade_time'} #day, server, time 구분\n","    train_trade.rename(columns=trade_rename, inplace=True)\n","    test_trade.rename(columns=trade_rename, inplace=True)\n","\n","    #divide source and target id, source 및 target 횟수 카운트 할 수 있도록 변수 생성 후 merge\n","    \n","    train_source_trade = train_trade.drop(columns = ['target_acc_id','source_char_id','target_char_id']).rename(columns={'source_acc_id' : 'acc_id'})\n","    train_source_trade['source'] = 1\n","    train_target_trade = train_trade.drop(columns = ['source_acc_id','source_char_id','target_char_id']).rename(columns={'target_acc_id' : 'acc_id'})\n","    train_target_trade['target'] = 1\n","    \n","    test_source_trade = test_trade.drop(columns = ['target_acc_id','source_char_id','target_char_id']).rename(columns={'source_acc_id' : 'acc_id'})\n","    test_source_trade['source'] = 1\n","    test_target_trade = test_trade.drop(columns = ['source_acc_id','source_char_id','target_char_id']).rename(columns={'target_acc_id' : 'acc_id'})\n","    test_target_trade['target'] = 1\n","\n","\n","    #aggregate\n","    trade_source_agg = {'trade_day':'nunique', 'trade_time' : 'nunique', 'trade_server' : 'nunique','item_amount' : 'sum',\n","                        'item_price': 'sum', 'trade_shop' : 'sum', 'personal_shop' : 'sum', 'accessory' : 'sum', 'adena' : 'sum',\n","                        'armor' : 'sum', 'enchant_scroll' : 'sum', 'etc' : 'sum', 'spell' : 'sum', 'weapon' : 'sum', 'source' : 'sum'}\n","\n","    trade_target_agg = {'trade_day':'nunique', 'trade_time' : 'nunique', 'trade_server' : 'nunique','item_amount' : 'sum',\n","                        'item_price': 'sum', 'trade_shop' : 'sum', 'personal_shop' : 'sum', 'accessory' : 'sum', 'adena' : 'sum',\n","                        'armor' : 'sum', 'enchant_scroll' : 'sum', 'etc' : 'sum', 'spell' : 'sum', 'weapon' : 'sum', 'target' : 'sum'}\n","\n","    train_source_trade = train_source_trade.groupby('acc_id', as_index = False).agg(trade_source_agg).reset_index(drop=True)\n","    train_target_trade = train_target_trade.groupby('acc_id', as_index = False).agg(trade_target_agg).reset_index(drop=True)\n","    \n","    test_source_trade = test_source_trade.groupby('acc_id', as_index = False).agg(trade_source_agg).reset_index(drop=True)\n","    test_target_trade = test_target_trade.groupby('acc_id', as_index = False).agg(trade_target_agg).reset_index(drop=True)\n","\n","    print('trade id unique and shape :')\n","    print(train_source_trade['acc_id'].nunique(), train_source_trade.shape, train_target_trade['acc_id'].nunique(), train_target_trade.shape)\n","    print(test_source_trade['acc_id'].nunique(), test_source_trade.shape, test_target_trade['acc_id'].nunique(), test_target_trade.shape)\n","    \n","    \n","    \n","    #merge all data\n","    print('\\nMerging all data ...')\n","    train_merge = train_label.merge(train_activity_merge, on = ['acc_id'], how ='left').merge(\n","    train_combat_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    train_pledge_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    train_payment_merge.drop(['day'], axis=1), on = ['acc_id'], how ='left').merge(\n","    train_source_trade, on = ['acc_id'], how = 'left').merge(\n","    train_target_trade, on = ['acc_id'], how = 'left')\n","        \n","    test_merge = test_activity_merge.merge(\n","    test_combat_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    test_pledge_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    test_payment_merge.drop(['day'], axis=1), on = ['acc_id'], how ='left').merge(\n","    test_source_trade, on = ['acc_id'], how = 'left').merge(\n","    test_target_trade, on = ['acc_id'], how = 'left')\n","    \n","    print('\\nmerge id unique and shape :')\n","    print(train_merge['acc_id'].nunique(), train_merge.shape)\n","    print(test_merge['acc_id'].nunique(), test_merge.shape)\n","    \n","            \n","    def fill_NA(df):\n","        print('\\nFilling NA ...')\n","\n","        na_ratio = ((df.isnull().sum() / len(df)) * 100).sort_values(ascending=False)\n","#         print('NA ratio: ')\n","#         print(na_ratio) \n","\n","        for feature in df:\n","            if df[feature].dtype == 'object':\n","                df[feature] = df[feature].fillna(\"None\") #fill.na option. object type인 경우 None으로, 그 외는 0으로 설정\n","            else:\n","                df[feature] = df[feature].fillna(0)\n","\n","    def encode_features(df):\n","        print('\\nEncoding features ...')\n","\n","        for feature in df:\n","            if df[feature].dtype == 'object':\n","                print('Encoding ', feature)\n","                lbl = preprocessing.LabelEncoder()\n","                lbl.fit(list(df[feature].values))\n","                df[feature] = lbl.transform(list(df[feature].values))\n","    #fill NA\n","    fill_NA(train_merge)\n","    fill_NA(test_merge)\n","    \n","    #encode features\n","    encode_features(train_merge)\n","    encode_features(test_merge)\n","    \n","    \n","    \n","    \n","    \n","    \n","### STEP1.1 : prepare train and vaild data\n","\n","def _prepare_data():\n","    # prepare train and valid data\n","    print('\\nPreparing train and valid data ...')\n","\n","    global X\n","\n","    global X_train\n","    global X_test\n","    \n","    global ya_train\n","    global ya_test\n","    \n","    global ys_train\n","    global ys_test\n","    \n","    global X_test_dataset\n","    global test_acc_id\n","    \n","    \n","    drop_vars = ['acc_id','amount_spent','survival_time']\n","    \n","    \n","    ya = train_merge['amount_spent'].values\n","    ys = train_merge['survival_time'].values\n","    \n","    train_merge.drop(columns=drop_vars, inplace=True)\n","    X = train_merge.values\n","    \n","    test_acc_id = test_merge['acc_id']\n","    test_merge.drop(columns=['acc_id'], inplace=True)\n","    X_test_dataset = test_merge.values\n","\n","    X_train, X_test, ya_train, ya_test = train_test_split(X, ya, test_size=1/5, random_state = SEED) #test size = 1/10\n","    X_train, X_test, ys_train, ys_test = train_test_split(X, ys, test_size=1/5, random_state = SEED) #test size = 1/10\n","\n","    \n","    print('train x shape: ', X.shape)\n","    print('train ya shape: ', ya_train.shape)\n","    print('train ys shape: ', ys_train.shape)\n","    \n","    print('valid x shape: ', X_test.shape)\n","    print('valid ya shape: ', ya_test.shape)\n","    print('valid ys shape: ', ys_test.shape)\n","    \n","    print('test x shape: ', X_test_dataset.shape)   \n","    \n","   \n","    \n","    \n","    \n","    \n","## STEP2: build model\n","def _build_model():\n","    print('\\n\\nSTEP2: building model ...')\n","    \n","    XGB_regressor = xgb.XGBRegressor()\n","    \n","    global XGB_hyper_params\n","    \n","    parameters = {'learning_rate': [0.07, 0.1, 0.3],\n","                  'max_depth': [3, 5, 7],\n","                  'n_estimators': [200, 400, 500]}\n","\n","    XGB_hyper_params = GridSearchCV(estimator=XGB_regressor, param_grid=parameters, n_jobs=-1, cv=5)\n","    \n","   \n","    \n","## STEP3: train    \n","def _train_predict_amount():\n","    print('\\n\\nSTEP3: training amount...')\n","    \n","    global XGB_model_a\n","    global y_XGB_predict_a\n","    \n","    if testname == 'test2' : \n","        XGB_model_a = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n","             colsample_bynode=1, colsample_bytree=1, gamma=0,\n","             importance_type='gain', learning_rate=0.07, max_delta_step=0,\n","             max_depth=3, min_child_weight=1, missing=None, n_estimators=200,\n","             n_jobs=1, nthread=None, objective='reg:linear', random_state=SEED,\n","             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n","             silent=None, subsample=1, verbosity=1)\n","                \n","    elif testname == 'test1' :\n","        XGB_hyper_params.fit(X_train, ya_train)\n","        print('\\nXGB_hyper_params.best_params_ of amount :')\n","        print(XGB_hyper_params.best_estimator_) # find out the best hyper parameters\n","    \n","        XGB_model_a = XGB_hyper_params.best_estimator_\n","    else :\n","        print('testname is wrong!')\n","\n","    XGB_model_a.fit(X_train, ya_train)\n","    \n","    %%time\n","    y_XGB_predict_a = XGB_model_a.predict(X_test)\n","   \n","    # let's plot feature_importance \n","    print('\\nDraw feature importance of amount_spent...')\n","    sns.set_style('ticks')\n","    fig, ax = plt.subplots()\n","    sns.barplot(y=list(train_merge.columns), x=list(XGB_model_a.feature_importances_)).set_title('feature_importances_ of amount_spent')\n","    fig.set_size_inches(20, 20)\n","    sns.despine()\n","    plt.show()\n","    \n","    \n","    \n","    \n","def _train_predict_survival():\n","    print('\\n\\nSTEP4: training survival...')\n","    \n","    global XGB_model_s\n","    global y_XGB_predict_s\n","    \n","    if testname == 'test2' :\n","        XGB_model_s = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n","             colsample_bynode=1, colsample_bytree=1, gamma=0,\n","             importance_type='gain', learning_rate=0.07, max_delta_step=0,\n","             max_depth=7, min_child_weight=1, missing=None, n_estimators=200,\n","             n_jobs=1, nthread=None, objective='reg:linear', random_state=SEED,\n","             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n","             silent=None, subsample=1, verbosity=1)\n","        \n","    elif testname == 'test1' :\n","        XGB_hyper_params.fit(X_train, ys_train)\n","        print('\\nXGB_hyper_params.best_params_ of amount :')\n","        print(XGB_hyper_params.best_estimator_) # find out the best hyper parameters\n","        \n","        XGB_model_a = XGB_hyper_params.best_estimator_\n","    else :\n","        print('testname is wrong!')    \n","    \n","    XGB_model_s.fit(X_train, ys_train)\n","    %%time\n","    y_XGB_predict_s = XGB_model_s.predict(X_test)\n","\n","    # let's plot feature_importance \n","    print('\\nDraw feature importance of survival_time...')\n","    sns.set_style('ticks')\n","    fig, ax = plt.subplots()\n","    sns.barplot(y=list(train_merge.columns), x=list(XGB_model_s.feature_importances_)).set_title('feature_importances_ of survival_time')\n","    fig.set_size_inches(20, 20)\n","    sns.despine()\n","    plt.show()\n","\n","\n","## STEP5: generate submission \n","def _generate_submission():\n","    print('\\n\\nSTEP5: generating submission ...')\n","    \n","    test_predict = pd.DataFrame(columns = ['acc_id', 'survival_time','amount_spent'])\n","    \n","    ya_XGB_amount = np.expm1(XGB_model_a.predict(X_test_dataset))\n","    ys_XGB_amount = np.expm1(XGB_model_s.predict(X_test_dataset))\n","\n","    # submitting our predictions\n","    test_predict['acc_id'] = test_acc_id\n","    test_predict['survival_time'] = ys_XGB_amount\n","    test_predict['amount_spent'] = ya_XGB_amount\n","    \n","    test_predict['amount_spent'] = [0 if i < 0 else i for i in test_predict['amount_spent']]\n","    test_predict['survival_time'] = [1 if i < 1 else i for i in test_predict['survival_time']]\n","\n","    \n","    # testname에 따라 amountspent value 키우기\n","    if testname == 'test1' :\n","        test_predict['amount_spent'] = test_predict['amount_spent']*20\n","        print('amount * 20')\n","#     elif testname == 'test2' :\n","#         test_predict['amount_spent'] = test_predict['amount_spent']*5\n","#         print('amount * 5')\n","    else :\n","        print('eveything is fine!')\n","    \n","    test_predict.to_csv(submission_path + testname + '_predict.csv',index=False) \n","    print('\\nsave completed.\\n')\n","    print(testname + '_predict id unique and shape :')\n","    print(test_predict['acc_id'].nunique(), test_predict.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ThhDBj1MmEpV","colab_type":"text"},"source":["#### ver1.4 CODE - merge trade (4102.32\t2496.54\t6598.863)"]},{"cell_type":"code","metadata":{"id":"ku1AgsitmJPg","colab_type":"code","colab":{}},"source":["#ref : https://www.kaggle.com/cuongvn08/xgboost-cross-validation-clean-coding-lb-0-64x\n","\n","# -*- coding: utf-8 -*-\n","\n","\n","from enum import Enum\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","from sklearn import preprocessing\n","import xgboost as xgb\n","from datetime import datetime\n","import gc\n","\n","import warnings\n","def ignore_warn(*args, **kwargs):\n","    pass\n","warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n","\n","\n","## STEP0: do setting\n","class Settings(Enum):\n","    global train_path\n","    global test_path\n","    global submission_path\n","    global testname\n","    global codeversion\n","    \n","    codeversion = 'ver1.4 - simple merge trade'\n","    \n","#     testname = 'test1'\n","    \n","    path            = 'drive/My Drive/bigcontest2019/'\n","    train_path      = path + 'data/train/'\n","    test_path       = path + 'data/test/'\n","    submission_path = path + 'scripts/model/metrics/nes_inference/'\n","        \n","    def __str__(self):\n","        return self.value\n","        \n","    \n","## STEP1: process data    \n","def fill_NA(df):\n","    print('\\nFilling NA ...')\n","    \n","    na_ratio = ((df.isnull().sum() / len(df)) * 100).sort_values(ascending=False)\n","    print('NA ratio: ')\n","    print(na_ratio) \n","    \n","    for feature in df:\n","        if df[feature].dtype == 'object':\n","            df[feature] = df[feature].fillna(\"None\")\n","        else:\n","            df[feature] = df[feature].fillna(0)\n","    \n","def encode_features(df):\n","    print('\\nEncoding features ...')\n","    \n","    for feature in df:\n","        if df[feature].dtype == 'object':\n","            print('Encoding ', feature)\n","            lbl = preprocessing.LabelEncoder()\n","            lbl.fit(list(df[feature].values))\n","            df[feature] = lbl.transform(list(df[feature].values))\n","    \n","# def display_feature_target(df, feature):\n","#     fig, ax = plt.subplots()\n","#     ax.scatter(x = df[feature], y = df['logerror'])\n","#     plt.ylabel('logerror', fontsize=13)\n","#     plt.xlabel(feature, fontsize=13)\n","#     plt.show()\n","    \n","def _process_data():\n","    \n","    print('CODE VERSION : ', codeversion)\n","    print('\\n\\nSTEP1: processing data ...')\n","    \n","    global train_merge\n","    global test_merge\n","\n","        \n","    # load data\n","    print('\\nLoading data ...')\n","    \n","    train_label = pd.read_csv(train_path + 'train_label.csv')\n","    train_activity = pd.read_csv(train_path + 'train_activity.csv')\n","    train_combat = pd.read_csv(train_path + 'train_combat.csv')\n","    train_payment = pd.read_csv(train_path + 'train_payment.csv')\n","    train_pledge = pd.read_csv(train_path + 'train_pledge.csv')\n","    train_trade = pd.read_csv(train_path + 'train_trade.csv')\n","\n","    test_activity = pd.read_csv(test_path + testname + '_activity.csv')\n","    test_combat = pd.read_csv(test_path + testname + '_combat.csv')\n","    test_payment = pd.read_csv(test_path + testname + '_payment.csv')\n","    test_pledge = pd.read_csv(test_path + testname + '_pledge.csv')\n","    test_trade = pd.read_csv(test_path  + testname + '_trade.csv')\n","\n","    \n","    # merge payment\n","    print('\\nGrouping activity data ...')\n","\n","    train_activity['game_money_change_minus'] = train_activity['game_money_change'] #game_money_change를 마이너스끼리, 플러스끼리 분리\n","    train_activity['game_money_change_minus'] = [0 if i > 0 else i for i in train_activity['game_money_change_minus']]\n","    train_activity['game_money_change'] = [0 if i < 0 else i for i in train_activity['game_money_change']]\n","\n","    test_activity['game_money_change_minus'] = test_activity['game_money_change']\n","    test_activity['game_money_change_minus'] = [0 if i > 0 else i for i in test_activity['game_money_change_minus']]\n","    test_activity['game_money_change'] = [0 if i < 0 else i for i in test_activity['game_money_change']]\n","\n","    activity_agg = {\n","        'day':'nunique', 'char_id':'nunique','server':'nunique', 'playtime':'sum', 'npc_kill':'sum', \n","        'solo_exp':'sum','party_exp':'sum','quest_exp':'sum','rich_monster':'sum', \n","        'death':'sum', 'revive':'sum','exp_recovery':'sum','fishing':'sum','private_shop':'sum',\n","        'game_money_change':'sum', 'game_money_change_minus':'sum','enchant_count':'sum'}\n","    \n","    train_activity_merge = train_activity.groupby('acc_id', as_index = False).agg(activity_agg).reset_index(drop=True)\n","    test_activity_merge = test_activity.groupby('acc_id', as_index = False).agg(activity_agg).reset_index(drop=True)\n","    \n","    print('activity id unique and shape :')\n","    print(train_activity_merge['acc_id'].nunique(), train_activity_merge.shape)\n","    print(test_activity_merge['acc_id'].nunique(), test_activity_merge.shape)\n","    \n","    \n","    # merge payment\n","    print('\\nGrouping payment data ...')\n","    \n","    train_payment_merge = train_payment.groupby( 'acc_id', as_index = False).agg({'day':'nunique','amount_spent':'max'}).reset_index(drop=True)\n","    test_payment_merge = test_payment.groupby('acc_id', as_index = False).agg({'day':'nunique','amount_spent':'max'}).reset_index(drop=True)\n","\n","    train_payment_merge.rename(columns={'amount_spent':'max_spent'}, inplace=True) #amout_spent를 일일 최고 구매량으로 변경\n","    test_payment_merge.rename(columns={'amount_spent':'max_spent'}, inplace=True)\n","\n","    print('payment id unique and shape :')\n","    print(train_payment_merge['acc_id'].nunique(), train_payment_merge.shape)\n","    print(test_payment_merge['acc_id'].nunique(), test_payment_merge.shape)\n","    \n","    \n","    # merge combat \n","    print('\\nGrouping combat data ...')\n","    class_names = {0:'liege', 1:'knight', 2:'elf', 3:'magician', 4:'dark-elf', 5:'dragon',6:'illusionist',7:'warrior'}\n","    for elem in train_combat['class'].unique():train_combat[class_names[elem]] = train_combat['class'] == elem\n","    for elem in test_combat['class'].unique():test_combat[class_names[elem]] = test_combat['class'] == elem\n","    \n","    combat_agg = {'day':'nunique','char_id' : 'nunique', 'server' : 'nunique', 'class' : 'nunique', 'level' : 'max',\n","                  'pledge_cnt' : 'sum','random_attacker_cnt' : 'sum','random_defender_cnt' : 'sum', 'temp_cnt' : 'sum',\n","                  'same_pledge_cnt' : 'sum','etc_cnt' : 'sum','num_opponent' : 'sum','elf' : 'sum','magician' : 'sum',\n","                  'warrior' : 'sum','dark-elf' : 'sum','knight' : 'sum','dragon' : 'sum','liege' : 'sum','illusionist' : 'sum'}\n","        \n","    combat_rename = {'random_attacker_cnt':'combat_random_attacker_cnt','random_defender_cnt' : 'combat_random_defender_cnt',\n","                     'temp_cnt' : 'combat_temp_cnt','same_pledge_cnt' : 'combat_same_pledge_cnt','etc_cnt' : 'combat_etc_cnt'}\n","\n","    train_combat_merge = train_combat.groupby('acc_id', as_index = False).agg(combat_agg).reset_index(drop=True)\n","    test_combat_merge = test_combat.groupby('acc_id', as_index = False).agg(combat_agg).reset_index(drop=True)\n","  \n","    train_combat_merge.rename(columns=combat_rename, inplace=True)\n","    test_combat_merge.rename(columns=combat_rename, inplace=True)\n","\n","    print('combat id unique and shape :')\n","    print(train_combat_merge['acc_id'].nunique(), train_combat_merge.shape)\n","    print(test_combat_merge['acc_id'].nunique(), test_combat_merge.shape)\n","    \n","    \n","    # merge combat \n","    print('\\nGrouping pledge data ...')\n","    \n","    pledge_agg = {'day':'nunique','char_id':'nunique','server':'nunique','pledge_id':'nunique', 'play_char_cnt' : 'sum',\n","                  'combat_char_cnt' : 'sum','pledge_combat_cnt' : 'sum','random_attacker_cnt' : 'sum','random_defender_cnt' : 'sum',\n","                  'same_pledge_cnt' : 'sum', 'temp_cnt' : 'sum','etc_cnt' : 'sum','combat_play_time' : 'sum','non_combat_play_time' : 'sum'}\n","    \n","    train_pledge_merge = train_pledge.groupby('acc_id', as_index = False).agg(pledge_agg).reset_index(drop=True)\n","    test_pledge_merge = test_pledge.groupby('acc_id', as_index = False).agg(pledge_agg).reset_index(drop=True)\n","    \n","    print('pledge id unique and shape :')\n","    print(train_pledge_merge['acc_id'].nunique(), train_pledge_merge.shape)\n","    print(test_pledge_merge['acc_id'].nunique(), test_pledge_merge.shape)\n","    \n","    \n","    # merge trade \n","    print('\\nGrouping trade data ...')\n","\n","    # item_type dummies             \n","    train_item_type_dummy = pd.get_dummies(train_trade['item_type'])\n","    test_item_type_dummy = pd.get_dummies(test_trade['item_type'])\n","\n","    # trade_type dummies\n","    type_names = {0:'personal_shop', 1:'trade_shop'}\n","    for elem in train_trade['type'].unique():train_trade[type_names[elem]] = train_trade['type'] == elem\n","    for elem in test_trade['type'].unique():test_trade[type_names[elem]] = test_trade['type'] == elem\n","\n","    # merge dummies\n","    \n","    train_trade = train_trade.drop(columns=['type','item_type'])\n","    train_trade = pd.concat([train_trade, train_item_type_dummy], axis = 1)\n","    \n","    test_trade = test_trade.drop(columns=['type','item_type'])\n","    test_trade = pd.concat([test_trade, test_item_type_dummy], axis = 1)\n","\n","    # rename columns\n","    trade_rename = {'day':'trade_day','server' : 'trade_server', 'time' : 'trade_time'}\n","    train_trade.rename(columns=trade_rename, inplace=True)\n","    test_trade.rename(columns=trade_rename, inplace=True)\n","\n","    #divide source and target id\n","    \n","    train_source_trade = train_trade.drop(columns = ['target_acc_id','source_char_id','target_char_id']).rename(columns={'source_acc_id' : 'acc_id'})\n","    train_source_trade['source'] = 1\n","    train_target_trade = train_trade.drop(columns = ['source_acc_id','source_char_id','target_char_id']).rename(columns={'target_acc_id' : 'acc_id'})\n","    train_target_trade['target'] = 1\n","    \n","    test_source_trade = test_trade.drop(columns = ['target_acc_id','source_char_id','target_char_id']).rename(columns={'source_acc_id' : 'acc_id'})\n","    test_source_trade['source'] = 1\n","    test_target_trade = test_trade.drop(columns = ['source_acc_id','source_char_id','target_char_id']).rename(columns={'target_acc_id' : 'acc_id'})\n","    test_target_trade['target'] = 1\n","\n","\n","    #aggregate\n","    trade_source_agg = {'trade_day':'nunique', 'trade_time' : 'nunique', 'trade_server' : 'nunique','item_amount' : 'sum',\n","                        'item_price': 'sum', 'trade_shop' : 'sum', 'personal_shop' : 'sum', 'accessory' : 'sum', 'adena' : 'sum',\n","                        'armor' : 'sum', 'enchant_scroll' : 'sum', 'etc' : 'sum', 'spell' : 'sum', 'weapon' : 'sum', 'source' : 'sum'}\n","\n","    trade_target_agg = {'trade_day':'nunique', 'trade_time' : 'nunique', 'trade_server' : 'nunique','item_amount' : 'sum',\n","                        'item_price': 'sum', 'trade_shop' : 'sum', 'personal_shop' : 'sum', 'accessory' : 'sum', 'adena' : 'sum',\n","                        'armor' : 'sum', 'enchant_scroll' : 'sum', 'etc' : 'sum', 'spell' : 'sum', 'weapon' : 'sum', 'target' : 'sum'}\n","\n","    train_source_trade = train_source_trade.groupby('acc_id', as_index = False).agg(trade_source_agg).reset_index(drop=True)\n","    train_target_trade = train_target_trade.groupby('acc_id', as_index = False).agg(trade_target_agg).reset_index(drop=True)\n","    \n","    test_source_trade = test_source_trade.groupby('acc_id', as_index = False).agg(trade_source_agg).reset_index(drop=True)\n","    test_target_trade = test_target_trade.groupby('acc_id', as_index = False).agg(trade_target_agg).reset_index(drop=True)\n","\n","    print('trade id unique and shape :')\n","    print(train_source_trade['acc_id'].nunique(), train_source_trade.shape, train_target_trade['acc_id'].nunique(), train_target_trade.shape)\n","    print(test_source_trade['acc_id'].nunique(), test_source_trade.shape, test_target_trade['acc_id'].nunique(), test_target_trade.shape)\n","    \n","    \n","    \n","    #merge all data\n","    print('\\nMerging all data ...')\n","    train_merge = train_label.merge(train_activity_merge, on = ['acc_id'], how ='left').merge(\n","    train_combat_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    train_pledge_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    train_payment_merge.drop(['day'], axis=1), on = ['acc_id'], how ='left').merge(\n","    train_source_trade, on = ['acc_id'], how = 'left').merge(\n","    train_target_trade, on = ['acc_id'], how = 'left')\n","        \n","    test_merge = test_activity_merge.merge(\n","    test_combat_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    test_pledge_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    test_payment_merge.drop(['day'], axis=1), on = ['acc_id'], how ='left').merge(\n","    test_source_trade, on = ['acc_id'], how = 'left').merge(\n","    test_target_trade, on = ['acc_id'], how = 'left')\n","    \n","    print('\\nmerge id unique and shape :')\n","    print(train_merge['acc_id'].nunique(), train_merge.shape)\n","    print(test_merge['acc_id'].nunique(), test_merge.shape)\n","    \n","    \n","    # fill NA\n","#     fill_NA(train_merge)\n","    \n","    # encode features\n","#     encode_features(train_merge)\n","    \n","### STEP1.1 : prepare train and vaild data\n","\n","def _prepare_data():\n","    # prepare train and valid data\n","    print('\\nPreparing train and valid data ...')\n","\n","    global train_x\n","    global train_ya\n","    global train_ys\n","    \n","    global valid_x\n","    global valid_ya\n","    global valid_ys\n","    \n","    global test_x\n","    global test_acc_id\n","    \n","    \n","    drop_vars = ['acc_id','amount_spent','survival_time']\n","    \n","    train_ya = train_merge['amount_spent']\n","    train_ys = train_merge['survival_time']\n","    \n","    train_x = train_merge\n","    train_x.drop(columns=drop_vars, inplace=True)\n","    \n","    valid_x = train_x[30000:]\n","    valid_ya = train_ya[30000:]\n","    \n","    train_x = train_x[:30000]\n","    train_ya = train_ya[:30000]\n","    \n","    valid_ys = train_ys[30000:]\n","    train_ys = train_ys[:30000]\n","    \n","    print('train x shape: ', train_x.shape)\n","    print('train ya shape: ', train_ya.shape)\n","    print('train ys shape: ', train_ys.shape)\n","    \n","    print('valid x shape: ', valid_x.shape)\n","    print('valid ya shape: ', valid_ya.shape)\n","    print('valid ys shape: ', valid_ys.shape)\n","    \n","    \n","    # prepare test data\n","    print('\\nPreparing test data ...')\n","    \n","    test_x = test_merge\n","    test_acc_id = test_merge['acc_id']\n","    test_x.drop(columns=['acc_id'], inplace=True)\n","    print('test x shape: ', test_x.shape)\n","\n","    \n","## STEP2: build model\n","def _build_model():\n","    print('\\n\\nSTEP2: building model ...')\n","    \n","    global xgb_params\n","\n","    xgb_params = {\n","        'eta': 0.007,\n","        'max_depth': 8, \n","        'subsample': 0.8,\n","        'objective': 'reg:linear',\n","        'eval_metric': 'mae',\n","        'lambda': 9.0,\n","        'alpha': 0.8,\n","        'colsample_bytree': 0.7,\n","        'silent': 1,\n","        'random_state' : 42\n","    }\n","\n","    \n","\n","## STEP3: train    \n","def _train_predict_amount():\n","    print('\\n\\nSTEP3: training amount...')\n","    \n","    global a_xgb_clf\n","    global a_pred\n","    \n","    ad_train = xgb.DMatrix(train_x, label=train_ya)\n","    ad_valid = xgb.DMatrix(valid_x, label=valid_ya)\n","    \n","    a_evals = [(ad_train, 'train'), (ad_valid, 'valid')]\n","    a_xgb_clf = xgb.train(xgb_params, ad_train, num_boost_round=10000, evals=a_evals, \n","                        early_stopping_rounds=100, verbose_eval=20)\n","    \n","\n","    ad_test = xgb.DMatrix(test_x)\n","    a_pred = a_xgb_clf.predict(ad_test)\n","    \n","def _train_predict_survival():\n","    print('\\n\\nSTEP4: training survival...')\n","    \n","    global s_xgb_clf\n","    global s_pred\n","    \n","    sd_train = xgb.DMatrix(train_x, label=train_ys)\n","    sd_valid = xgb.DMatrix(valid_x, label=valid_ys)\n","    \n","    s_evals = [(sd_train, 'train'), (sd_valid, 'valid')]\n","    s_xgb_clf = xgb.train(xgb_params, sd_train, num_boost_round=10000, evals=s_evals, \n","                        early_stopping_rounds=100, verbose_eval=20)\n","\n","    sd_test = xgb.DMatrix(test_x)\n","    s_pred = s_xgb_clf.predict(sd_test)\n","    \n","\n","## STEP5: generate submission \n","def _generate_submission():\n","    print('\\n\\nSTEP5: generating submission ...')\n","    \n","    test_predict = pd.DataFrame(columns = ['acc_id', 'survival_time','amount_spent'])\n","    \n","    test_predict['acc_id'] = test_acc_id\n","    test_predict['survival_time'] = s_pred\n","    test_predict['amount_spent'] = a_pred\n","    \n","    test_predict['amount_spent'] = [0 if i < 0 else i for i in test_predict['amount_spent']] # 마이너스값 0 처리 \n","    test_predict['survival_time'] = [1 if i < 1 else i for i in test_predict['survival_time']] # 1 미만 값 1 처리\n","    \n","    #testname에 따라 amountspent value 키우기\n","    if testname == 'test1' :\n","        test_predict['amount_spent'] = test_predict['amount_spent']*30\n","    elif testname == 'test2' :\n","        test_predict['amount_spent'] = test_predict['amount_spent']*5\n","    else :\n","        print('testname is wrong!')\n","        \n","    test_predict.to_csv(submission_path + testname + '_predict.csv',index=False)    \n"," "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"r01vMENYkOSm","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C6ZulL9fGZY9","colab_type":"text"},"source":["\n","#### CODE - predict for testname"]},{"cell_type":"code","metadata":{"id":"r5IxfKrmeyrq","colab_type":"code","colab":{}},"source":["#ref : https://www.kaggle.com/cuongvn08/xgboost-cross-validation-clean-coding-lb-0-64x\n","\n","# -*- coding: utf-8 -*-\n","\n","from enum import Enum\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","from sklearn import preprocessing\n","import xgboost as xgb\n","from datetime import datetime\n","import gc\n","\n","import warnings\n","def ignore_warn(*args, **kwargs):\n","    pass\n","warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n","\n","\n","## STEP0: do setting\n","class Settings(Enum):\n","    global train_path\n","    global test_path\n","    global submission_path\n","    global testname\n","    \n","    testname = 'test2'\n","    \n","    path            = 'drive/My Drive/bigcontest2019/'\n","    train_path      = path + 'data/train/'\n","    test_path       = path + 'data/test/'\n","    submission_path = path + 'scripts/model/metrics/nes_inference/'\n","        \n","    def __str__(self):\n","        return self.value\n","        \n","    \n","## STEP1: process data    \n","def fill_NA(df):\n","    print('\\nFilling NA ...')\n","    \n","    na_ratio = ((df.isnull().sum() / len(df)) * 100).sort_values(ascending=False)\n","    print('NA ratio: ')\n","    print(na_ratio) \n","    \n","    for feature in df:\n","        if df[feature].dtype == 'object':\n","            df[feature] = df[feature].fillna(\"None\")\n","        else:\n","            df[feature] = df[feature].fillna(0)\n","    \n","def encode_features(df):\n","    print('\\nEncoding features ...')\n","    \n","    for feature in df:\n","        if df[feature].dtype == 'object':\n","            print('Encoding ', feature)\n","            lbl = preprocessing.LabelEncoder()\n","            lbl.fit(list(df[feature].values))\n","            df[feature] = lbl.transform(list(df[feature].values))\n","    \n","# def display_feature_target(df, feature):\n","#     fig, ax = plt.subplots()\n","#     ax.scatter(x = df[feature], y = df['logerror'])\n","#     plt.ylabel('logerror', fontsize=13)\n","#     plt.xlabel(feature, fontsize=13)\n","#     plt.show()\n","    \n","def _process_data():\n","    print('\\n\\nSTEP1: processing data ...')\n","    \n","    global train_merge\n","    global test_merge\n","\n","        \n","    # load data\n","    print('\\nLoading data ...')\n","    \n","    train_label = pd.read_csv(train_path + 'train_label.csv')\n","    train_activity = pd.read_csv(train_path + 'train_activity.csv')\n","    train_combat = pd.read_csv(train_path + 'train_combat.csv')\n","    train_payment = pd.read_csv(train_path + 'train_payment.csv')\n","    train_pledge = pd.read_csv(train_path + 'train_pledge.csv')\n","    train_trade = pd.read_csv(train_path + 'train_trade.csv')\n","\n","    test_activity = pd.read_csv(test_path + testname + '_activity.csv')\n","    test_combat = pd.read_csv(test_path + testname + '_combat.csv')\n","    test_payment = pd.read_csv(test_path + testname + '_payment.csv')\n","    test_pledge = pd.read_csv(test_path + testname + '_pledge.csv')\n","    test_trade = pd.read_csv(test_path  + testname + '_trade.csv')\n","\n","    \n","    # merge payment\n","    print('\\nGrouping activity data ...')\n","\n","    train_activity['game_money_change_minus'] = train_activity['game_money_change'] #game_money_change를 마이너스끼리, 플러스끼리 분리\n","    train_activity['game_money_change_minus'] = [0 if i > 0 else i for i in train_activity['game_money_change_minus']]\n","    train_activity['game_money_change'] = [0 if i < 0 else i for i in train_activity['game_money_change']]\n","\n","    test_activity['game_money_change_minus'] = test_activity['game_money_change']\n","    test_activity['game_money_change_minus'] = [0 if i > 0 else i for i in test_activity['game_money_change_minus']]\n","    test_activity['game_money_change'] = [0 if i < 0 else i for i in test_activity['game_money_change']]\n","\n","    activity_agg = {\n","        'day':'nunique', 'char_id':'nunique','server':'nunique', 'playtime':'sum', 'npc_kill':'sum', \n","        'solo_exp':'sum','party_exp':'sum','quest_exp':'sum','rich_monster':'sum', \n","        'death':'sum', 'revive':'sum','exp_recovery':'sum','fishing':'sum','private_shop':'sum',\n","        'game_money_change':'sum', 'game_money_change_minus':'sum','enchant_count':'sum'}\n","    \n","    train_activity_merge = train_activity.groupby('acc_id', as_index = False).agg(activity_agg).reset_index(drop=True)\n","    test_activity_merge = test_activity.groupby('acc_id', as_index = False).agg(activity_agg).reset_index(drop=True)\n","    \n","    print('activity id unique and shape :')\n","    print(train_activity_merge['acc_id'].nunique(), train_activity_merge.shape)\n","    print(test_activity_merge['acc_id'].nunique(), test_activity_merge.shape)\n","    \n","    \n","    # merge payment\n","    print('\\nGrouping payment data ...')\n","    \n","    train_payment_merge = train_payment.groupby( 'acc_id', as_index = False).agg({'day':'nunique','amount_spent':'max'}).reset_index(drop=True)\n","    test_payment_merge = test_payment.groupby('acc_id', as_index = False).agg({'day':'nunique','amount_spent':'max'}).reset_index(drop=True)\n","\n","    train_payment_merge.rename(columns={'amount_spent':'max_spent'}, inplace=True) #amout_spent를 일일 최고 구매량으로 변경\n","    test_payment_merge.rename(columns={'amount_spent':'max_spent'}, inplace=True)\n","\n","    print('payment id unique and shape :')\n","    print(train_payment_merge['acc_id'].nunique(), train_payment_merge.shape)\n","    print(test_payment_merge['acc_id'].nunique(), test_payment_merge.shape)\n","    \n","    \n","    # merge combat \n","    print('\\nGrouping combat data ...')\n","    class_names = {0:'liege', 1:'knight', 2:'elf', 3:'magician', 4:'dark-elf', 5:'dragon',6:'illusionist',7:'warrior'}\n","    for elem in train_combat['class'].unique():train_combat[class_names[elem]] = train_combat['class'] == elem\n","    for elem in test_combat['class'].unique():test_combat[class_names[elem]] = test_combat['class'] == elem\n","    \n","    combat_agg = {'day':'nunique','char_id' : 'nunique', 'server' : 'nunique', 'class' : 'nunique', 'level' : 'max',\n","                  'pledge_cnt' : 'sum','random_attacker_cnt' : 'sum','random_defender_cnt' : 'sum', 'temp_cnt' : 'sum',\n","                  'same_pledge_cnt' : 'sum','etc_cnt' : 'sum','num_opponent' : 'sum','elf' : 'sum','magician' : 'sum',\n","                  'warrior' : 'sum','dark-elf' : 'sum','knight' : 'sum','dragon' : 'sum','liege' : 'sum','illusionist' : 'sum'}\n","        \n","    combat_rename = {'random_attacker_cnt':'combat_random_attacker_cnt','random_defender_cnt' : 'combat_random_defender_cnt',\n","                     'temp_cnt' : 'combat_temp_cnt','same_pledge_cnt' : 'combat_same_pledge_cnt','etc_cnt' : 'combat_etc_cnt'}\n","\n","    train_combat_merge = train_combat.groupby('acc_id', as_index = False).agg(combat_agg).reset_index(drop=True)\n","    test_combat_merge = test_combat.groupby('acc_id', as_index = False).agg(combat_agg).reset_index(drop=True)\n","  \n","    train_combat_merge.rename(columns=combat_rename, inplace=True)\n","    test_combat_merge.rename(columns=combat_rename, inplace=True)\n","\n","    print('combat id unique and shape :')\n","    print(train_combat_merge['acc_id'].nunique(), train_combat_merge.shape)\n","    print(test_combat_merge['acc_id'].nunique(), test_combat_merge.shape)\n","    \n","    \n","    # merge combat \n","    print('\\nGrouping pledge data ...')\n","    \n","    pledge_agg = {'day':'nunique','char_id':'nunique','server':'nunique','pledge_id':'nunique', 'play_char_cnt' : 'sum',\n","                  'combat_char_cnt' : 'sum','pledge_combat_cnt' : 'sum','random_attacker_cnt' : 'sum','random_defender_cnt' : 'sum',\n","                  'same_pledge_cnt' : 'sum', 'temp_cnt' : 'sum','etc_cnt' : 'sum','combat_play_time' : 'sum','non_combat_play_time' : 'sum'}\n","    \n","    train_pledge_merge = train_pledge.groupby('acc_id', as_index = False).agg(pledge_agg).reset_index(drop=True)\n","    test_pledge_merge = test_pledge.groupby('acc_id', as_index = False).agg(pledge_agg).reset_index(drop=True)\n","    \n","    print('pledge id unique and shape :')\n","    print(train_pledge_merge['acc_id'].nunique(), train_pledge_merge.shape)\n","    print(test_pledge_merge['acc_id'].nunique(), test_pledge_merge.shape)\n","    \n","    \n","    #merge all data\n","    print('\\nMerging all data ...')\n","    train_merge = train_label.merge(train_activity_merge, on = ['acc_id'], how ='left').merge(\n","    train_combat_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    train_pledge_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    train_payment_merge.drop(['day'], axis=1), on = ['acc_id'], how ='left')\n","        \n","    test_merge = test_activity_merge.merge(\n","    test_combat_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    test_pledge_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    test_payment_merge.drop(['day'], axis=1), on = ['acc_id'], how ='left')\n","    \n","    print('\\nmerge id unique and shape :')\n","    print(train_merge['acc_id'].nunique(), train_merge.shape)\n","    print(test_merge['acc_id'].nunique(), test_merge.shape)\n","    \n","    \n","    # fill NA\n","#     fill_NA(train_merge)\n","    \n","    # encode features\n","#     encode_features(train_merge)\n","    \n","### STEP1.1 : prepare train and vaild data\n","\n","def _prepare_data():\n","    # prepare train and valid data\n","    print('\\nPreparing train and valid data ...')\n","\n","    global train_x\n","    global train_ya\n","    global train_ys\n","    \n","    global valid_x\n","    global valid_ya\n","    global valid_ys\n","    \n","    global test_x\n","    global test_acc_id\n","    \n","    drop_vars = ['acc_id','amount_spent','survival_time']\n","    \n","    train_ya = train_merge['amount_spent']\n","    train_ys = train_merge['survival_time']\n","    \n","    train_x = train_merge\n","    train_x.drop(columns=drop_vars, inplace=True)\n","    \n","    valid_x = train_x[30000:]\n","    valid_ya = train_ya[30000:]\n","    \n","    train_x = train_x[:30000]\n","    train_ya = train_ya[:30000]\n","    \n","    valid_ys = train_ys[30000:]\n","    train_ys = train_ys[:30000]\n","    \n","    print('train x shape: ', train_x.shape)\n","    print('train ya shape: ', train_ya.shape)\n","    print('train ys shape: ', train_ys.shape)\n","    \n","    print('valid x shape: ', valid_x.shape)\n","    print('valid ya shape: ', valid_ya.shape)\n","    print('valid ys shape: ', valid_ys.shape)\n","    \n","    \n","    # prepare test data\n","    print('\\nPreparing test data ...')\n","    \n","    test_x = test_merge\n","    test_acc_id = test_merge['acc_id']\n","    test_x.drop(columns=['acc_id'], inplace=True)\n","    print('test x shape: ', test_x.shape)\n","\n","    \n","## STEP2: build model\n","def _build_model():\n","    print('\\n\\nSTEP2: building model ...')\n","    \n","    global xgb_params\n","    xgb_params = {\n","        'eta': 0.007,\n","        'max_depth': 6, \n","        'subsample': 0.6,\n","        'objective': 'reg:linear',\n","        'eval_metric': 'mae',\n","        'lambda': 9.0,\n","        'alpha': 0.8,\n","        'colsample_bytree': 0.7,\n","        'silent': 1,\n","        'random_state' : 42\n","    }\n","    \n","\n","## STEP3: train    \n","def _train_predict_amount():\n","    print('\\n\\nSTEP3: training amount...')\n","    \n","    global a_xgb_clf\n","    global a_pred\n","    \n","    ad_train = xgb.DMatrix(train_x, label=train_ya)\n","    ad_valid = xgb.DMatrix(valid_x, label=valid_ya)\n","    \n","    a_evals = [(ad_train, 'train'), (ad_valid, 'valid')]\n","    a_xgb_clf = xgb.train(xgb_params, ad_train, num_boost_round=10000, evals=a_evals, \n","                        early_stopping_rounds=100, verbose_eval=10)\n","\n","    ad_test = xgb.DMatrix(test_x)\n","    a_pred = a_xgb_clf.predict(ad_test)\n","    \n","def _train_predict_survival():\n","    print('\\n\\nSTEP4: training survival...')\n","    \n","    global s_xgb_clf\n","    global s_pred\n","    \n","    sd_train = xgb.DMatrix(train_x, label=train_ys)\n","    sd_valid = xgb.DMatrix(valid_x, label=valid_ys)\n","    \n","    s_evals = [(sd_train, 'train'), (sd_valid, 'valid')]\n","    s_xgb_clf = xgb.train(xgb_params, sd_train, num_boost_round=10000, evals=s_evals, \n","                        early_stopping_rounds=100, verbose_eval=10)\n","\n","    sd_test = xgb.DMatrix(test_x)\n","    s_pred = s_xgb_clf.predict(sd_test)\n","    \n","\n","## STEP5: generate submission \n","def _generate_submission():\n","    print('\\n\\nSTEP5: generating submission ...')\n","    \n","    test_predict = pd.DataFrame(columns = ['acc_id', 'survival_time','amount_spent'])\n","    \n","    test_predict['acc_id'] = test_acc_id\n","    test_predict['survival_time'] = s_pred\n","    test_predict['amount_spent'] = a_pred\n","    \n","    test_predict['amount_spent'] = [0 if i < 0 else i for i in test_predict['amount_spent']]\n","    test_predict['survival_time'] = [1 if i < 1 else i for i in test_predict['survival_time']]\n","    \n","    test_predict.to_csv(submission_path + testname + '_predict.csv',index=False)    \n"," "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"62XbnrCic8IR","colab_type":"text"},"source":["#### CODE - predict for single test1"]},{"cell_type":"code","metadata":{"id":"VAzeUzdoPONN","colab_type":"code","colab":{}},"source":["#ref : https://www.kaggle.com/cuongvn08/xgboost-cross-validation-clean-coding-lb-0-64x\n","\n","# -*- coding: utf-8 -*-\n","\n","from enum import Enum\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","from sklearn import preprocessing\n","import xgboost as xgb\n","from datetime import datetime\n","import gc\n","\n","import warnings\n","def ignore_warn(*args, **kwargs):\n","    pass\n","warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n","\n","\n","## STEP0: do setting\n","class Settings(Enum):\n","    global train_path\n","    global test_path\n","    global submission_path\n","\n","    path            = 'drive/My Drive/bigcontest2019/'\n","    train_path      = path + 'data/train/'\n","    test_path       = path + 'data/test/'\n","    submission_path = path + 'scripts/model/metrics/nes_inference/'\n","        \n","    def __str__(self):\n","        return self.value\n","        \n","    \n","## STEP1: process data    \n","def fill_NA(df):\n","    print('\\nFilling NA ...')\n","    \n","    na_ratio = ((df.isnull().sum() / len(df)) * 100).sort_values(ascending=False)\n","    print('NA ratio: ')\n","    print(na_ratio) \n","    \n","    for feature in df:\n","        if df[feature].dtype == 'object':\n","            df[feature] = df[feature].fillna(\"None\")\n","        else:\n","            df[feature] = df[feature].fillna(0)\n","    \n","def encode_features(df):\n","    print('\\nEncoding features ...')\n","    \n","    for feature in df:\n","        if df[feature].dtype == 'object':\n","            print('Encoding ', feature)\n","            lbl = preprocessing.LabelEncoder()\n","            lbl.fit(list(df[feature].values))\n","            df[feature] = lbl.transform(list(df[feature].values))\n","    \n","# def display_feature_target(df, feature):\n","#     fig, ax = plt.subplots()\n","#     ax.scatter(x = df[feature], y = df['logerror'])\n","#     plt.ylabel('logerror', fontsize=13)\n","#     plt.xlabel(feature, fontsize=13)\n","#     plt.show()\n","    \n","def _process_data():\n","    print('\\n\\nSTEP1: processing data ...')\n","    \n","    global train_merge\n","    global test1_merge\n","        \n","    # load data\n","    print('\\nLoading data ...')\n","    \n","    train_label = pd.read_csv(train_path + 'train_label.csv')\n","    train_activity = pd.read_csv(train_path + 'train_activity.csv')\n","    train_combat = pd.read_csv(train_path + 'train_combat.csv')\n","    train_payment = pd.read_csv(train_path + 'train_payment.csv')\n","    train_pledge = pd.read_csv(train_path + 'train_pledge.csv')\n","    train_trade = pd.read_csv(train_path + 'train_trade.csv')\n","\n","    test1_activity = pd.read_csv(test_path + 'test1_activity.csv')\n","    test1_combat = pd.read_csv(test_path + 'test1_combat.csv')\n","    test1_payment = pd.read_csv(test_path + 'test1_payment.csv')\n","    test1_pledge = pd.read_csv(test_path + 'test1_pledge.csv')\n","    test1_trade = pd.read_csv(test_path + 'test1_trade.csv')\n","\n","    \n","    # merge payment\n","    print('\\nGrouping activity data ...')\n","\n","    train_activity['game_money_change_minus'] = train_activity['game_money_change'] #game_money_change를 마이너스끼리, 플러스끼리 분리\n","    train_activity['game_money_change_minus'] = [0 if i > 0 else i for i in train_activity['game_money_change_minus']]\n","    train_activity['game_money_change'] = [0 if i < 0 else i for i in train_activity['game_money_change']]\n","\n","    test1_activity['game_money_change_minus'] = test1_activity['game_money_change']\n","    test1_activity['game_money_change_minus'] = [0 if i > 0 else i for i in test1_activity['game_money_change_minus']]\n","    test1_activity['game_money_change'] = [0 if i < 0 else i for i in test1_activity['game_money_change']]\n","\n","    activity_agg = {\n","        'day':'nunique', 'char_id':'nunique','server':'nunique', 'playtime':'sum', 'npc_kill':'sum', \n","        'solo_exp':'sum','party_exp':'sum','quest_exp':'sum','rich_monster':'sum', \n","        'death':'sum', 'revive':'sum','exp_recovery':'sum','fishing':'sum','private_shop':'sum',\n","        'game_money_change':'sum', 'game_money_change_minus':'sum','enchant_count':'sum'}\n","    \n","    train_activity_merge = train_activity.groupby('acc_id', as_index = False).agg(activity_agg).reset_index(drop=True)\n","    test1_activity_merge = test1_activity.groupby('acc_id', as_index = False).agg(activity_agg).reset_index(drop=True)\n","    \n","    print('activity id unique and shape :')\n","    print(train_activity_merge['acc_id'].nunique(), train_activity_merge.shape)\n","    print(test1_activity_merge['acc_id'].nunique(), test1_activity_merge.shape)\n","    \n","    \n","    # merge payment\n","    print('\\nGrouping payment data ...')\n","    \n","    train_payment_merge = train_payment.groupby( 'acc_id', as_index = False).agg({'day':'nunique','amount_spent':'max'}).reset_index(drop=True)\n","    test1_payment_merge = test1_payment.groupby('acc_id', as_index = False).agg({'day':'nunique','amount_spent':'max'}).reset_index(drop=True)\n","\n","    train_payment_merge.rename(columns={'amount_spent':'max_spent'}, inplace=True) #amout_spent를 일일 최고 구매량으로 변경\n","    test1_payment_merge.rename(columns={'amount_spent':'max_spent'}, inplace=True)\n","\n","    print('payment id unique and shape :')\n","    print(train_payment_merge['acc_id'].nunique(), train_payment_merge.shape)\n","    print(test1_payment_merge['acc_id'].nunique(), test1_payment_merge.shape)\n","    \n","    \n","    # merge combat \n","    print('\\nGrouping combat data ...')\n","    class_names = {0:'liege', 1:'knight', 2:'elf', 3:'magician', 4:'dark-elf', 5:'dragon',6:'illusionist',7:'warrior'}\n","    for elem in train_combat['class'].unique():train_combat[class_names[elem]] = train_combat['class'] == elem\n","    for elem in test1_combat['class'].unique():test1_combat[class_names[elem]] = test1_combat['class'] == elem\n","    \n","    combat_agg = {'day':'nunique','char_id' : 'nunique', 'server' : 'nunique', 'class' : 'nunique', 'level' : 'max',\n","                  'pledge_cnt' : 'sum','random_attacker_cnt' : 'sum','random_defender_cnt' : 'sum', 'temp_cnt' : 'sum',\n","                  'same_pledge_cnt' : 'sum','etc_cnt' : 'sum','num_opponent' : 'sum','elf' : 'sum','magician' : 'sum',\n","                  'warrior' : 'sum','dark-elf' : 'sum','knight' : 'sum','dragon' : 'sum','liege' : 'sum','illusionist' : 'sum'}\n","        \n","    combat_rename = {'random_attacker_cnt':'combat_random_attacker_cnt','random_defender_cnt' : 'combat_random_defender_cnt',\n","                     'temp_cnt' : 'combat_temp_cnt','same_pledge_cnt' : 'combat_same_pledge_cnt','etc_cnt' : 'combat_etc_cnt'}\n","\n","    train_combat_merge = train_combat.groupby('acc_id', as_index = False).agg(combat_agg).reset_index(drop=True)\n","    test1_combat_merge = test1_combat.groupby('acc_id', as_index = False).agg(combat_agg).reset_index(drop=True)\n","  \n","    train_combat_merge.rename(columns=combat_rename, inplace=True)\n","    test1_combat_merge.rename(columns=combat_rename, inplace=True)\n","\n","    print('combat id unique and shape :')\n","    print(train_combat_merge['acc_id'].nunique(), train_combat_merge.shape)\n","    print(test1_combat_merge['acc_id'].nunique(), test1_combat_merge.shape)\n","    \n","    \n","    # merge combat \n","    print('\\nGrouping pledge data ...')\n","    \n","    pledge_agg = {'day':'nunique','char_id':'nunique','server':'nunique','pledge_id':'nunique', 'play_char_cnt' : 'sum',\n","                  'combat_char_cnt' : 'sum','pledge_combat_cnt' : 'sum','random_attacker_cnt' : 'sum','random_defender_cnt' : 'sum',\n","                  'same_pledge_cnt' : 'sum', 'temp_cnt' : 'sum','etc_cnt' : 'sum','combat_play_time' : 'sum','non_combat_play_time' : 'sum'}\n","    \n","    train_pledge_merge = train_pledge.groupby('acc_id', as_index = False).agg(pledge_agg).reset_index(drop=True)\n","    test1_pledge_merge = test1_pledge.groupby('acc_id', as_index = False).agg(pledge_agg).reset_index(drop=True)\n","    \n","    print('pledge id unique and shape :')\n","    print(train_pledge_merge['acc_id'].nunique(), train_pledge_merge.shape)\n","    print(test1_pledge_merge['acc_id'].nunique(), test1_pledge_merge.shape)\n","    \n","    \n","    #merge all data\n","    print('\\nMerging all data ...')\n","    train_merge = train_label.merge(train_activity_merge, on = ['acc_id'], how ='left').merge(\n","    train_combat_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    train_pledge_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    train_payment_merge.drop(['day'], axis=1), on = ['acc_id'], how ='left')\n","        \n","    test1_merge = test1_activity_merge.merge(\n","    test1_combat_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    test1_pledge_merge.drop(['day', 'char_id','server'], axis=1), on = ['acc_id'], how ='left').merge(\n","    test1_payment_merge.drop(['day'], axis=1), on = ['acc_id'], how ='left')\n","    \n","    print('\\nmerge id unique and shape :')\n","    print(train_merge['acc_id'].nunique(), train_merge.shape)\n","    print(test1_merge['acc_id'].nunique(), test1_merge.shape)\n","    \n","    \n","    # fill NA\n","#     fill_NA(train_merge)\n","    \n","    # encode features\n","#     encode_features(train_merge)\n","    \n","### STEP1.1 : prepare train and vaild data\n","\n","def _prepare_data():\n","    # prepare train and valid data\n","    print('\\nPreparing train and valid data ...')\n","\n","    global train_x\n","    global train_ya\n","    global train_ys\n","    \n","    global valid_x\n","    global valid_ya\n","    global valid_ys\n","    \n","    global test_x\n","    global test_acc_id\n","    \n","    drop_vars = ['acc_id','amount_spent','survival_time']\n","    \n","    train_ya = train_merge['amount_spent']\n","    train_ys = train_merge['survival_time']\n","    \n","    train_x = train_merge\n","    train_x.drop(columns=drop_vars, inplace=True)\n","    \n","    valid_x = train_x[30000:]\n","    valid_ya = train_ya[30000:]\n","    \n","    train_x = train_x[:30000]\n","    train_ya = train_ya[:30000]\n","    \n","    valid_ys = train_ys[30000:]\n","    train_ys = train_ys[:30000]\n","    \n","    print('train x shape: ', train_x.shape)\n","    print('train ya shape: ', train_ya.shape)\n","    print('train ys shape: ', train_ys.shape)\n","    \n","    print('valid x shape: ', valid_x.shape)\n","    print('valid ya shape: ', valid_ya.shape)\n","    print('valid ys shape: ', valid_ys.shape)\n","    \n","    \n","    # prepare test data\n","    print('\\nPreparing test data ...')\n","    \n","    test_x = test1_merge\n","    test_acc_id = test1_merge['acc_id']\n","    test_x.drop(columns=['acc_id'], inplace=True)\n","    print('test x shape: ', test_x.shape)\n","\n","    \n","## STEP2: build model\n","def _build_model():\n","    print('\\n\\nSTEP2: building model ...')\n","    \n","    global xgb_params\n","    xgb_params = {\n","        'eta': 0.007,\n","        'max_depth': 6, \n","        'subsample': 0.6,\n","        'objective': 'reg:linear',\n","        'eval_metric': 'mae',\n","        'lambda': 9.0,\n","        'alpha': 0.8,\n","        'colsample_bytree': 0.7,\n","        'silent': 1,\n","        'random_state' : 42\n","    }\n","    \n","\n","## STEP3: train    \n","def _train_predict_amount():\n","    print('\\n\\nSTEP3: training amount...')\n","    \n","    global a_xgb_clf\n","    global a_pred\n","    \n","    ad_train = xgb.DMatrix(train_x, label=train_ya)\n","    ad_valid = xgb.DMatrix(valid_x, label=valid_ya)\n","    \n","    a_evals = [(ad_train, 'train'), (ad_valid, 'valid')]\n","    a_xgb_clf = xgb.train(xgb_params, ad_train, num_boost_round=10000, evals=a_evals, \n","                        early_stopping_rounds=100, verbose_eval=10)\n","\n","    ad_test = xgb.DMatrix(test_x)\n","    a_pred = a_xgb_clf.predict(ad_test)\n","    \n","def _train_predict_survival():\n","    print('\\n\\nSTEP4: training survival...')\n","    \n","    global s_xgb_clf\n","    global s_pred\n","    \n","    sd_train = xgb.DMatrix(train_x, label=train_ys)\n","    sd_valid = xgb.DMatrix(valid_x, label=valid_ys)\n","    \n","    s_evals = [(sd_train, 'train'), (sd_valid, 'valid')]\n","    s_xgb_clf = xgb.train(xgb_params, sd_train, num_boost_round=10000, evals=s_evals, \n","                        early_stopping_rounds=100, verbose_eval=10)\n","\n","    sd_test = xgb.DMatrix(test_x)\n","    s_pred = s_xgb_clf.predict(sd_test)\n","    \n","\n","## STEP5: generate submission \n","def _generate_submission():\n","    print('\\n\\nSTEP5: generating submission ...')\n","    \n","    filename = 'test1_predict.csv'\n","    \n","    test_predict = pd.DataFrame(columns = ['acc_id', 'survival_time','amount_spent'])\n","    \n","    test_predict['acc_id'] = test_acc_id\n","    test_predict['survival_time'] = s_pred\n","    test_predict['amount_spent'] = a_pred\n","    \n","    test_predict['amount_spent'] = [1 if i < 1 else i for i in test_predict['amount_spent']]\n","    test_predict['survival_time'] = [1 if i < 1 else i for i in test_predict['survival_time']]\n","    \n","    test_predict.to_csv(submission_path + filename,index=False)    \n"," "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YhAnTwxPKN_5","colab_type":"text"},"source":["### etc"]},{"cell_type":"code","metadata":{"id":"N-zXciZw2iAM","colab_type":"code","colab":{}},"source":["\n","from enum import Enum\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.metrics import mean_squared_error\n","from sklearn import preprocessing\n","\n","import xgboost as xgb\n","from xgboost import XGBRegressor\n","import lightgbm as lgb\n","\n","from datetime import datetime\n","import gc\n","\n","\n","from math import sqrt\n","\n","\n","import warnings\n","def ignore_warn(*args, **kwargs):\n","    pass\n","warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n","\n","\n","## STEP0: do setting\n","class Settings(Enum):\n","    global train_path\n","    global test_path\n","    global submission_path\n","    global testname\n","    \n","    testname = 'test2'\n","    \n","    path            = 'drive/My Drive/bigcontest2019/'\n","    train_path      = path + 'data/train/'\n","    test_path       = path + 'data/test/'\n","    submission_path = path + 'scripts/model/metrics/nes_inference/'\n","        \n","    def __str__(self):\n","        return self.value\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5t5jwqyl3_Z1","colab_type":"code","outputId":"fcb63a13-114c-45d9-e6f8-55da7a423f45","executionInfo":{"status":"ok","timestamp":1566788814259,"user_tz":-540,"elapsed":556,"user":{"displayName":"Eunice","photoUrl":"https://lh5.googleusercontent.com/-cSzctfkN1RA/AAAAAAAAAAI/AAAAAAAAQlo/izeHohCao7Y/s64/photo.jpg","userId":"07489983497392537979"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["a = test_trade['source_acc_id'].unique()\n","b = test_acc_id\n","len(set(a) & set(b))     # & is intersection - elements common to both"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["12362"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"jAxOjNVXCpoD","colab_type":"code","colab":{}},"source":["test1_predict = pd.read_csv(submission_path + 'test1_predict.csv')\n","test1_predict['amount_spent'] = test1_predict['amount_spent']*16\n","test1_predict.to_csv(submission_path + 'test1_predict_t.csv',index=False) \n","\n","test2_predict = pd.read_csv(submission_path + 'test2_predict.csv')\n","test2_predict['amount_spent'] = test2_predict['amount_spent']*3\n","test2_predict.to_csv(submission_path + 'test2_predict_t.csv',index=False) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CTZnVzEjuZVe","colab_type":"code","outputId":"322afafe-fbba-407f-e57a-3073318e4f66","executionInfo":{"status":"error","timestamp":1566999209552,"user_tz":-540,"elapsed":593,"user":{"displayName":"Eunice","photoUrl":"https://lh5.googleusercontent.com/-cSzctfkN1RA/AAAAAAAAAAI/AAAAAAAAQlo/izeHohCao7Y/s64/photo.jpg","userId":"07489983497392537979"}},"colab":{"base_uri":"https://localhost:8080/","height":181}},"source":["test1_predict['survival_time'] = [63 if i > 63 else i for i in test_predict['survival_time']] # 63 이상 값 63 처리\n","test1_predict.to_csv(submission_path + testname + '_predict.csv',index=False)    "],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-70-b70787e8668f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest1_predict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'survival_time'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m63\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m63\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_predict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'survival_time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# 63 이상 값 63 처리\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest1_predict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmission_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtestname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_predict.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'test_predict' is not defined"]}]},{"cell_type":"code","metadata":{"id":"PITWEJ5bYedD","colab_type":"code","colab":{}},"source":["train_merge = pd.read_csv(submission_path  + 'merge/' + 'train_merge.csv')\n","test_merge = pd.read_csv(submission_path  + 'merge/' + 'test1_merge.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ietgnLuMIOUX","colab_type":"code","outputId":"34555ad4-a18e-4ab5-ba15-33261a9bfabc","executionInfo":{"status":"ok","timestamp":1567166331563,"user_tz":-540,"elapsed":1040,"user":{"displayName":"Eunice","photoUrl":"https://lh5.googleusercontent.com/-cSzctfkN1RA/AAAAAAAAAAI/AAAAAAAAQlo/izeHohCao7Y/s64/photo.jpg","userId":"07489983497392537979"}},"colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["train_merge.columns"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['char_id', 'server', 'playtime', 'npc_kill', 'solo_exp', 'party_exp',\n","       'quest_exp', 'rich_monster', 'death', 'exp_recovery', 'private_shop',\n","       'game_money_change', 'game_money_change_minus', 'enchant_count',\n","       'login_day_max', 'logout_day', 'level', 'pledge_cnt',\n","       'combat_random_attacker_cnt', 'combat_random_defender_cnt', 'liege',\n","       'pledge_id', 'play_char_cnt', 'combat_char_cnt', 'pledge_combat_cnt',\n","       'combat_play_time', 'non_combat_play_time', 'mean_spent', 'trade_day_x',\n","       'trade_time_x', 'trade_server_x', 'item_amount_x', 'item_price_x',\n","       'trade_shop_x', 'personal_shop_x', 'accessory_x', 'adena_x', 'armor_x',\n","       'enchant_scroll_x', 'etc_x', 'spell_x', 'weapon_x', 'source',\n","       'trade_day_y', 'trade_time_y', 'trade_server_y', 'item_amount_y',\n","       'item_price_y', 'trade_shop_y', 'personal_shop_y', 'accessory_y',\n","       'adena_y', 'armor_y', 'enchant_scroll_y', 'etc_y', 'spell_y',\n","       'weapon_y', 'target', 'attack', 'cnt'],\n","      dtype='object')"]},"metadata":{"tags":[]},"execution_count":257}]}]}